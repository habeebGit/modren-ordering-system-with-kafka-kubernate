time="2026-02-25T11:50:10-06:00" level=warning msg="/Users/habeebmohammed/software/modren-ordering-system-with-kafka-kubernate/modren-ordering-system-with-kafka/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
my-ordering-app-1  | [1G[0K\[1G[0K
my-ordering-app-1  | > my-ordering-app@0.1.0 start
my-ordering-app-1  | > react-scripts start
my-ordering-app-1  | 
my-ordering-app-1  | [1G[0K\[1G[0K[baseline-browser-mapping] The data in this module is over two months old.  To ensure accurate Baseline data, please update: `npm i baseline-browser-mapping@latest -D`
my-ordering-app-1  | (node:25) [DEP_WEBPACK_DEV_SERVER_ON_AFTER_SETUP_MIDDLEWARE] DeprecationWarning: 'onAfterSetupMiddleware' option is deprecated. Please use the 'setupMiddlewares' option.
my-ordering-app-1  | (Use `node --trace-deprecation ...` to show where the warning was created)
my-ordering-app-1  | (node:25) [DEP_WEBPACK_DEV_SERVER_ON_BEFORE_SETUP_MIDDLEWARE] DeprecationWarning: 'onBeforeSetupMiddleware' option is deprecated. Please use the 'setupMiddlewares' option.
my-ordering-app-1  | [2J[3J[H[36mStarting the development server...[39m
my-ordering-app-1  | [36m[39m
my-ordering-app-1  | [2J[3J[H[32mCompiled successfully![39m
my-ordering-app-1  | 
my-ordering-app-1  | You can now view [1mmy-ordering-app[22m in the browser.
my-ordering-app-1  | 
my-ordering-app-1  |   [1mLocal:[22m            http://localhost:[1m3000[22m
my-ordering-app-1  |   [1mOn Your Network:[22m  http://172.18.0.10:[1m3000[22m
my-ordering-app-1  | 
my-ordering-app-1  | Note that the development build is not optimized.
my-ordering-app-1  | To create a production build, use [36mnpm run build[39m.
my-ordering-app-1  | 
my-ordering-app-1  | webpack compiled [1m[32msuccessfully[39m[22m
my-ordering-app-1  | [2J[3J[HCompiling...
my-ordering-app-1  | [2J[3J[H[32mCompiled successfully![39m
my-ordering-app-1  | 
my-ordering-app-1  | You can now view [1mmy-ordering-app[22m in the browser.
my-ordering-app-1  | 
my-ordering-app-1  |   [1mLocal:[22m            http://localhost:[1m3000[22m
my-ordering-app-1  |   [1mOn Your Network:[22m  http://172.18.0.10:[1m3000[22m
my-ordering-app-1  | 
my-ordering-app-1  | Note that the development build is not optimized.
my-ordering-app-1  | To create a production build, use [36mnpm run build[39m.
my-ordering-app-1  | 
my-ordering-app-1  | webpack compiled [1m[32msuccessfully[39m[22m
products_db-1      | The files belonging to this database system will be owned by user "postgres".
products_db-1      | This user must also own the server process.
orders_db-1        | The files belonging to this database system will be owned by user "postgres".
orders_db-1        | This user must also own the server process.
orders_db-1        | 
orders_db-1        | The database cluster will be initialized with locale "en_US.utf8".
orders_db-1        | The default database encoding has accordingly been set to "UTF8".
orders_db-1        | The default text search configuration will be set to "english".
orders_db-1        | 
orders_db-1        | Data page checksums are disabled.
orders_db-1        | 
orders_db-1        | fixing permissions on existing directory /var/lib/postgresql/data ... ok
orders_db-1        | creating subdirectories ... ok
orders_db-1        | selecting dynamic shared memory implementation ... posix
orders_db-1        | selecting default max_connections ... 100
orders_db-1        | selecting default shared_buffers ... 128MB
orders_db-1        | selecting default time zone ... Etc/UTC
orders_db-1        | creating configuration files ... ok
orders_db-1        | running bootstrap script ... ok
orders_db-1        | performing post-bootstrap initialization ... ok
orders_db-1        | initdb: warning: enabling "trust" authentication for local connections
orders_db-1        | You can change this by editing pg_hba.conf or using the option -A, or
orders_db-1        | --auth-local and --auth-host, the next time you run initdb.
orders_db-1        | syncing data to disk ... ok
orders_db-1        | 
orders_db-1        | 
orders_db-1        | Success. You can now start the database server using:
orders_db-1        | 
orders_db-1        |     pg_ctl -D /var/lib/postgresql/data -l logfile start
orders_db-1        | 
orders_db-1        | waiting for server to start....2026-02-25 11:40:01.772 UTC [48] LOG:  starting PostgreSQL 14.21 (Debian 14.21-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
orders_db-1        | 2026-02-25 11:40:01.773 UTC [48] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
orders_db-1        | 2026-02-25 11:40:01.776 UTC [49] LOG:  database system was shut down at 2026-02-25 11:40:01 UTC
orders_db-1        | 2026-02-25 11:40:01.779 UTC [48] LOG:  database system is ready to accept connections
orders_db-1        |  done
orders_db-1        | server started
orders_db-1        | CREATE DATABASE
products_db-1      | 
products_db-1      | The database cluster will be initialized with locale "en_US.utf8".
products_db-1      | The default database encoding has accordingly been set to "UTF8".
products_db-1      | The default text search configuration will be set to "english".
products_db-1      | 
products_db-1      | Data page checksums are disabled.
products_db-1      | 
products_db-1      | fixing permissions on existing directory /var/lib/postgresql/data ... ok
products_db-1      | creating subdirectories ... ok
products_db-1      | selecting dynamic shared memory implementation ... posix
products_db-1      | selecting default max_connections ... 100
products_db-1      | selecting default shared_buffers ... 128MB
products_db-1      | selecting default time zone ... Etc/UTC
products_db-1      | creating configuration files ... ok
products_db-1      | running bootstrap script ... ok
products_db-1      | performing post-bootstrap initialization ... ok
products_db-1      | syncing data to disk ... ok
products_db-1      | 
products_db-1      | 
products_db-1      | Success. You can now start the database server using:
products_db-1      | 
products_db-1      |     pg_ctl -D /var/lib/postgresql/data -l logfile start
products_db-1      | 
products_db-1      | initdb: warning: enabling "trust" authentication for local connections
products_db-1      | You can change this by editing pg_hba.conf or using the option -A, or
products_db-1      | --auth-local and --auth-host, the next time you run initdb.
products_db-1      | waiting for server to start....2026-02-25 11:40:01.774 UTC [48] LOG:  starting PostgreSQL 14.21 (Debian 14.21-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
products_db-1      | 2026-02-25 11:40:01.775 UTC [48] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
products_db-1      | 2026-02-25 11:40:01.778 UTC [49] LOG:  database system was shut down at 2026-02-25 11:40:01 UTC
products_db-1      | 2026-02-25 11:40:01.781 UTC [48] LOG:  database system is ready to accept connections
products_db-1      |  done
products_db-1      | server started
products_db-1      | CREATE DATABASE
products_db-1      | 
products_db-1      | 
products_db-1      | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/00-init.sql
products_db-1      | CREATE EXTENSION
products_db-1      | 
products_db-1      | 
products_db-1      | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/01-seed-products.sql
products_db-1      | CREATE TABLE
products_db-1      | INSERT 0 8
products_db-1      | 
products_db-1      | 
products_db-1      | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/02-seed-orders.sql
products_db-1      | CREATE TABLE
prometheus-1       | time=2026-02-25T11:40:01.071Z level=INFO source=main.go:1549 msg="updated GOGC" old=100 new=75
prometheus-1       | time=2026-02-25T11:40:01.071Z level=INFO source=main.go:680 msg="Leaving GOMAXPROCS=12: CPU quota undefined" component=automaxprocs
prometheus-1       | time=2026-02-25T11:40:01.072Z level=INFO source=memlimit.go:198 msg="GOMEMLIMIT is updated" component=automemlimit package=github.com/KimMachineGun/automemlimit/memlimit GOMEMLIMIT=7395976396 previous=9223372036854775807
prometheus-1       | time=2026-02-25T11:40:01.072Z level=INFO source=main.go:722 msg="No time or size retention was set so using the default time retention" duration=15d
prometheus-1       | time=2026-02-25T11:40:01.072Z level=INFO source=main.go:773 msg="Starting Prometheus Server" mode=server version="(version=3.7.1, branch=HEAD, revision=0aeb4fddc93b64e4e95104d5e8ea8b55ad36fb61)"
prometheus-1       | time=2026-02-25T11:40:01.072Z level=INFO source=main.go:778 msg="operational information" build_context="(go=go1.25.3, platform=linux/arm64, user=root@54bf11233185, date=20251017-06:35:17, tags=netgo,builtinassets)" host_details="(Linux 6.10.14-linuxkit #1 SMP Sat May 17 08:28:57 UTC 2025 aarch64 09df80b8b90c (none))" fd_limits="(soft=1048576, hard=1048576)" vm_limits="(soft=unlimited, hard=unlimited)"
prometheus-1       | time=2026-02-25T11:40:01.076Z level=INFO source=web.go:660 msg="Start listening for connections" component=web address=0.0.0.0:9090
prometheus-1       | time=2026-02-25T11:40:01.080Z level=INFO source=main.go:1293 msg="Starting TSDB ..."
prometheus-1       | time=2026-02-25T11:40:01.082Z level=INFO source=tls_config.go:346 msg="Listening on" component=web address=[::]:9090
prometheus-1       | time=2026-02-25T11:40:01.082Z level=INFO source=tls_config.go:349 msg="TLS is disabled." component=web http2=false address=[::]:9090
prometheus-1       | time=2026-02-25T11:40:01.086Z level=INFO source=head.go:669 msg="Replaying on-disk memory mappable chunks if any" component=tsdb
prometheus-1       | time=2026-02-25T11:40:01.087Z level=INFO source=head.go:755 msg="On-disk memory mappable chunks replay completed" component=tsdb duration=12.375Âµs
prometheus-1       | time=2026-02-25T11:40:01.087Z level=INFO source=head.go:763 msg="Replaying WAL, this may take a while" component=tsdb
prometheus-1       | time=2026-02-25T11:40:01.087Z level=INFO source=head.go:836 msg="WAL segment loaded" component=tsdb segment=0 maxSegment=0 duration=251.292Âµs
prometheus-1       | time=2026-02-25T11:40:01.087Z level=INFO source=head.go:873 msg="WAL replay completed" component=tsdb checkpoint_replay_duration=20.209Âµs wal_replay_duration=273.291Âµs wbl_replay_duration=125ns chunk_snapshot_load_duration=0s mmap_chunk_replay_duration=12.375Âµs total_replay_duration=323.75Âµs
prometheus-1       | time=2026-02-25T11:40:01.088Z level=INFO source=main.go:1314 msg="filesystem information" fs_type=EXT4_SUPER_MAGIC
prometheus-1       | time=2026-02-25T11:40:01.088Z level=INFO source=main.go:1317 msg="TSDB started"
prometheus-1       | time=2026-02-25T11:40:01.088Z level=INFO source=main.go:1502 msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
prometheus-1       | time=2026-02-25T11:40:01.089Z level=INFO source=main.go:1542 msg="Completed loading of configuration file" db_storage=1.208Âµs remote_storage=1.125Âµs web_handler=542ns query_engine=875ns scrape=522.542Âµs scrape_sd=50.875Âµs notify=1.166Âµs notify_sd=1.917Âµs rules=1.458Âµs tracing=18.167Âµs filename=/etc/prometheus/prometheus.yml totalDuration=1.17075ms
prometheus-1       | time=2026-02-25T11:40:01.089Z level=INFO source=main.go:1278 msg="Server is ready to receive web requests."
prometheus-1       | time=2026-02-25T11:40:01.090Z level=INFO source=manager.go:190 msg="Starting rule manager..." component="rule manager"
prometheus-1       | time=2026-02-25T15:13:57.531Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1772019619471 maxt=1772020800000 ulid=01KJANTB8R3HQGE3AQ86661B74
prometheus-1       | time=2026-02-25T15:13:57.580Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1772019619471 maxt=1772020800000 ulid=01KJANTB8R3HQGE3AQ86661B74 duration=52.607458ms ooo=false
prometheus-1       | time=2026-02-25T15:13:57.581Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus-1       | time=2026-02-25T15:13:57.583Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=938.292Âµs
prometheus-1       | time=2026-02-25T15:13:57.583Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1772020804480 maxt=1772028000000 ulid=01KJANTBAF9VDVC333ZCC311PZ
prometheus-1       | time=2026-02-25T15:13:57.592Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1772020804480 maxt=1772028000000 ulid=01KJANTBAF9VDVC333ZCC311PZ duration=9.562916ms ooo=false
prometheus-1       | time=2026-02-25T15:13:57.592Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus-1       | time=2026-02-25T15:13:57.593Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.005125ms
prometheus-1       | time=2026-02-25T17:15:43.617Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1772028775099 maxt=1772035200000 ulid=01KJAWSA412YT3SMF4CG0TE4YQ
prometheus-1       | time=2026-02-25T17:15:43.647Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1772028775099 maxt=1772035200000 ulid=01KJAWSA412YT3SMF4CG0TE4YQ duration=30.281709ms ooo=false
prometheus-1       | time=2026-02-25T17:15:43.648Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus-1       | time=2026-02-25T17:15:43.650Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.716083ms
products_db-1      | CREATE TABLE
products_db-1      | INSERT 0 3
products_db-1      | INSERT 0 6
products_db-1      | 
products_db-1      | 
products_db-1      | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init-database.sql
products_db-1      | DO
products_db-1      | psql:/docker-entrypoint-initdb.d/init-database.sql:12: NOTICE:  Role setup done
products_db-1      | DO
products_db-1      | DO
products_db-1      | 
products_db-1      | 
products_db-1      | waiting for server to shut down....2026-02-25 11:40:02.102 UTC [48] LOG:  received fast shutdown request
products_db-1      | 2026-02-25 11:40:02.103 UTC [48] LOG:  aborting any active transactions
products_db-1      | 2026-02-25 11:40:02.104 UTC [48] LOG:  background worker "logical replication launcher" (PID 55) exited with exit code 1
orders_db-1        | 
orders_db-1        | 
orders_db-1        | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/00-init.sql
orders_db-1        | CREATE EXTENSION
orders_db-1        | 
orders_db-1        | 
orders_db-1        | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/01-seed-products.sql
orders_db-1        | CREATE TABLE
orders_db-1        | INSERT 0 8
orders_db-1        | 
products_db-1      | 2026-02-25 11:40:02.104 UTC [50] LOG:  shutting down
products_db-1      | 2026-02-25 11:40:02.120 UTC [48] LOG:  database system is shut down
products_db-1      |  done
orders_db-1        | 
orders_db-1        | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/02-seed-orders.sql
orders_db-1        | CREATE TABLE
orders_db-1        | CREATE TABLE
orders_db-1        | INSERT 0 3
orders_db-1        | INSERT 0 6
products_db-1      | server stopped
products_db-1      | 
products_db-1      | PostgreSQL init process complete; ready for start up.
products_db-1      | 
products_db-1      | 2026-02-25 11:40:02.224 UTC [1] LOG:  starting PostgreSQL 14.21 (Debian 14.21-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
products_db-1      | 2026-02-25 11:40:02.224 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
products_db-1      | 2026-02-25 11:40:02.224 UTC [1] LOG:  listening on IPv6 address "::", port 5432
products_db-1      | 2026-02-25 11:40:02.226 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
products_db-1      | 2026-02-25 11:40:02.229 UTC [71] LOG:  database system was shut down at 2026-02-25 11:40:02 UTC
products_db-1      | 2026-02-25 11:40:02.231 UTC [1] LOG:  database system is ready to accept connections
products_db-1      | 2026-02-25 11:58:10.326 UTC [460] ERROR:  column "reservedstock" of relation "products" does not exist at character 36
products_db-1      | 2026-02-25 11:58:10.326 UTC [460] STATEMENT:  INSERT INTO Products (name, stock, reservedStock, version, createdAt, updatedAt) VALUES ('E2E Test Product', 10, 0, 1, now(), now());
orders_db-1        | 
orders_db-1        | 
products_db-1      | 2026-02-25 11:59:13.517 UTC [488] ERROR:  column "reservedstock" of relation "products" does not exist at character 36
products_db-1      | 2026-02-25 11:59:13.517 UTC [488] STATEMENT:  INSERT INTO products (name, stock, reservedstock, version, createdat, updatedat) VALUES ('E2E Test Product',10,0,1,now(),now());
products_db-1      | 2026-02-25 12:02:31.769 UTC [568] ERROR:  syntax error at or near "Test" at character 103
products_db-1      | 2026-02-25 12:02:31.769 UTC [568] STATEMENT:  INSERT INTO "Products" ("name","stock","reservedStock","version","createdAt","updatedAt") VALUES (E2E Test Product, 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:03:21.443 UTC [593] ERROR:  column "e2etestproduct" does not exist at character 99
products_db-1      | 2026-02-25 12:03:21.443 UTC [593] STATEMENT:  INSERT INTO "Products" ("name","stock","reservedStock","version","createdAt","updatedAt") VALUES (E2ETestProduct, 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:03:51.990 UTC [609] ERROR:  column "e2etestproduct" does not exist at character 99
products_db-1      | 2026-02-25 12:03:51.990 UTC [609] STATEMENT:  INSERT INTO "Products" ("name","stock","reservedStock","version","createdAt","updatedAt") VALUES (E2ETestProduct, 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:04:17.257 UTC [628] ERROR:  column "testproduct" does not exist at character 99
products_db-1      | 2026-02-25 12:04:17.257 UTC [628] STATEMENT:  INSERT INTO "Products" ("name","stock","reservedStock","version","createdAt","updatedAt") VALUES (TestProduct, 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:04:36.780 UTC [636] ERROR:  column "TestProduct" does not exist at character 99
products_db-1      | 2026-02-25 12:04:36.780 UTC [636] STATEMENT:  INSERT INTO "Products" ("name","stock","reservedStock","version","createdAt","updatedAt") VALUES ("TestProduct", 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:05:48.707 UTC [669] ERROR:  column "testproduct" does not exist at character 99
products_db-1      | 2026-02-25 12:05:48.707 UTC [669] STATEMENT:  INSERT INTO "Products" ("name","stock","reservedStock","version","createdAt","updatedAt") VALUES (TestProduct, 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:07:09.717 UTC [698] ERROR:  column "testproduct" does not exist at character 99
products_db-1      | 2026-02-25 12:07:09.717 UTC [698] STATEMENT:  INSERT INTO "Products" ("name","stock","reservedStock","version","createdAt","updatedAt") VALUES (TestProduct, 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:07:40.432 UTC [714] ERROR:  column "reservedstock" of relation "products" does not exist at character 34
products_db-1      | 2026-02-25 12:07:40.432 UTC [714] STATEMENT:  INSERT INTO Products (name,stock,reservedStock,version,createdAt,updatedAt) VALUES ('TestProduct', 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:08:34.970 UTC [742] ERROR:  column "reservedstock" of relation "products" does not exist at character 34
products_db-1      | 2026-02-25 12:08:34.970 UTC [742] STATEMENT:  INSERT INTO Products (name,stock,reservedStock,version,createdAt,updatedAt) VALUES ('TestProduct', 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:09:26.628 UTC [766] ERROR:  column "reservedstock" of relation "products" does not exist at character 34
products_db-1      | 2026-02-25 12:09:26.628 UTC [766] STATEMENT:  INSERT INTO Products (name,stock,reservedStock,version,createdAt,updatedAt) VALUES ('TestProduct', 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:10:54.832 UTC [810] ERROR:  column "reservedstock" of relation "products" does not exist at character 34
orders_db-1        | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init-database.sql
orders_db-1        | DO
orders_db-1        | psql:/docker-entrypoint-initdb.d/init-database.sql:12: NOTICE:  Role setup done
orders_db-1        | DO
orders_db-1        | DO
orders_db-1        | 
orders_db-1        | 
orders_db-1        | waiting for server to shut down....2026-02-25 11:40:02.103 UTC [48] LOG:  received fast shutdown request
orders_db-1        | 2026-02-25 11:40:02.104 UTC [48] LOG:  aborting any active transactions
products_db-1      | 2026-02-25 12:10:54.832 UTC [810] STATEMENT:  INSERT INTO Products (name,stock,reservedStock,version,createdAt,updatedAt) VALUES ('TestProduct', 10, 0, 1, now(), now());
products_db-1      | 2026-02-25 12:11:26.479 UTC [826] ERROR:  column "reservedstock" of relation "products" does not exist at character 34
products_db-1      | 2026-02-25 12:11:26.479 UTC [826] STATEMENT:  INSERT INTO Products (name,stock,reservedstock,version,createdAt,updatedAt) VALUES ('TestProduct', 10, 0, 1, now(), now());
orders_db-1        | 2026-02-25 11:40:02.105 UTC [48] LOG:  background worker "logical replication launcher" (PID 55) exited with exit code 1
orders_db-1        | 2026-02-25 11:40:02.105 UTC [50] LOG:  shutting down
orders_db-1        | 2026-02-25 11:40:02.120 UTC [48] LOG:  database system is shut down
orders_db-1        |  done
orders_db-1        | server stopped
orders_db-1        | 
orders_db-1        | PostgreSQL init process complete; ready for start up.
orders_db-1        | 
orders_db-1        | 2026-02-25 11:40:02.223 UTC [1] LOG:  starting PostgreSQL 14.21 (Debian 14.21-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
orders_db-1        | 2026-02-25 11:40:02.223 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
orders_db-1        | 2026-02-25 11:40:02.224 UTC [1] LOG:  listening on IPv6 address "::", port 5432
orders_db-1        | 2026-02-25 11:40:02.225 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
orders_db-1        | 2026-02-25 11:40:02.228 UTC [71] LOG:  database system was shut down at 2026-02-25 11:40:02 UTC
orders_db-1        | 2026-02-25 11:40:02.231 UTC [1] LOG:  database system is ready to accept connections
zookeeper-1        | ===> User
zookeeper-1        | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
zookeeper-1        | ===> Configuring ...
zookeeper-1        | ===> Running preflight checks ... 
zookeeper-1        | ===> Check if /var/lib/zookeeper/data is writable ...
zookeeper-1        | ===> Check if /var/lib/zookeeper/log is writable ...
zookeeper-1        | ===> Launching ... 
zookeeper-1        | ===> Launching zookeeper ... 
zookeeper-1        | [2026-02-25 11:33:23,681] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,702] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,702] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,702] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,703] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,704] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1        | [2026-02-25 11:33:23,704] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1        | [2026-02-25 11:33:23,704] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1        | [2026-02-25 11:33:23,704] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
zookeeper-1        | [2026-02-25 11:33:23,712] INFO Log4j 1.2 jmx support found and enabled. (org.apache.zookeeper.jmx.ManagedUtil)
zookeeper-1        | [2026-02-25 11:33:23,727] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,728] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,728] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,728] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,728] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 11:33:23,728] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
zookeeper-1        | [2026-02-25 11:33:23,765] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@52102734 (org.apache.zookeeper.server.ServerMetrics)
zookeeper-1        | [2026-02-25 11:33:23,769] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,782] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:host.name=0330c7d8d81b (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:java.version=11.0.13 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-json-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-raft-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/netty-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/connect-transforms-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-shell-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-clients-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/trogdor-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.68.Final.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.68.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.19.3.jar:/usr/bin/../share/java/kafka/kafka-streams-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.68.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/connect-mirror-7.0.1-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
kafka-1            | ===> User
kafka-1            | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
kafka-1            | ===> Configuring ...
kafka-1            | ===> Running preflight checks ... 
kafka-1            | ===> Check if /var/lib/kafka/data is writable ...
kafka-1            | ===> Check if Zookeeper is healthy ...
kafka-1            | SLF4J: Class path contains multiple SLF4J bindings.
kafka-1            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
kafka-1            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
kafka-1            | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
kafka-1            | SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=1b97dae00011
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.13
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/usr/share/java/cp-base-new/lz4-java-1.7.1.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/cp-base-new/zstd-jni-1.5.0-2.jar:/usr/share/java/cp-base-new/jolokia-core-1.6.2.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/kafka-metadata-7.0.1-ccs.jar:/usr/share/java/cp-base-new/json-simple-1.1.1.jar:/usr/share/java/cp-base-new/kafka-raft-7.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-databind-2.12.3.jar:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar:/usr/share/java/cp-base-new/kafka_2.13-7.0.1-ccs.jar:/usr/share/java/cp-base-new/kafka-server-common-7.0.1-ccs.jar:/usr/share/java/cp-base-new/metrics-core-4.1.12.1.jar:/usr/share/java/cp-base-new/snakeyaml-1.27.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/kafka-clients-7.0.1-ccs.jar:/usr/share/java/cp-base-new/confluent-log4j-1.2.17-cp2.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.3.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/jmx_prometheus_javaagent-0.14.0.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.12.3.jar:/usr/share/java/cp-base-new/kafka-storage-api-7.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/cp-base-new/snappy-java-1.1.8.1.jar:/usr/share/java/cp-base-new/gson-2.8.6.jar:/usr/share/java/cp-base-new/jackson-annotations-2.12.3.jar:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar:/usr/share/java/cp-base-new/disk-usage-agent-7.0.1.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.30.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.6.3.jar:/usr/share/java/cp-base-new/jackson-core-2.12.3.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.5.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/kafka-storage-7.0.1-ccs.jar:/usr/share/java/cp-base-new/common-utils-7.0.1.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/jolokia-jvm-1.6.2-agent.jar:/usr/share/java/cp-base-new/zookeeper-3.6.3.jar:/usr/share/java/cp-base-new/utility-belt-7.0.1.jar:/usr/share/java/cp-base-new/scala-library-2.13.5.jar
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=6.10.14-linuxkit
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/home/appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/home/appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=117MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=1960MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=124MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@a7e666
kafka-1            | [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
kafka-1            | [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 1048575 Bytes
kafka-1            | [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=false
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.2:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.2:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.2:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.18.0.3:37018, server: zookeeper/172.18.0.2:2181
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.18.0.2:2181, session id = 0x100001c05930000, negotiated timeout = 40000
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - An exception was thrown while closing send thread for session 0x100001c05930000.
kafka-1            | EndOfStreamException: Unable to read additional data from server sessionid 0x100001c05930000, likely server has closed socket
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x100001c05930000 closed
kafka-1            | [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100001c05930000
kafka-1            | ===> Launching ... 
kafka-1            | ===> Launching kafka ... 
kafka-1            | [2026-02-25 11:33:26,454] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
kafka-1            | [2026-02-25 11:33:26,882] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
kafka-1            | [2026-02-25 11:33:26,985] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
kafka-1            | [2026-02-25 11:33:26,990] INFO starting (kafka.server.KafkaServer)
kafka-1            | [2026-02-25 11:33:26,990] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
kafka-1            | [2026-02-25 11:33:27,007] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2026-02-25 11:33:27,013] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,013] INFO Client environment:host.name=1b97dae00011 (org.apache.zookeeper.ZooKeeper)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:os.memory.free=492MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,784] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,785] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,785] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,785] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,785] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,786] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)
zookeeper-1        | [2026-02-25 11:33:23,787] INFO minSessionTimeout set to 4000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,787] INFO maxSessionTimeout set to 40000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,789] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper-1        | [2026-02-25 11:33:23,789] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper-1        | [2026-02-25 11:33:23,790] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 11:33:23,790] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 11:33:23,790] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 11:33:23,790] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 11:33:23,790] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 11:33:23,790] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 11:33:23,793] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,794] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,794] INFO Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:23,821] INFO Logging initialized @1286ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
zookeeper-1        | [2026-02-25 11:33:24,026] WARN o.e.j.s.ServletContextHandler@2584b82d{/,null,STOPPED} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2026-02-25 11:33:24,027] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2026-02-25 11:33:24,069] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS (org.eclipse.jetty.server.Server)
zookeeper-1        | [2026-02-25 11:33:24,118] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
zookeeper-1        | [2026-02-25 11:33:24,118] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
zookeeper-1        | [2026-02-25 11:33:24,121] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
zookeeper-1        | [2026-02-25 11:33:24,125] WARN ServletContext@o.e.j.s.ServletContextHandler@2584b82d{/,null,STARTING} has uncovered http methods for path: /* (org.eclipse.jetty.security.SecurityHandler)
zookeeper-1        | [2026-02-25 11:33:24,138] INFO Started o.e.j.s.ServletContextHandler@2584b82d{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2026-02-25 11:33:24,162] INFO Started ServerConnector@247310d0{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
zookeeper-1        | [2026-02-25 11:33:24,162] INFO Started @1628ms (org.eclipse.jetty.server.Server)
zookeeper-1        | [2026-02-25 11:33:24,162] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
zookeeper-1        | [2026-02-25 11:33:24,168] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper-1        | [2026-02-25 11:33:24,169] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper-1        | [2026-02-25 11:33:24,170] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 24 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper-1        | [2026-02-25 11:33:24,172] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper-1        | [2026-02-25 11:33:24,188] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
zookeeper-1        | [2026-02-25 11:33:24,188] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
zookeeper-1        | [2026-02-25 11:33:24,190] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2026-02-25 11:33:24,190] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2026-02-25 11:33:24,202] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)
zookeeper-1        | [2026-02-25 11:33:24,202] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2026-02-25 11:33:24,205] INFO Snapshot loaded in 15 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2026-02-25 11:33:24,205] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2026-02-25 11:33:24,205] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 11:33:24,213] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)
zookeeper-1        | [2026-02-25 11:33:24,215] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)
zookeeper-1        | [2026-02-25 11:33:24,240] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)
zookeeper-1        | [2026-02-25 11:33:24,241] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)
zookeeper-1        | [2026-02-25 11:33:24,325] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ð ï¸  run anywhere with `dotenvx run -- yourcommand`
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T11:40:08.030Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): CREATE TABLE IF NOT EXISTS "Orders" ("id"   SERIAL , "userId" INTEGER NOT NULL, "status" VARCHAR(255) DEFAULT 'Pending', "createdAt" TIMESTAMP WITH TIME ZONE NOT NULL, "updatedAt" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id"));
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): CREATE TABLE IF NOT EXISTS "OrderItems" ("id"   SERIAL , "productId" INTEGER NOT NULL, "quantity" INTEGER NOT NULL, "createdAt" TIMESTAMP WITH TIME ZONE NOT NULL, "updatedAt" TIMESTAMP WITH TIME ZONE NOT NULL, "OrderId" INTEGER REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE, PRIMARY KEY ("id"));
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): CREATE TABLE IF NOT EXISTS "ProcessedEvents" ("id" VARCHAR(255) , "eventType" VARCHAR(255) NOT NULL, "processedAt" TIMESTAMP WITH TIME ZONE NOT NULL, "createdAt" TIMESTAMP WITH TIME ZONE NOT NULL, "updatedAt" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id"));
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): CREATE TABLE IF NOT EXISTS "DeadLetterEvents" ("id" VARCHAR(255) , "eventType" VARCHAR(255), "payload" JSON, "errorMessage" TEXT, "createdAt" TIMESTAMP WITH TIME ZONE, "updatedAt" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id"));
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"info","message":"Kafka producer connected","timestamp":"2026-02-25T11:40:08.166Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T11:40:08.228Z","logger":"kafkajs","message":"[Connection] Response Metadata(key: 3, version: 6)","broker":"kafka:9092","clientId":"kafkajs","error":"This server does not host this topic-partition","correlationId":1,"size":82}
order-service-1    | {"error":"This server does not host this topic-partition","level":"warn","message":"Consumer start attempt 1 failed","timestamp":"2026-02-25T11:40:08.229Z"}
order-service-1    | {"level":"INFO","timestamp":"2026-02-25T11:40:10.233Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"order-service-group"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T11:40:10.241Z","logger":"kafkajs","message":"[Connection] Response GroupCoordinator(key: 10, version: 2)","broker":"kafka:9092","clientId":"kafkajs","error":"The group coordinator is not available","correlationId":3,"size":55}
order-service-1    | {"level":"INFO","timestamp":"2026-02-25T11:40:10.904Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"order-service-group","memberId":"kafkajs-cdcde567-88ff-4efd-a00b-1f159868e7d4","leaderId":"kafkajs-cdcde567-88ff-4efd-a00b-1f159868e7d4","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":670}
order-service-1    | {"level":"info","message":"Kafka consumer started","timestamp":"2026-02-25T11:40:10.905Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:40:11.835Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:40:11 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:40:11.842Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:40:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:40:19.499Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:40:21.933Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:40:21 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:40:21.934Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:40:32.173Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:40:32 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:40:32.174Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:40:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:40:34.493Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:40:42.254Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:40:42 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:40:42.255Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:40:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:40:49.490Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:40:52.357Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:40:52 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:40:52.359Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:41:02.488Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:41:02 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:41:02.495Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:41:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:41:04.492Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:41:12.597Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:41:12 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:41:12.599Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:41:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:41:19.526Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:41:22.719Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:41:22 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:41:22.720Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:41:32.818Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:41:32 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:41:32.819Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:41:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:41:34.483Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:41:42.978Z"}
kafka-1            | [2026-02-25 11:33:27,013] INFO Client environment:java.version=11.0.13 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,013] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,013] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,013] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-json-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-raft-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/netty-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/connect-transforms-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-shell-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-clients-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/trogdor-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.68.Final.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.68.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.19.3.jar:/usr/bin/../share/java/kafka/kafka-streams-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.68.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/connect-mirror-7.0.1-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,013] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:os.memory.free=242MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:os.memory.max=512MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,014] INFO Client environment:os.memory.total=256MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,016] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@30b19518 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 11:33:27,022] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
kafka-1            | [2026-02-25 11:33:27,028] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 11:33:27,045] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2026-02-25 11:33:27,057] INFO Opening socket connection to server zookeeper/172.18.0.2:2181. (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 11:33:27,058] INFO SASL config status: Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 11:33:27,064] INFO Socket connection established, initiating session, client: /172.18.0.3:37034, server: zookeeper/172.18.0.2:2181 (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 11:33:27,078] INFO Session establishment complete on server zookeeper/172.18.0.2:2181, session id = 0x100001c05930001, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 11:33:27,081] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2026-02-25 11:33:27,247] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
kafka-1            | [2026-02-25 11:33:27,266] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
kafka-1            | [2026-02-25 11:33:27,266] INFO Cleared cache (kafka.server.FinalizedFeatureCache)
kafka-1            | [2026-02-25 11:33:27,459] INFO Cluster ID = RlU0PbmjTQWpLjIxXBn_Bw (kafka.server.KafkaServer)
kafka-1            | [2026-02-25 11:33:27,468] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
kafka-1            | [2026-02-25 11:33:27,522] INFO KafkaConfig values: 
kafka-1            | 	advertised.listeners = PLAINTEXT://kafka:9092
kafka-1            | 	alter.config.policy.class.name = null
kafka-1            | 	alter.log.dirs.replication.quota.window.num = 11
kafka-1            | 	alter.log.dirs.replication.quota.window.size.seconds = 1
kafka-1            | 	authorizer.class.name = 
kafka-1            | 	auto.create.topics.enable = true
kafka-1            | 	auto.leader.rebalance.enable = true
kafka-1            | 	background.threads = 10
kafka-1            | 	broker.heartbeat.interval.ms = 2000
kafka-1            | 	broker.id = 1
kafka-1            | 	broker.id.generation.enable = true
kafka-1            | 	broker.rack = null
kafka-1            | 	broker.session.timeout.ms = 9000
kafka-1            | 	client.quota.callback.class = null
kafka-1            | 	compression.type = producer
kafka-1            | 	connection.failed.authentication.delay.ms = 100
kafka-1            | 	connections.max.idle.ms = 600000
kafka-1            | 	connections.max.reauth.ms = 0
kafka-1            | 	control.plane.listener.name = null
kafka-1            | 	controlled.shutdown.enable = true
kafka-1            | 	controlled.shutdown.max.retries = 3
kafka-1            | 	controlled.shutdown.retry.backoff.ms = 5000
kafka-1            | 	controller.listener.names = null
kafka-1            | 	controller.quorum.append.linger.ms = 25
kafka-1            | 	controller.quorum.election.backoff.max.ms = 1000
kafka-1            | 	controller.quorum.election.timeout.ms = 1000
kafka-1            | 	controller.quorum.fetch.timeout.ms = 2000
kafka-1            | 	controller.quorum.request.timeout.ms = 2000
kafka-1            | 	controller.quorum.retry.backoff.ms = 20
kafka-1            | 	controller.quorum.voters = []
kafka-1            | 	controller.quota.window.num = 11
kafka-1            | 	controller.quota.window.size.seconds = 1
kafka-1            | 	controller.socket.timeout.ms = 30000
kafka-1            | 	create.topic.policy.class.name = null
kafka-1            | 	default.replication.factor = 1
kafka-1            | 	delegation.token.expiry.check.interval.ms = 3600000
kafka-1            | 	delegation.token.expiry.time.ms = 86400000
kafka-1            | 	delegation.token.master.key = null
kafka-1            | 	delegation.token.max.lifetime.ms = 604800000
kafka-1            | 	delegation.token.secret.key = null
kafka-1            | 	delete.records.purgatory.purge.interval.requests = 1
kafka-1            | 	delete.topic.enable = true
kafka-1            | 	fetch.max.bytes = 57671680
kafka-1            | 	fetch.purgatory.purge.interval.requests = 1000
kafka-1            | 	group.initial.rebalance.delay.ms = 0
kafka-1            | 	group.max.session.timeout.ms = 1800000
kafka-1            | 	group.max.size = 2147483647
kafka-1            | 	group.min.session.timeout.ms = 6000
kafka-1            | 	initial.broker.registration.timeout.ms = 60000
kafka-1            | 	inter.broker.listener.name = PLAINTEXT
kafka-1            | 	inter.broker.protocol.version = 3.0-IV1
kafka-1            | 	kafka.metrics.polling.interval.secs = 10
kafka-1            | 	kafka.metrics.reporters = []
kafka-1            | 	leader.imbalance.check.interval.seconds = 300
kafka-1            | 	leader.imbalance.per.broker.percentage = 10
kafka-1            | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
kafka-1            | 	listeners = PLAINTEXT://0.0.0.0:9092
kafka-1            | 	log.cleaner.backoff.ms = 15000
kafka-1            | 	log.cleaner.dedupe.buffer.size = 134217728
kafka-1            | 	log.cleaner.delete.retention.ms = 86400000
kafka-1            | 	log.cleaner.enable = true
kafka-1            | 	log.cleaner.io.buffer.load.factor = 0.9
kafka-1            | 	log.cleaner.io.buffer.size = 524288
kafka-1            | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka-1            | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka-1            | 	log.cleaner.min.cleanable.ratio = 0.5
kafka-1            | 	log.cleaner.min.compaction.lag.ms = 0
kafka-1            | 	log.cleaner.threads = 1
kafka-1            | 	log.cleanup.policy = [delete]
kafka-1            | 	log.dir = /tmp/kafka-logs
kafka-1            | 	log.dirs = /var/lib/kafka/data
kafka-1            | 	log.flush.interval.messages = 9223372036854775807
kafka-1            | 	log.flush.interval.ms = null
kafka-1            | 	log.flush.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.flush.scheduler.interval.ms = 9223372036854775807
kafka-1            | 	log.flush.start.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.index.interval.bytes = 4096
kafka-1            | 	log.index.size.max.bytes = 10485760
kafka-1            | 	log.message.downconversion.enable = true
kafka-1            | 	log.message.format.version = 3.0-IV1
kafka-1            | 	log.message.timestamp.difference.max.ms = 9223372036854775807
kafka-1            | 	log.message.timestamp.type = CreateTime
kafka-1            | 	log.preallocate = false
kafka-1            | 	log.retention.bytes = -1
kafka-1            | 	log.retention.check.interval.ms = 300000
kafka-1            | 	log.retention.hours = 168
kafka-1            | 	log.retention.minutes = null
kafka-1            | 	log.retention.ms = null
kafka-1            | 	log.roll.hours = 168
kafka-1            | 	log.roll.jitter.hours = 0
kafka-1            | 	log.roll.jitter.ms = null
kafka-1            | 	log.roll.ms = null
kafka-1            | 	log.segment.bytes = 1073741824
kafka-1            | 	log.segment.delete.delay.ms = 60000
kafka-1            | 	max.connection.creation.rate = 2147483647
kafka-1            | 	max.connections = 2147483647
kafka-1            | 	max.connections.per.ip = 2147483647
kafka-1            | 	max.connections.per.ip.overrides = 
kafka-1            | 	max.incremental.fetch.session.cache.slots = 1000
kafka-1            | 	message.max.bytes = 1048588
kafka-1            | 	metadata.log.dir = null
kafka-1            | 	metadata.log.max.record.bytes.between.snapshots = 20971520
kafka-1            | 	metadata.log.segment.bytes = 1073741824
kafka-1            | 	metadata.log.segment.min.bytes = 8388608
kafka-1            | 	metadata.log.segment.ms = 604800000
kafka-1            | 	metadata.max.retention.bytes = -1
kafka-1            | 	metadata.max.retention.ms = 604800000
kafka-1            | 	metric.reporters = []
kafka-1            | 	metrics.num.samples = 2
kafka-1            | 	metrics.recording.level = INFO
kafka-1            | 	metrics.sample.window.ms = 30000
kafka-1            | 	min.insync.replicas = 1
kafka-1            | 	node.id = -1
kafka-1            | 	num.io.threads = 8
kafka-1            | 	num.network.threads = 3
kafka-1            | 	num.partitions = 1
kafka-1            | 	num.recovery.threads.per.data.dir = 1
kafka-1            | 	num.replica.alter.log.dirs.threads = null
kafka-1            | 	num.replica.fetchers = 1
kafka-1            | 	offset.metadata.max.bytes = 4096
kafka-1            | 	offsets.commit.required.acks = -1
kafka-1            | 	offsets.commit.timeout.ms = 5000
kafka-1            | 	offsets.load.buffer.size = 5242880
kafka-1            | 	offsets.retention.check.interval.ms = 600000
kafka-1            | 	offsets.retention.minutes = 10080
kafka-1            | 	offsets.topic.compression.codec = 0
kafka-1            | 	offsets.topic.num.partitions = 50
kafka-1            | 	offsets.topic.replication.factor = 1
kafka-1            | 	offsets.topic.segment.bytes = 104857600
kafka-1            | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka-1            | 	password.encoder.iterations = 4096
kafka-1            | 	password.encoder.key.length = 128
kafka-1            | 	password.encoder.keyfactory.algorithm = null
kafka-1            | 	password.encoder.old.secret = null
kafka-1            | 	password.encoder.secret = null
kafka-1            | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka-1            | 	process.roles = []
kafka-1            | 	producer.purgatory.purge.interval.requests = 1000
kafka-1            | 	queued.max.request.bytes = -1
kafka-1            | 	queued.max.requests = 500
kafka-1            | 	quota.window.num = 11
kafka-1            | 	quota.window.size.seconds = 1
kafka-1            | 	remote.log.index.file.cache.total.size.bytes = 1073741824
kafka-1            | 	remote.log.manager.task.interval.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.max.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.ms = 500
kafka-1            | 	remote.log.manager.task.retry.jitter = 0.2
kafka-1            | 	remote.log.manager.thread.pool.size = 10
kafka-1            | 	remote.log.metadata.manager.class.name = null
kafka-1            | 	remote.log.metadata.manager.class.path = null
kafka-1            | 	remote.log.metadata.manager.impl.prefix = null
kafka-1            | 	remote.log.metadata.manager.listener.name = null
kafka-1            | 	remote.log.reader.max.pending.tasks = 100
kafka-1            | 	remote.log.reader.threads = 10
kafka-1            | 	remote.log.storage.manager.class.name = null
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð add access controls to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:44:12.643Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:44:12.765Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:44:37.807Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-461c0a3b-2a4a-45b2-809c-6ebdeeeac3c1","leaderId":"kafkajs-461c0a3b-2a4a-45b2-809c-6ebdeeeac3c1","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":25042}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | npm notice
product-service-1  | npm notice New major version of npm available! 10.8.2 -> 11.10.1
product-service-1  | npm notice Changelog: https://github.com/npm/cli/releases/tag/v11.10.1
product-service-1  | npm notice To update run: npm install -g npm@11.10.1
product-service-1  | npm notice
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð add access controls to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:46:09.656Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	remote.log.storage.manager.class.path = null
kafka-1            | 	remote.log.storage.manager.impl.prefix = null
kafka-1            | 	remote.log.storage.system.enable = false
kafka-1            | 	replica.fetch.backoff.ms = 1000
kafka-1            | 	replica.fetch.max.bytes = 1048576
kafka-1            | 	replica.fetch.min.bytes = 1
kafka-1            | 	replica.fetch.response.max.bytes = 10485760
kafka-1            | 	replica.fetch.wait.max.ms = 500
kafka-1            | 	replica.high.watermark.checkpoint.interval.ms = 5000
kafka-1            | 	replica.lag.time.max.ms = 30000
kafka-1            | 	replica.selector.class = null
kafka-1            | 	replica.socket.receive.buffer.bytes = 65536
kafka-1            | 	replica.socket.timeout.ms = 30000
kafka-1            | 	replication.quota.window.num = 11
kafka-1            | 	replication.quota.window.size.seconds = 1
kafka-1            | 	request.timeout.ms = 30000
kafka-1            | 	reserved.broker.max.id = 1000
kafka-1            | 	sasl.client.callback.handler.class = null
kafka-1            | 	sasl.enabled.mechanisms = [GSSAPI]
kafka-1            | 	sasl.jaas.config = null
kafka-1            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka-1            | 	sasl.kerberos.min.time.before.relogin = 60000
kafka-1            | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka-1            | 	sasl.kerberos.service.name = null
kafka-1            | 	sasl.kerberos.ticket.renew.jitter = 0.05
kafka-1            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
kafka-1            | 	sasl.login.callback.handler.class = null
kafka-1            | 	sasl.login.class = null
kafka-1            | 	sasl.login.refresh.buffer.seconds = 300
kafka-1            | 	sasl.login.refresh.min.period.seconds = 60
kafka-1            | 	sasl.login.refresh.window.factor = 0.8
kafka-1            | 	sasl.login.refresh.window.jitter = 0.05
kafka-1            | 	sasl.mechanism.controller.protocol = GSSAPI
kafka-1            | 	sasl.mechanism.inter.broker.protocol = GSSAPI
kafka-1            | 	sasl.server.callback.handler.class = null
kafka-1            | 	security.inter.broker.protocol = PLAINTEXT
kafka-1            | 	security.providers = null
kafka-1            | 	socket.connection.setup.timeout.max.ms = 30000
kafka-1            | 	socket.connection.setup.timeout.ms = 10000
kafka-1            | 	socket.receive.buffer.bytes = 102400
kafka-1            | 	socket.request.max.bytes = 104857600
kafka-1            | 	socket.send.buffer.bytes = 102400
kafka-1            | 	ssl.cipher.suites = []
kafka-1            | 	ssl.client.auth = none
kafka-1            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka-1            | 	ssl.endpoint.identification.algorithm = https
kafka-1            | 	ssl.engine.factory.class = null
kafka-1            | 	ssl.key.password = null
kafka-1            | 	ssl.keymanager.algorithm = SunX509
kafka-1            | 	ssl.keystore.certificate.chain = null
kafka-1            | 	ssl.keystore.key = null
kafka-1            | 	ssl.keystore.location = null
kafka-1            | 	ssl.keystore.password = null
kafka-1            | 	ssl.keystore.type = JKS
kafka-1            | 	ssl.principal.mapping.rules = DEFAULT
kafka-1            | 	ssl.protocol = TLSv1.3
kafka-1            | 	ssl.provider = null
kafka-1            | 	ssl.secure.random.implementation = null
kafka-1            | 	ssl.trustmanager.algorithm = PKIX
kafka-1            | 	ssl.truststore.certificates = null
kafka-1            | 	ssl.truststore.location = null
kafka-1            | 	ssl.truststore.password = null
kafka-1            | 	ssl.truststore.type = JKS
kafka-1            | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
kafka-1            | 	transaction.max.timeout.ms = 900000
kafka-1            | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
kafka-1            | 	transaction.state.log.load.buffer.size = 5242880
kafka-1            | 	transaction.state.log.min.isr = 2
kafka-1            | 	transaction.state.log.num.partitions = 50
kafka-1            | 	transaction.state.log.replication.factor = 3
kafka-1            | 	transaction.state.log.segment.bytes = 104857600
kafka-1            | 	transactional.id.expiration.ms = 604800000
kafka-1            | 	unclean.leader.election.enable = false
kafka-1            | 	zookeeper.clientCnxnSocket = null
kafka-1            | 	zookeeper.connect = zookeeper:2181
kafka-1            | 	zookeeper.connection.timeout.ms = null
kafka-1            | 	zookeeper.max.in.flight.requests = 10
kafka-1            | 	zookeeper.session.timeout.ms = 18000
kafka-1            | 	zookeeper.set.acl = false
kafka-1            | 	zookeeper.ssl.cipher.suites = null
kafka-1            | 	zookeeper.ssl.client.enable = false
kafka-1            | 	zookeeper.ssl.crl.enable = false
kafka-1            | 	zookeeper.ssl.enabled.protocols = null
kafka-1            | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka-1            | 	zookeeper.ssl.keystore.location = null
kafka-1            | 	zookeeper.ssl.keystore.password = null
kafka-1            | 	zookeeper.ssl.keystore.type = null
kafka-1            | 	zookeeper.ssl.ocsp.enable = false
kafka-1            | 	zookeeper.ssl.protocol = TLSv1.2
kafka-1            | 	zookeeper.ssl.truststore.location = null
kafka-1            | 	zookeeper.ssl.truststore.password = null
kafka-1            | 	zookeeper.ssl.truststore.type = null
kafka-1            | 	zookeeper.sync.time.ms = 2000
kafka-1            |  (kafka.server.KafkaConfig)
kafka-1            | [2026-02-25 11:33:27,539] INFO KafkaConfig values: 
kafka-1            | 	advertised.listeners = PLAINTEXT://kafka:9092
kafka-1            | 	alter.config.policy.class.name = null
kafka-1            | 	alter.log.dirs.replication.quota.window.num = 11
kafka-1            | 	alter.log.dirs.replication.quota.window.size.seconds = 1
kafka-1            | 	authorizer.class.name = 
kafka-1            | 	auto.create.topics.enable = true
kafka-1            | 	auto.leader.rebalance.enable = true
kafka-1            | 	background.threads = 10
kafka-1            | 	broker.heartbeat.interval.ms = 2000
kafka-1            | 	broker.id = 1
kafka-1            | 	broker.id.generation.enable = true
kafka-1            | 	broker.rack = null
kafka-1            | 	broker.session.timeout.ms = 9000
kafka-1            | 	client.quota.callback.class = null
kafka-1            | 	compression.type = producer
kafka-1            | 	connection.failed.authentication.delay.ms = 100
kafka-1            | 	connections.max.idle.ms = 600000
kafka-1            | 	connections.max.reauth.ms = 0
kafka-1            | 	control.plane.listener.name = null
kafka-1            | 	controlled.shutdown.enable = true
kafka-1            | 	controlled.shutdown.max.retries = 3
kafka-1            | 	controlled.shutdown.retry.backoff.ms = 5000
kafka-1            | 	controller.listener.names = null
kafka-1            | 	controller.quorum.append.linger.ms = 25
kafka-1            | 	controller.quorum.election.backoff.max.ms = 1000
kafka-1            | 	controller.quorum.election.timeout.ms = 1000
kafka-1            | 	controller.quorum.fetch.timeout.ms = 2000
kafka-1            | 	controller.quorum.request.timeout.ms = 2000
kafka-1            | 	controller.quorum.retry.backoff.ms = 20
kafka-1            | 	controller.quorum.voters = []
kafka-1            | 	controller.quota.window.num = 11
kafka-1            | 	controller.quota.window.size.seconds = 1
kafka-1            | 	controller.socket.timeout.ms = 30000
kafka-1            | 	create.topic.policy.class.name = null
kafka-1            | 	default.replication.factor = 1
kafka-1            | 	delegation.token.expiry.check.interval.ms = 3600000
kafka-1            | 	delegation.token.expiry.time.ms = 86400000
kafka-1            | 	delegation.token.master.key = null
kafka-1            | 	delegation.token.max.lifetime.ms = 604800000
kafka-1            | 	delegation.token.secret.key = null
kafka-1            | 	delete.records.purgatory.purge.interval.requests = 1
kafka-1            | 	delete.topic.enable = true
kafka-1            | 	fetch.max.bytes = 57671680
kafka-1            | 	fetch.purgatory.purge.interval.requests = 1000
kafka-1            | 	group.initial.rebalance.delay.ms = 0
kafka-1            | 	group.max.session.timeout.ms = 1800000
kafka-1            | 	group.max.size = 2147483647
kafka-1            | 	group.min.session.timeout.ms = 6000
kafka-1            | 	initial.broker.registration.timeout.ms = 60000
kafka-1            | 	inter.broker.listener.name = PLAINTEXT
kafka-1            | 	inter.broker.protocol.version = 3.0-IV1
kafka-1            | 	kafka.metrics.polling.interval.secs = 10
kafka-1            | 	kafka.metrics.reporters = []
kafka-1            | 	leader.imbalance.check.interval.seconds = 300
kafka-1            | 	leader.imbalance.per.broker.percentage = 10
kafka-1            | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
kafka-1            | 	listeners = PLAINTEXT://0.0.0.0:9092
kafka-1            | 	log.cleaner.backoff.ms = 15000
kafka-1            | 	log.cleaner.dedupe.buffer.size = 134217728
kafka-1            | 	log.cleaner.delete.retention.ms = 86400000
kafka-1            | 	log.cleaner.enable = true
kafka-1            | 	log.cleaner.io.buffer.load.factor = 0.9
kafka-1            | 	log.cleaner.io.buffer.size = 524288
kafka-1            | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka-1            | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka-1            | 	log.cleaner.min.cleanable.ratio = 0.5
kafka-1            | 	log.cleaner.min.compaction.lag.ms = 0
kafka-1            | 	log.cleaner.threads = 1
kafka-1            | 	log.cleanup.policy = [delete]
kafka-1            | 	log.dir = /tmp/kafka-logs
kafka-1            | 	log.dirs = /var/lib/kafka/data
kafka-1            | 	log.flush.interval.messages = 9223372036854775807
kafka-1            | 	log.flush.interval.ms = null
kafka-1            | 	log.flush.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.flush.scheduler.interval.ms = 9223372036854775807
kafka-1            | 	log.flush.start.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.index.interval.bytes = 4096
kafka-1            | 	log.index.size.max.bytes = 10485760
kafka-1            | 	log.message.downconversion.enable = true
kafka-1            | 	log.message.format.version = 3.0-IV1
kafka-1            | 	log.message.timestamp.difference.max.ms = 9223372036854775807
kafka-1            | 	log.message.timestamp.type = CreateTime
kafka-1            | 	log.preallocate = false
kafka-1            | 	log.retention.bytes = -1
kafka-1            | 	log.retention.check.interval.ms = 300000
kafka-1            | 	log.retention.hours = 168
kafka-1            | 	log.retention.minutes = null
kafka-1            | 	log.retention.ms = null
kafka-1            | 	log.roll.hours = 168
kafka-1            | 	log.roll.jitter.hours = 0
kafka-1            | 	log.roll.jitter.ms = null
kafka-1            | 	log.roll.ms = null
kafka-1            | 	log.segment.bytes = 1073741824
kafka-1            | 	log.segment.delete.delay.ms = 60000
kafka-1            | 	max.connection.creation.rate = 2147483647
kafka-1            | 	max.connections = 2147483647
kafka-1            | 	max.connections.per.ip = 2147483647
kafka-1            | 	max.connections.per.ip.overrides = 
kafka-1            | 	max.incremental.fetch.session.cache.slots = 1000
kafka-1            | 	message.max.bytes = 1048588
kafka-1            | 	metadata.log.dir = null
kafka-1            | 	metadata.log.max.record.bytes.between.snapshots = 20971520
kafka-1            | 	metadata.log.segment.bytes = 1073741824
kafka-1            | 	metadata.log.segment.min.bytes = 8388608
kafka-1            | 	metadata.log.segment.ms = 604800000
kafka-1            | 	metadata.max.retention.bytes = -1
kafka-1            | 	metadata.max.retention.ms = 604800000
kafka-1            | 	metric.reporters = []
kafka-1            | 	metrics.num.samples = 2
kafka-1            | 	metrics.recording.level = INFO
kafka-1            | 	metrics.sample.window.ms = 30000
kafka-1            | 	min.insync.replicas = 1
kafka-1            | 	node.id = -1
kafka-1            | 	num.io.threads = 8
kafka-1            | 	num.network.threads = 3
kafka-1            | 	num.partitions = 1
kafka-1            | 	num.recovery.threads.per.data.dir = 1
kafka-1            | 	num.replica.alter.log.dirs.threads = null
kafka-1            | 	num.replica.fetchers = 1
kafka-1            | 	offset.metadata.max.bytes = 4096
kafka-1            | 	offsets.commit.required.acks = -1
kafka-1            | 	offsets.commit.timeout.ms = 5000
kafka-1            | 	offsets.load.buffer.size = 5242880
kafka-1            | 	offsets.retention.check.interval.ms = 600000
kafka-1            | 	offsets.retention.minutes = 10080
kafka-1            | 	offsets.topic.compression.codec = 0
kafka-1            | 	offsets.topic.num.partitions = 50
kafka-1            | 	offsets.topic.replication.factor = 1
kafka-1            | 	offsets.topic.segment.bytes = 104857600
kafka-1            | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka-1            | 	password.encoder.iterations = 4096
kafka-1            | 	password.encoder.key.length = 128
kafka-1            | 	password.encoder.keyfactory.algorithm = null
kafka-1            | 	password.encoder.old.secret = null
kafka-1            | 	password.encoder.secret = null
kafka-1            | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka-1            | 	process.roles = []
kafka-1            | 	producer.purgatory.purge.interval.requests = 1000
kafka-1            | 	queued.max.request.bytes = -1
kafka-1            | 	queued.max.requests = 500
kafka-1            | 	quota.window.num = 11
kafka-1            | 	quota.window.size.seconds = 1
kafka-1            | 	remote.log.index.file.cache.total.size.bytes = 1073741824
kafka-1            | 	remote.log.manager.task.interval.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.max.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.ms = 500
kafka-1            | 	remote.log.manager.task.retry.jitter = 0.2
kafka-1            | 	remote.log.manager.thread.pool.size = 10
kafka-1            | 	remote.log.metadata.manager.class.name = null
kafka-1            | 	remote.log.metadata.manager.class.path = null
kafka-1            | 	remote.log.metadata.manager.impl.prefix = null
kafka-1            | 	remote.log.metadata.manager.listener.name = null
kafka-1            | 	remote.log.reader.max.pending.tasks = 100
kafka-1            | 	remote.log.reader.threads = 10
kafka-1            | 	remote.log.storage.manager.class.name = null
kafka-1            | 	remote.log.storage.manager.class.path = null
kafka-1            | 	remote.log.storage.manager.impl.prefix = null
kafka-1            | 	remote.log.storage.system.enable = false
kafka-1            | 	replica.fetch.backoff.ms = 1000
kafka-1            | 	replica.fetch.max.bytes = 1048576
kafka-1            | 	replica.fetch.min.bytes = 1
kafka-1            | 	replica.fetch.response.max.bytes = 10485760
kafka-1            | 	replica.fetch.wait.max.ms = 500
kafka-1            | 	replica.high.watermark.checkpoint.interval.ms = 5000
kafka-1            | 	replica.lag.time.max.ms = 30000
kafka-1            | 	replica.selector.class = null
kafka-1            | 	replica.socket.receive.buffer.bytes = 65536
kafka-1            | 	replica.socket.timeout.ms = 30000
kafka-1            | 	replication.quota.window.num = 11
kafka-1            | 	replication.quota.window.size.seconds = 1
kafka-1            | 	request.timeout.ms = 30000
kafka-1            | 	reserved.broker.max.id = 1000
kafka-1            | 	sasl.client.callback.handler.class = null
kafka-1            | 	sasl.enabled.mechanisms = [GSSAPI]
kafka-1            | 	sasl.jaas.config = null
kafka-1            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka-1            | 	sasl.kerberos.min.time.before.relogin = 60000
kafka-1            | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka-1            | 	sasl.kerberos.service.name = null
kafka-1            | 	sasl.kerberos.ticket.renew.jitter = 0.05
kafka-1            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
kafka-1            | 	sasl.login.callback.handler.class = null
kafka-1            | 	sasl.login.class = null
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:41:42 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:41:42.979Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:41:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:41:49.518Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:41:50.281Z"}
order-service-1    | {"level":"info","message":"::ffff:173.194.208.207 - - [25/Feb/2026:11:41:50 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"curl/8.7.1\"","timestamp":"2026-02-25T11:41:50.283Z"}
grafana-1          | logger=settings t=2026-02-25T11:40:01.455367922Z level=info msg="Starting Grafana" version=12.2.1 commit=563109b696e9c1cbaf345f2ab7a11f7f78422982 branch=release-12.2.1 compiled=2026-02-25T11:40:01Z
grafana-1          | logger=settings t=2026-02-25T11:40:01.455676964Z level=info msg="Config loaded from" file=/usr/share/grafana/conf/defaults.ini
grafana-1          | logger=settings t=2026-02-25T11:40:01.455689672Z level=info msg="Config loaded from" file=/etc/grafana/grafana.ini
grafana-1          | logger=settings t=2026-02-25T11:40:01.455692089Z level=info msg="Config overridden from command line" arg="default.paths.data=/var/lib/grafana"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455693756Z level=info msg="Config overridden from command line" arg="default.paths.logs=/var/log/grafana"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455695297Z level=info msg="Config overridden from command line" arg="default.paths.plugins=/var/lib/grafana/plugins"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455696839Z level=info msg="Config overridden from command line" arg="default.paths.provisioning=/etc/grafana/provisioning"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455699297Z level=info msg="Config overridden from command line" arg="default.log.mode=console"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455701006Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_DATA=/var/lib/grafana"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455702672Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_LOGS=/var/log/grafana"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455704797Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PLUGINS=/var/lib/grafana/plugins"
grafana-1          | logger=settings t=2026-02-25T11:40:01.455706381Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PROVISIONING=/etc/grafana/provisioning"
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:41:53.138Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:41:53 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:41:53.140Z"}
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:46:09.770Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:46:38.337Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-87d3b683-ad23-4c0b-bf51-d2e676f8fbbf","leaderId":"kafkajs-87d3b683-ad23-4c0b-bf51-d2e676f8fbbf","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28566}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:42:03.246Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:42:03 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:42:03.247Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:42:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:42:04.501Z"}
grafana-1          | logger=settings t=2026-02-25T11:40:01.455708881Z level=info msg=Target target=[all]
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:42:13.317Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:42:13 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:42:13.317Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:42:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:42:19.660Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:42:23.433Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:42:23 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:42:23.435Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:42:33.590Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:42:33 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:42:33.594Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:42:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:42:34.499Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:42:43.717Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:42:43 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:42:43.723Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:42:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:42:49.601Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:42:53.812Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:42:53 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:42:53.813Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:43:03.912Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:43:03 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:43:03.914Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:43:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:43:04.481Z"}
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
grafana-1          | logger=settings t=2026-02-25T11:40:01.455714172Z level=info msg="Path Home" path=/usr/share/grafana
grafana-1          | logger=settings t=2026-02-25T11:40:01.455717172Z level=info msg="Path Data" path=/var/lib/grafana
grafana-1          | logger=settings t=2026-02-25T11:40:01.455719464Z level=info msg="Path Logs" path=/var/log/grafana
grafana-1          | logger=settings t=2026-02-25T11:40:01.455764797Z level=info msg="Path Plugins" path=/var/lib/grafana/plugins
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:43:14.069Z"}
kafka-1            | 	sasl.login.refresh.buffer.seconds = 300
kafka-1            | 	sasl.login.refresh.min.period.seconds = 60
kafka-1            | 	sasl.login.refresh.window.factor = 0.8
kafka-1            | 	sasl.login.refresh.window.jitter = 0.05
kafka-1            | 	sasl.mechanism.controller.protocol = GSSAPI
kafka-1            | 	sasl.mechanism.inter.broker.protocol = GSSAPI
kafka-1            | 	sasl.server.callback.handler.class = null
kafka-1            | 	security.inter.broker.protocol = PLAINTEXT
kafka-1            | 	security.providers = null
kafka-1            | 	socket.connection.setup.timeout.max.ms = 30000
kafka-1            | 	socket.connection.setup.timeout.ms = 10000
kafka-1            | 	socket.receive.buffer.bytes = 102400
kafka-1            | 	socket.request.max.bytes = 104857600
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:43:14 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:43:14.071Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:43:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:43:19.629Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:43:24.199Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:43:24 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:43:24.202Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:43:34.322Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:43:34 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:43:34.324Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:43:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:43:34.483Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:43:44.423Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:43:44 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:43:44.424Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:43:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:43:49.518Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:43:54.516Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:43:54 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:43:54.517Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:44:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:44:04.483Z"}
kafka-1            | 	socket.send.buffer.bytes = 102400
kafka-1            | 	ssl.cipher.suites = []
kafka-1            | 	ssl.client.auth = none
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:44:04.582Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:44:04 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:44:04.583Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:44:14.696Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:44:14 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:44:14.699Z"}
kafka-1            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka-1            | 	ssl.endpoint.identification.algorithm = https
kafka-1            | 	ssl.engine.factory.class = null
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:44:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:44:19.686Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:44:24.803Z"}
kafka-1            | 	ssl.key.password = null
kafka-1            | 	ssl.keymanager.algorithm = SunX509
kafka-1            | 	ssl.keystore.certificate.chain = null
kafka-1            | 	ssl.keystore.key = null
kafka-1            | 	ssl.keystore.location = null
kafka-1            | 	ssl.keystore.password = null
kafka-1            | 	ssl.keystore.type = JKS
grafana-1          | logger=settings t=2026-02-25T11:40:01.455766464Z level=info msg="Path Provisioning" path=/etc/grafana/provisioning
grafana-1          | logger=settings t=2026-02-25T11:40:01.455768006Z level=info msg="App mode production"
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:44:24 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:44:24.803Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:44:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:44:34.491Z"}
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:44:34.907Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:44:34 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:44:34.907Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:44:35.785Z"}
kafka-1            | 	ssl.principal.mapping.rules = DEFAULT
kafka-1            | 	ssl.protocol = TLSv1.3
kafka-1            | 	ssl.provider = null
order-service-1    | {"level":"info","message":"::ffff:173.194.208.207 - - [25/Feb/2026:11:44:35 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"curl/8.7.1\"","timestamp":"2026-02-25T11:44:35.786Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:44:45.015Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:44:45 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:44:45.017Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:44:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:44:49.488Z"}
kafka-1            | 	ssl.secure.random.implementation = null
grafana-1          | logger=featuremgmt t=2026-02-25T11:40:01.456073131Z level=info msg=FeatureToggles lokiQuerySplitting=true grafanaconThemes=true alertingMigrationUI=true newFiltersUI=true awsAsyncQueryCaching=true groupToNestedTableTransformation=true unifiedStorageHistoryPruner=true cloudWatchRoundUpEndTime=true improvedExternalSessionHandlingSAML=true adhocFiltersInTooltips=true annotationPermissionUpdate=true panelMonitoring=true publicDashboardsScene=true logsExploreTableVisualisation=true alertingRuleVersionHistoryRestore=true tlsMemcached=true dashboardSceneSolo=true azureMonitorPrometheusExemplars=true promQLScope=true alertingRulePermanentlyDelete=true skipTokenRotationIfRecent=true cloudWatchNewLabelParsing=true alertingRuleRecoverDeleted=true newDashboardSharingComponent=true logsContextDatasourceUi=true alertingNotificationsStepMode=true grafanaAssistantInProfilesDrilldown=true addFieldFromCalculationStatFunctions=true azureMonitorEnableUserAuth=true alertingQueryAndExpressionsStepMode=true transformationsRedesign=true alertingUIOptimizeReducer=true dashboardDsAdHocFiltering=true alertingImportYAMLUI=true ssoSettingsLDAP=true alertingSaveStateCompressed=true kubernetesDashboards=true alertingBulkActionsInUI=true alertRuleRestore=true cloudWatchCrossAccountQuerying=true newPDFRendering=true logRowsPopoverMenu=true useSessionStorageForRedirection=true pinNavItems=true dashboardSceneForViewers=true unifiedRequestLog=true lokiLabelNamesQueryApi=true awsDatasourcesTempCredentials=true onPremToCloudMigrations=true formatString=true improvedExternalSessionHandling=true preinstallAutoUpdate=true correlations=true dashgpt=true dashboardScene=true dataplaneFrontendFallback=true logsPanelControls=true recordedQueriesMulti=true logsInfiniteScrolling=true influxdbBackendMigration=true prometheusAzureOverrideAudience=true
grafana-1          | logger=sqlstore t=2026-02-25T11:40:01.456117506Z level=info msg="Connecting to DB" dbtype=sqlite3
grafana-1          | logger=sqlstore t=2026-02-25T11:40:01.456132547Z level=info msg="Creating SQLite database file" path=/var/lib/grafana/grafana.db
grafana-1          | logger=migrator t=2026-02-25T11:40:01.462976089Z level=info msg="Locking database"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.463002089Z level=info msg="Starting DB migrations"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.463505506Z level=info msg="Executing migration" id="create migration_log table"
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:44:55.103Z"}
kafka-1            | 	ssl.trustmanager.algorithm = PKIX
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:44:55 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:44:55.104Z"}
kafka-1            | 	ssl.truststore.certificates = null
kafka-1            | 	ssl.truststore.location = null
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:45:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:45:04.512Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:45:05.211Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:45:05 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:45:05.215Z"}
kafka-1            | 	ssl.truststore.password = null
kafka-1            | 	ssl.truststore.type = JKS
kafka-1            | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
kafka-1            | 	transaction.max.timeout.ms = 900000
kafka-1            | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:45:15.316Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:45:15 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:45:15.317Z"}
kafka-1            | 	transaction.state.log.load.buffer.size = 5242880
kafka-1            | 	transaction.state.log.min.isr = 2
kafka-1            | 	transaction.state.log.num.partitions = 50
kafka-1            | 	transaction.state.log.replication.factor = 3
kafka-1            | 	transaction.state.log.segment.bytes = 104857600
kafka-1            | 	transactional.id.expiration.ms = 604800000
grafana-1          | logger=migrator t=2026-02-25T11:40:01.464483131Z level=info msg="Migration successfully executed" id="create migration_log table" duration=976.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.468002756Z level=info msg="Executing migration" id="create user table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.468648631Z level=info msg="Migration successfully executed" id="create user table" duration=648.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.472375506Z level=info msg="Executing migration" id="add unique index user.login"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.473057589Z level=info msg="Migration successfully executed" id="add unique index user.login" duration=682.334Âµs
kafka-1            | 	unclean.leader.election.enable = false
kafka-1            | 	zookeeper.clientCnxnSocket = null
kafka-1            | 	zookeeper.connect = zookeeper:2181
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:45:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:45:19.489Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:45:25.424Z"}
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
grafana-1          | logger=migrator t=2026-02-25T11:40:01.476685339Z level=info msg="Executing migration" id="add unique index user.email"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.477243047Z level=info msg="Migration successfully executed" id="add unique index user.email" duration=558.667Âµs
kafka-1            | 	zookeeper.connection.timeout.ms = null
kafka-1            | 	zookeeper.max.in.flight.requests = 10
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:45:25 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:45:25.425Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:45:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:45:34.482Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:45:35.521Z"}
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:45:35 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:45:35.522Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:45:45.590Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:45:45 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:45:45.590Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:45:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:45:49.487Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:45:55.720Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:45:55 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:45:55.721Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:46:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:46:04.490Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:46:05.888Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:46:05 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:46:05.890Z"}
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
kafka-1            | 	zookeeper.session.timeout.ms = 18000
kafka-1            | 	zookeeper.set.acl = false
kafka-1            | 	zookeeper.ssl.cipher.suites = null
kafka-1            | 	zookeeper.ssl.client.enable = false
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:46:16.004Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:46:16 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:46:16.005Z"}
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
kafka-1            | 	zookeeper.ssl.crl.enable = false
kafka-1            | 	zookeeper.ssl.enabled.protocols = null
kafka-1            | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka-1            | 	zookeeper.ssl.keystore.location = null
kafka-1            | 	zookeeper.ssl.keystore.password = null
kafka-1            | 	zookeeper.ssl.keystore.type = null
kafka-1            | 	zookeeper.ssl.ocsp.enable = false
grafana-1          | logger=migrator t=2026-02-25T11:40:01.484350047Z level=info msg="Executing migration" id="drop index UQE_user_login - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.485083964Z level=info msg="Migration successfully executed" id="drop index UQE_user_login - v1" duration=734.209Âµs
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
grafana-1          | logger=migrator t=2026-02-25T11:40:01.490067714Z level=info msg="Executing migration" id="drop index UQE_user_email - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.490740297Z level=info msg="Migration successfully executed" id="drop index UQE_user_email - v1" duration=674.292Âµs
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:46:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:46:19.681Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:46:26.085Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:46:26 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:46:26.085Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.493851589Z level=info msg="Executing migration" id="Rename table user to user_v1 - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.495424922Z level=info msg="Migration successfully executed" id="Rename table user to user_v1 - v1" duration=1.573083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.501984881Z level=info msg="Executing migration" id="create user table v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.503215714Z level=info msg="Migration successfully executed" id="create user table v2" duration=1.231167ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.511241214Z level=info msg="Executing migration" id="create index UQE_user_login - v2"
kafka-1            | 	zookeeper.ssl.protocol = TLSv1.2
kafka-1            | 	zookeeper.ssl.truststore.location = null
kafka-1            | 	zookeeper.ssl.truststore.password = null
kafka-1            | 	zookeeper.ssl.truststore.type = null
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:46:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:46:34.482Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:46:36.178Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:46:36 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:46:36.179Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.511926672Z level=info msg="Migration successfully executed" id="create index UQE_user_login - v2" duration=687.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.516284047Z level=info msg="Executing migration" id="create index UQE_user_email - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.516925256Z level=info msg="Migration successfully executed" id="create index UQE_user_email - v2" duration=642.25Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:46:46.275Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:46:46 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:46:46.276Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:46:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:46:49.489Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:46:56.363Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:46:56 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:46:56.363Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:47:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:47:04.483Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:47:06.435Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:47:06 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:47:06.435Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:47:16.525Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:47:16 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:47:16.526Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:47:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:47:19.487Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:47:26.609Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:47:26 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:47:26.610Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:47:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:47:34.493Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:47:36.739Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:47:36 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:47:36.740Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:47:46.832Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:47:46 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:47:46.833Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:47:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:47:49.520Z"}
kafka-1            | 	zookeeper.sync.time.ms = 2000
kafka-1            |  (kafka.server.KafkaConfig)
kafka-1            | [2026-02-25 11:33:27,610] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 11:33:27,611] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 11:33:27,613] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 11:33:27,615] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 11:33:27,660] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:33:27,664] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:33:27,671] INFO Loaded 0 logs in 11ms. (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:33:27,671] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:33:27,675] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | 
kafka-1            | [2026-02-25 11:33:27,685] INFO Starting the log cleaner (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 11:33:27,884] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 11:33:28,122] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:47:56.899Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:47:56 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:47:56.900Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:48:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:48:04.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:48:06.999Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:48:07 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:48:07.000Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:48:17.089Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:48:17 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:48:17.090Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:48:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:48:19.565Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:48:27.190Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:48:27 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:48:27.191Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:48:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:48:34.482Z"}
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:48:37.293Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:48:37 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:48:37.294Z"}
kafka-1            | [2026-02-25 11:33:28,410] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
kafka-1            | [2026-02-25 11:33:28,414] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
kafka-1            | [2026-02-25 11:33:28,467] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka-1            | [2026-02-25 11:33:28,478] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.522521506Z level=info msg="Executing migration" id="copy data_source v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.522921297Z level=info msg="Migration successfully executed" id="copy data_source v1 to v2" duration=402.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.529275339Z level=info msg="Executing migration" id="Drop old table user_v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.530065006Z level=info msg="Migration successfully executed" id="Drop old table user_v1" duration=788.875Âµs
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð encrypt with Dotenvx: https://dotenvx.com
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:48:10.066Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
kafka-1            | [2026-02-25 11:33:28,570] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 11:33:28,574] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 11:33:28,576] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 11:33:28,577] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 11:33:28,600] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka-1            | [2026-02-25 11:33:28,683] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
kafka-1            | [2026-02-25 11:33:28,713] INFO Stat of the created znode at /brokers/ids/1 is: 27,27,1772019208702,1772019208702,1,0,0,72057714390532097,194,0,27
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
kafka-1            |  (kafka.zk.KafkaZkClient)
kafka-1            | [2026-02-25 11:33:28,715] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://kafka:9092, czxid (broker epoch): 27 (kafka.zk.KafkaZkClient)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:48:47.386Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:48:47 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:48:47.387Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.535512381Z level=info msg="Executing migration" id="Add column help_flags1 to user table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.536372131Z level=info msg="Migration successfully executed" id="Add column help_flags1 to user table" duration=860.292Âµs
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:48:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:48:49.519Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:48:57.484Z"}
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:48:57 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:48:57.485Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:49:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:49:04.486Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:49:07.578Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:49:07 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:49:07.579Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.542334422Z level=info msg="Executing migration" id="Update user table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.542366214Z level=info msg="Migration successfully executed" id="Update user table charset" duration=33.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.547270672Z level=info msg="Executing migration" id="Add last_seen_at column to user"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.548210297Z level=info msg="Migration successfully executed" id="Add last_seen_at column to user" duration=941Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.585424089Z level=info msg="Executing migration" id="Add missing user data"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.585699172Z level=info msg="Migration successfully executed" id="Add missing user data" duration=286.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.613365714Z level=info msg="Executing migration" id="Add is_disabled column to user"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.614324422Z level=info msg="Migration successfully executed" id="Add is_disabled column to user" duration=959.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.617120964Z level=info msg="Executing migration" id="Add index user.login/user.email"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.617691172Z level=info msg="Migration successfully executed" id="Add index user.login/user.email" duration=569.917Âµs
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:49:17.679Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:49:17 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:49:17.680Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:49:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:49:19.537Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:49:27.763Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:49:27 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:49:27.764Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:49:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:49:34.508Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:49:37.905Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:49:37 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:49:37.906Z"}
kafka-1            | [2026-02-25 11:33:28,779] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
kafka-1            | [2026-02-25 11:33:28,792] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:49:48.015Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:49:48 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:49:48.017Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:49:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:49:49.489Z"}
kafka-1            | [2026-02-25 11:33:28,804] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 11:33:28,805] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 11:33:28,807] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
kafka-1            | [2026-02-25 11:33:28,822] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:28,828] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:33:28,830] INFO [Controller id=1] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:28,836] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:33:28,836] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
kafka-1            | [2026-02-25 11:33:28,870] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
kafka-1            | [2026-02-25 11:33:28,874] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache)
kafka-1            | [2026-02-25 11:33:28,874] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:28,881] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
kafka-1            | [2026-02-25 11:33:28,883] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.619401297Z level=info msg="Executing migration" id="Add is_service_account column to user"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.620303464Z level=info msg="Migration successfully executed" id="Add is_service_account column to user" duration=902.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.621529797Z level=info msg="Executing migration" id="Update is_service_account column to nullable"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.627215464Z level=info msg="Migration successfully executed" id="Update is_service_account column to nullable" duration=5.682667ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.631942047Z level=info msg="Executing migration" id="Add uid column to user"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.632951922Z level=info msg="Migration successfully executed" id="Add uid column to user" duration=1.010208ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.634423381Z level=info msg="Executing migration" id="Update uid column values for users"
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:49:58.188Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:49:58 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:49:58.191Z"}
kafka-1            | [2026-02-25 11:33:28,888] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:28,893] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:28,897] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:28,919] INFO [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 27) (kafka.controller.KafkaController)
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 11:33:28,922] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 11:33:28,926] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:28,966] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:50:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:50:04.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:50:08.323Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:50:08 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:50:08.324Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:50:18.396Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:50:18 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:50:18.397Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:50:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:50:19.742Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:50:28.479Z"}
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:50:28 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:50:28.480Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:50:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:50:34.481Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:50:38.564Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:50:38 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:50:38.565Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:50:48.664Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:50:48 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:50:48.665Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:50:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:50:49.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:50:58.763Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:50:58 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:50:58.766Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:51:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:51:04.494Z"}
kafka-1            | [2026-02-25 11:33:28,975] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
kafka-1            | [2026-02-25 11:33:29,000] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Starting socket server acceptors and processors (kafka.network.SocketServer)
kafka-1            | [2026-02-25 11:33:29,005] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:51:08.863Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:51:08 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:51:08.864Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:51:18.968Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:51:18 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:51:18.969Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.634583839Z level=info msg="Migration successfully executed" id="Update uid column values for users" duration=161.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.636277839Z level=info msg="Executing migration" id="Make sure users uid are set"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.636438714Z level=info msg="Migration successfully executed" id="Make sure users uid are set" duration=160.541Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.638240839Z level=info msg="Executing migration" id="Add unique index user_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.638751631Z level=info msg="Migration successfully executed" id="Add unique index user_uid" duration=511.334Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.640042797Z level=info msg="Executing migration" id="Add is_provisioned column to user"
kafka-1            | [2026-02-25 11:33:29,009] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,011] INFO [Controller id=1] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,013] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka-1            | [2026-02-25 11:33:29,014] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started socket server acceptors and processors (kafka.network.SocketServer)
kafka-1            | [2026-02-25 11:33:29,014] INFO [Controller id=1] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:51:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:51:19.485Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:51:29.065Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:51:29 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:51:29.066Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:51:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:51:34.493Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:51:39.143Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:51:39 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:51:39.144Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:51:49.237Z"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:01.640937881Z level=info msg="Migration successfully executed" id="Add is_provisioned column to user" duration=890.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.642366506Z level=info msg="Executing migration" id="update login field with orgid to allow for multiple service accounts with same name across orgs"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.642632881Z level=info msg="Migration successfully executed" id="update login field with orgid to allow for multiple service accounts with same name across orgs" duration=266.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.644127839Z level=info msg="Executing migration" id="update service accounts login field orgid to appear only once"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.644514089Z level=info msg="Migration successfully executed" id="update service accounts login field orgid to appear only once" duration=386.708Âµs
kafka-1            | [2026-02-25 11:33:29,015] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,025] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,028] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,028] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,030] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
kafka-1            | [2026-02-25 11:33:29,046] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,047] INFO Kafka version: 7.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
kafka-1            | [2026-02-25 11:33:29,048] INFO Kafka commitId: b7e52413e7cb3e8b (org.apache.kafka.common.utils.AppInfoParser)
kafka-1            | [2026-02-25 11:33:29,048] INFO Kafka startTimeMs: 1772019209014 (org.apache.kafka.common.utils.AppInfoParser)
kafka-1            | [2026-02-25 11:33:29,059] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.646266881Z level=info msg="Executing migration" id="update login and email fields to lowercase"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.646610131Z level=info msg="Migration successfully executed" id="update login and email fields to lowercase" duration=343.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.648103714Z level=info msg="Executing migration" id="update login and email fields to lowercase2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.648289506Z level=info msg="Migration successfully executed" id="update login and email fields to lowercase2" duration=185.958Âµs
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:51:49 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:51:49.238Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:51:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:51:49.489Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:51:59.349Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:51:59 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:51:59.350Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:52:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:52:04.478Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:52:09.444Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:52:09 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:52:09.445Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:52:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:52:19.494Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:52:19.575Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:52:19 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:52:19.576Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:52:29.673Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:52:29 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:52:29.673Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:52:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:52:34.499Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:52:39.790Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:52:39 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:52:39.791Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:52:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:52:49.499Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.649616964Z level=info msg="Executing migration" id="Add index on user.is_service_account and user.last_seen_at"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.650141714Z level=info msg="Migration successfully executed" id="Add index on user.is_service_account and user.last_seen_at" duration=525.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.652253464Z level=info msg="Executing migration" id="create temp user table v1-7"
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:52:49.903Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:52:49 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:52:49.905Z"}
order-service-1    | {"level":"info","message":"::ffff:173.194.208.207 - - [25/Feb/2026:11:52:51 +0000] \"POST /orders HTTP/1.1\" 400 36 \"-\" \"curl/8.7.1\"","timestamp":"2026-02-25T11:52:51.521Z"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 11:33:29,061] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:33:29,093] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
kafka-1            | [2026-02-25 11:33:29,103] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
kafka-1            | [2026-02-25 11:33:29,103] INFO [RequestSendThread controllerId=1] Controller 1 connected to kafka:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
kafka-1            | [2026-02-25 11:33:29,105] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.653026672Z level=info msg="Migration successfully executed" id="create temp user table v1-7" duration=772.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.654646964Z level=info msg="Executing migration" id="create index IDX_temp_user_email - v1-7"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.655126797Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_email - v1-7" duration=479.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.656501464Z level=info msg="Executing migration" id="create index IDX_temp_user_org_id - v1-7"
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 11:33:29,105] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
kafka-1            | [2026-02-25 11:33:29,106] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
kafka-1            | [2026-02-25 11:33:29,106] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
kafka-1            | [2026-02-25 11:33:29,108] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
kafka-1            | [2026-02-25 11:33:29,109] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,118] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,119] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,119] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,119] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,125] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,139] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:29,145] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 0 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2026-02-25 11:33:29,195] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use broker kafka:9092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 11:33:29,196] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Recorded new controller, from now on will use broker kafka:9092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 11:33:34,153] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:33:34,158] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:38:34,183] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:52:59.987Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:52:59 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:52:59.988Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:53:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:53:04.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:53:10.065Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:53:10 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:53:10.066Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:53:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:53:19.663Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:53:20.179Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:53:20 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:53:20.180Z"}
kafka-1            | [2026-02-25 11:38:34,185] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:40:08,265] INFO Creating topic order-events with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.656993797Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_org_id - v1-7" duration=492.666Âµs
kafka-1            | [2026-02-25 11:40:08,319] INFO [Controller id=1] New topics: [Set(order-events)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(order-events,Some(iDN5fkDbT_uD5RIK7tfZOQ),Map(order-events-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:40:08,320] INFO [Controller id=1] New partition creation callback for order-events-0 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:40:08,322] INFO [Controller id=1 epoch=1] Changed partition order-events-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.659199047Z level=info msg="Executing migration" id="create index IDX_temp_user_code - v1-7"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.659680964Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_code - v1-7" duration=482.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.660987506Z level=info msg="Executing migration" id="create index IDX_temp_user_status - v1-7"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.661479631Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_status - v1-7" duration=492.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.663345006Z level=info msg="Executing migration" id="Update temp_user table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.663366131Z level=info msg="Migration successfully executed" id="Update temp_user table charset" duration=22.041Âµs
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:01.665279881Z level=info msg="Executing migration" id="drop index IDX_temp_user_email - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.665882089Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_email - v1" duration=602.667Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.667783214Z level=info msg="Executing migration" id="drop index IDX_temp_user_org_id - v1"
kafka-1            | [2026-02-25 11:40:08,323] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,332] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition order-events-0 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,332] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,370] INFO [Controller id=1 epoch=1] Changed partition order-events-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.668277422Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_org_id - v1" duration=499.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.670248297Z level=info msg="Executing migration" id="drop index IDX_temp_user_code - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.670672464Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_code - v1" duration=425.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.671948714Z level=info msg="Executing migration" id="drop index IDX_temp_user_status - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.672361006Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_status - v1" duration=412.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.673993922Z level=info msg="Executing migration" id="Rename table temp_user to temp_user_tmp_qwerty - v1"
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:53:30.281Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:53:30 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:53:30.282Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:53:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:53:34.498Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:53:40.404Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:53:40 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:53:40.406Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:53:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:53:49.529Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:53:50.498Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.675794506Z level=info msg="Migration successfully executed" id="Rename table temp_user to temp_user_tmp_qwerty - v1" duration=1.801ms
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:53:50 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:53:50.498Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:54:00.582Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:54:00 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:54:00.584Z"}
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:54:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:54:04.479Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:54:10.670Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:54:10 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:54:10.671Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:54:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:54:19.596Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.677080214Z level=info msg="Executing migration" id="create temp_user v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.677532047Z level=info msg="Migration successfully executed" id="create temp_user v2" duration=451.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.678826881Z level=info msg="Executing migration" id="create index IDX_temp_user_email - v2"
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:54:20.831Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:54:20 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:54:20.833Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:54:30.926Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:54:30 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:54:30.927Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:54:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:54:34.504Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:54:41.026Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:54:41 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:54:41.027Z"}
order-service-1    | Executing (87af0168-dbfc-4abe-9584-9a0dffbadf3e): START TRANSACTION;
order-service-1    | Executing (87af0168-dbfc-4abe-9584-9a0dffbadf3e): INSERT INTO "Orders" ("id","userId","status","createdAt","updatedAt") VALUES (DEFAULT,$1,$2,$3,$4) RETURNING "id","userId","status","createdAt","updatedAt";
order-service-1    | Executing (87af0168-dbfc-4abe-9584-9a0dffbadf3e): INSERT INTO "OrderItems" ("id","productId","quantity","createdAt","updatedAt","OrderId") VALUES (DEFAULT,$1,$2,$3,$4,$5) RETURNING "id","productId","quantity","createdAt","updatedAt","OrderId";
order-service-1    | Executing (87af0168-dbfc-4abe-9584-9a0dffbadf3e): ROLLBACK;
kafka-1            | [2026-02-25 11:40:08,373] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='order-events', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition order-events-0 (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,373] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,377] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,379] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition order-events-0 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,380] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,387] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 for 1 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,388] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='order-events', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,411] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition order-events-0 (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,413] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(order-events-0) (kafka.server.ReplicaFetcherManager)
kafka-1            | [2026-02-25 11:40:08,413] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
order-service-1    | {"error":"Invalid URL","items":[{"productId":1,"quantity":1}],"level":"error","message":"Order creation error","stack":"TypeError [ERR_INVALID_URL]: Invalid URL\n    at new NodeError (node:internal/errors:405:5)\n    at new URL (node:internal/url:676:13)\n    at dispatchHttpRequest (/app/node_modules/axios/dist/node/axios.cjs:2925:20)\n    at /app/node_modules/axios/dist/node/axios.cjs:2845:5\n    at new Promise (<anonymous>)\n    at wrapAsync (/app/node_modules/axios/dist/node/axios.cjs:2825:10)\n    at http (/app/node_modules/axios/dist/node/axios.cjs:2863:10)\n    at Axios.dispatchRequest (/app/node_modules/axios/dist/node/axios.cjs:4323:10)\n    at Axios._request (/app/node_modules/axios/dist/node/axios.cjs:4621:33)\n    at Axios.request (/app/node_modules/axios/dist/node/axios.cjs:4478:25)\n    at Axios.request (/app/node_modules/axios/dist/node/axios.cjs:4483:41)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async /app/index.js:129:32","timestamp":"2026-02-25T11:54:48.406Z","userId":1}
order-service-1    | {"level":"info","message":"::ffff:173.194.208.207 - - [25/Feb/2026:11:54:48 +0000] \"POST /orders HTTP/1.1\" 400 56 \"-\" \"curl/8.7.1\"","timestamp":"2026-02-25T11:54:48.408Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:54:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:54:49.498Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:54:51.135Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:54:51 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:54:51.135Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:55:01.237Z"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:48:10.166Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.679133714Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_email - v2" duration=307.666Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.681055922Z level=info msg="Executing migration" id="create index IDX_temp_user_org_id - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.681472881Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_org_id - v2" duration=417.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.682914756Z level=info msg="Executing migration" id="create index IDX_temp_user_code - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.683378964Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_code - v2" duration=464.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.685393672Z level=info msg="Executing migration" id="create index IDX_temp_user_status - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.685940381Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_status - v2" duration=548.667Âµs
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:55:01 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:55:01.238Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:55:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:55:04.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:55:11.325Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.687324131Z level=info msg="Executing migration" id="copy temp_user v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.687566756Z level=info msg="Migration successfully executed" id="copy temp_user v1 to v2" duration=243.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.689727131Z level=info msg="Executing migration" id="drop temp_user_tmp_qwerty"
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:55:11 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:55:11.326Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:55:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:55:19.653Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:55:21.417Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:55:21 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:55:21.418Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:55:31.510Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:55:31 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:55:31.511Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.690146172Z level=info msg="Migration successfully executed" id="drop temp_user_tmp_qwerty" duration=419.666Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.691567006Z level=info msg="Executing migration" id="Set created for temp users that will otherwise prematurely expire"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.691869589Z level=info msg="Migration successfully executed" id="Set created for temp users that will otherwise prematurely expire" duration=302.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.693341214Z level=info msg="Executing migration" id="create star table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.693709297Z level=info msg="Migration successfully executed" id="create star table" duration=368.292Âµs
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:48:38.904Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-af7c0c7b-481f-4c09-827f-32dbfb29a571","leaderId":"kafkajs-af7c0c7b-481f-4c09-827f-32dbfb29a571","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28737}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:55:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:55:34.487Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:55:41.610Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:55:41 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:55:41.611Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:55:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:55:49.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:55:51.708Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:55:51 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:55:51.709Z"}
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:56:01.789Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:56:01 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:56:01.790Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:56:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:56:04.491Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:56:11.884Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:56:11 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:56:11.885Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:56:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:56:19.508Z"}
kafka-1            | [2026-02-25 11:40:08,506] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:08,522] INFO Created log for partition order-events-0 in /var/lib/kafka/data/order-events-0 with properties {} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:08,525] INFO [Partition order-events-0 broker=1] No checkpointed highwatermark is found for partition order-events-0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:08,526] INFO [Partition order-events-0 broker=1] Log loaded for partition order-events-0 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:08,552] INFO [Broker id=1] Leader order-events-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,727] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition order-events-0 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.695423297Z level=info msg="Executing migration" id="add unique index star.user_id_dashboard_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.696038464Z level=info msg="Migration successfully executed" id="add unique index star.user_id_dashboard_id" duration=615.709Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:56:22.015Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:56:22 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:56:22.016Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:56:32.108Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:56:32 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:56:32.109Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:56:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:56:34.492Z"}
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.697394089Z level=info msg="Executing migration" id="Add column dashboard_uid in star"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.698274964Z level=info msg="Migration successfully executed" id="Add column dashboard_uid in star" duration=879.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.699887714Z level=info msg="Executing migration" id="Add column org_id in star"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.700768422Z level=info msg="Migration successfully executed" id="Add column org_id in star" duration=880.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.702822089Z level=info msg="Executing migration" id="Add column updated in star"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.704143131Z level=info msg="Migration successfully executed" id="Add column updated in star" duration=1.317375ms
product-service-1  |     at async start (/app/index.js:538:5) {
grafana-1          | logger=migrator t=2026-02-25T11:40:01.706783464Z level=info msg="Executing migration" id="add index in star table on dashboard_uid, org_id and user_id columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.707298672Z level=info msg="Migration successfully executed" id="add index in star table on dashboard_uid, org_id and user_id columns" duration=516.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.709494256Z level=info msg="Executing migration" id="create org table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.709992506Z level=info msg="Migration successfully executed" id="create org table v1" duration=498.584Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.711388256Z level=info msg="Executing migration" id="create index UQE_org_name - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.711837297Z level=info msg="Migration successfully executed" id="create index UQE_org_name - v1" duration=449Âµs
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:56:42.181Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:56:42 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:56:42.182Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.713708839Z level=info msg="Executing migration" id="create org_user table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.714093547Z level=info msg="Migration successfully executed" id="create org_user table v1" duration=385.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.715709756Z level=info msg="Executing migration" id="create index IDX_org_user_org_id - v1"
kafka-1            | [2026-02-25 11:40:08,765] INFO [Broker id=1] Finished LeaderAndIsr request in 376ms correlationId 1 from controller 1 for 1 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,801] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=iDN5fkDbT_uD5RIK7tfZOQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 1 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,817] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='order-events', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition order-events-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.716195297Z level=info msg="Migration successfully executed" id="create index IDX_org_user_org_id - v1" duration=482.708Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.717423797Z level=info msg="Executing migration" id="create index UQE_org_user_org_id_user_id - v1"
kafka-1            | [2026-02-25 11:40:08,819] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
kafka-1            | [2026-02-25 11:40:08,821] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 2 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,456] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1), 25 -> ArrayBuffer(1), 26 -> ArrayBuffer(1), 27 -> ArrayBuffer(1), 28 -> ArrayBuffer(1), 29 -> ArrayBuffer(1), 30 -> ArrayBuffer(1), 31 -> ArrayBuffer(1), 32 -> ArrayBuffer(1), 33 -> ArrayBuffer(1), 34 -> ArrayBuffer(1), 35 -> ArrayBuffer(1), 36 -> ArrayBuffer(1), 37 -> ArrayBuffer(1), 38 -> ArrayBuffer(1), 39 -> ArrayBuffer(1), 40 -> ArrayBuffer(1), 41 -> ArrayBuffer(1), 42 -> ArrayBuffer(1), 43 -> ArrayBuffer(1), 44 -> ArrayBuffer(1), 45 -> ArrayBuffer(1), 46 -> ArrayBuffer(1), 47 -> ArrayBuffer(1), 48 -> ArrayBuffer(1), 49 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
kafka-1            | [2026-02-25 11:40:09,477] INFO [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(88z2T5O8T7KV0KsEEMBkmQ),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:56:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:56:49.498Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:56:52.278Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:56:52 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:56:52.279Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:57:02.385Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:57:02 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:57:02.385Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:57:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:57:04.488Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:57:12.494Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:57:12 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:57:12.495Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:57:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:57:19.480Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:57:22.583Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:57:22 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:57:22.584Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:57:32.659Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:57:32 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:57:32.660Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:57:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:57:34.478Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:57:42.754Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.717863047Z level=info msg="Migration successfully executed" id="create index UQE_org_user_org_id_user_id - v1" duration=439.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.719450464Z level=info msg="Executing migration" id="create index IDX_org_user_user_id - v1"
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:57:42 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:57:42.755Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:57:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:57:49.496Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:57:52.870Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:57:52 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:57:52.871Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:58:02.990Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:58:02 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:58:02.992Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:58:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:58:04.490Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:58:13.073Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:58:13 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:58:13.074Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:58:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:58:19.489Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:58:23.188Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:58:23 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:58:23.189Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:58:33.324Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:58:33 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:58:33.325Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:58:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:58:34.492Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:58:43.446Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.719882381Z level=info msg="Migration successfully executed" id="create index IDX_org_user_user_id - v1" duration=431.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.721201089Z level=info msg="Executing migration" id="Update org table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.721217464Z level=info msg="Migration successfully executed" id="Update org table charset" duration=17.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.722722547Z level=info msg="Executing migration" id="Update org_user table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.722746131Z level=info msg="Migration successfully executed" id="Update org_user table charset" duration=24.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.724563797Z level=info msg="Executing migration" id="Migrate all Read Only Viewers to Viewers"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.724772547Z level=info msg="Migration successfully executed" id="Migrate all Read Only Viewers to Viewers" duration=209.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.726085256Z level=info msg="Executing migration" id="create dashboard table"
kafka-1            | [2026-02-25 11:40:09,478] INFO [Controller id=1] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:58:43 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:58:43.451Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:58:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:58:49.499Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:58:53.544Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:58:53 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:58:53.545Z"}
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:59:03.659Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:59:03 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:59:03.660Z"}
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
product-service-1  |   [cause]: undefined
product-service-1  | }
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:59:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:59:04.483Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:59:13.767Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:59:13 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:59:13.769Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:59:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:59:19.480Z"}
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:59:23.861Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:59:23 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:59:23.862Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:59:33.981Z"}
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.726697756Z level=info msg="Migration successfully executed" id="create dashboard table" duration=613.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.728115756Z level=info msg="Executing migration" id="add index dashboard.account_id"
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:59:33 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:59:33.983Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:59:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:59:34.494Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:59:44.093Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:59:44 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:59:44.094Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:11:59:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T11:59:49.511Z"}
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð prevent committing .env to code: https://dotenvx.com/precommit
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:50:10.670Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T11:59:54.209Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:11:59:54 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T11:59:54.210Z"}
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,479] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:00:04.312Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:00:04 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:00:04.313Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:00:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:00:04.487Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:00:14.390Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:00:14 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:00:14.392Z"}
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.728572381Z level=info msg="Migration successfully executed" id="add index dashboard.account_id" duration=456.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.729771256Z level=info msg="Executing migration" id="add unique index dashboard_account_id_slug"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.730211547Z level=info msg="Migration successfully executed" id="add unique index dashboard_account_id_slug" duration=440.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.731570089Z level=info msg="Executing migration" id="create dashboard_tag table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.732014214Z level=info msg="Migration successfully executed" id="create dashboard_tag table" duration=441.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.733739589Z level=info msg="Executing migration" id="add unique index dashboard_tag.dasboard_id_term"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.734179089Z level=info msg="Migration successfully executed" id="add unique index dashboard_tag.dasboard_id_term" duration=439.334Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.735575756Z level=info msg="Executing migration" id="drop index UQE_dashboard_tag_dashboard_id_term - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.735980256Z level=info msg="Migration successfully executed" id="drop index UQE_dashboard_tag_dashboard_id_term - v1" duration=404.584Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.737267339Z level=info msg="Executing migration" id="Rename table dashboard to dashboard_v1 - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.739952131Z level=info msg="Migration successfully executed" id="Rename table dashboard to dashboard_v1 - v1" duration=2.683417ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.741306922Z level=info msg="Executing migration" id="create dashboard v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.741739381Z level=info msg="Migration successfully executed" id="create dashboard v2" duration=431.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.742831964Z level=info msg="Executing migration" id="create index IDX_dashboard_org_id - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.743247172Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_org_id - v2" duration=415.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.745102006Z level=info msg="Executing migration" id="create index UQE_dashboard_org_id_slug - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.745517047Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_org_id_slug - v2" duration=415.167Âµs
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:01.747436714Z level=info msg="Executing migration" id="copy dashboard v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.747896672Z level=info msg="Migration successfully executed" id="copy dashboard v1 to v2" duration=459.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.750102839Z level=info msg="Executing migration" id="drop table dashboard_v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.751249381Z level=info msg="Migration successfully executed" id="drop table dashboard_v1" duration=1.143833ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.753896422Z level=info msg="Executing migration" id="alter dashboard.data to mediumtext v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.753914131Z level=info msg="Migration successfully executed" id="alter dashboard.data to mediumtext v1" duration=19.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.756550089Z level=info msg="Executing migration" id="Add column updated_by in dashboard - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.758139381Z level=info msg="Migration successfully executed" id="Add column updated_by in dashboard - v2" duration=1.590125ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.760053089Z level=info msg="Executing migration" id="Add column created_by in dashboard - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.761243881Z level=info msg="Migration successfully executed" id="Add column created_by in dashboard - v2" duration=1.190959ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.762691881Z level=info msg="Executing migration" id="Add column gnetId in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.763749047Z level=info msg="Migration successfully executed" id="Add column gnetId in dashboard" duration=1.057166ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.765154881Z level=info msg="Executing migration" id="Add index for gnetId in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.765666422Z level=info msg="Migration successfully executed" id="Add index for gnetId in dashboard" duration=511.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.767124839Z level=info msg="Executing migration" id="Add column plugin_id in dashboard"
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,480] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:00:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:00:19.489Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:00:24.488Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:00:24 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:00:24.489Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:00:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:00:34.495Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:00:34.575Z"}
kafka-1            | [2026-02-25 11:40:09,481] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,481] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,481] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,481] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,481] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.768123922Z level=info msg="Migration successfully executed" id="Add column plugin_id in dashboard" duration=999.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.769526172Z level=info msg="Executing migration" id="Add index for plugin_id in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.769889881Z level=info msg="Migration successfully executed" id="Add index for plugin_id in dashboard" duration=363.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.771535922Z level=info msg="Executing migration" id="Add index for dashboard_id in dashboard_tag"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.771864714Z level=info msg="Migration successfully executed" id="Add index for dashboard_id in dashboard_tag" duration=328.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.773214631Z level=info msg="Executing migration" id="Update dashboard table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.773237297Z level=info msg="Migration successfully executed" id="Update dashboard table charset" duration=21.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.775716256Z level=info msg="Executing migration" id="Update dashboard_tag table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.775744089Z level=info msg="Migration successfully executed" id="Update dashboard_tag table charset" duration=27.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.778099381Z level=info msg="Executing migration" id="Add column folder_id in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.779485172Z level=info msg="Migration successfully executed" id="Add column folder_id in dashboard" duration=1.385625ms
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:00:34 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:00:34.576Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:00:44.687Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:00:44 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:00:44.688Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:00:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:00:49.489Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:00:54.809Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:00:54 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:00:54.811Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:01:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:01:04.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:01:04.893Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:01:04 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:01:04.893Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.780782297Z level=info msg="Executing migration" id="Add column isFolder in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.781999172Z level=info msg="Migration successfully executed" id="Add column isFolder in dashboard" duration=1.216292ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.783210547Z level=info msg="Executing migration" id="Add column has_acl in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.784180631Z level=info msg="Migration successfully executed" id="Add column has_acl in dashboard" duration=969.916Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:01:14.987Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:01:14 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:01:14.987Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:01:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:01:19.480Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.786705256Z level=info msg="Executing migration" id="Add column uid in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.787733964Z level=info msg="Migration successfully executed" id="Add column uid in dashboard" duration=1.028917ms
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:01:25.097Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:01:25 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:01:25.098Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:01:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:01:34.493Z"}
kafka-1            | [2026-02-25 11:40:09,484] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,484] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,484] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,484] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,484] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2026-02-25T11:40:01.789188172Z level=info msg="Executing migration" id="Update uid column values in dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.789314547Z level=info msg="Migration successfully executed" id="Update uid column values in dashboard" duration=126.917Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:01:35.195Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:01:35 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:01:35.195Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:01:45.301Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:01:45 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:01:45.303Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:01:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:01:49.491Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:01:55.383Z"}
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:01:55 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:01:55.384Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:02:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:02:04.490Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:02:05.491Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:02:05 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:02:05.492Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:02:15.579Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.790460089Z level=info msg="Executing migration" id="Add unique index dashboard_org_id_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.790887172Z level=info msg="Migration successfully executed" id="Add unique index dashboard_org_id_uid" duration=426.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.791914631Z level=info msg="Executing migration" id="Remove unique index org_id_slug"
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:02:15 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:02:15.580Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:02:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:02:19.489Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:02:25.689Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:02:25 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:02:25.690Z"}
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=migrator t=2026-02-25T11:40:01.792234547Z level=info msg="Migration successfully executed" id="Remove unique index org_id_slug" duration=322.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.793298797Z level=info msg="Executing migration" id="Update dashboard title length"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.793311214Z level=info msg="Migration successfully executed" id="Update dashboard title length" duration=12.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.794284214Z level=info msg="Executing migration" id="Add unique index for dashboard_org_id_title_folder_id"
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:02:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:02:34.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:02:35.783Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:02:35 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:02:35.784Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:02:45.896Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:02:45 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:02:45.897Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:02:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:02:49.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:02:56.060Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:02:56 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:02:56.061Z"}
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.794620464Z level=info msg="Migration successfully executed" id="Add unique index for dashboard_org_id_title_folder_id" duration=336.416Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.795598839Z level=info msg="Executing migration" id="create dashboard_provisioning"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.795999672Z level=info msg="Migration successfully executed" id="create dashboard_provisioning" duration=400.625Âµs
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:03:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:03:04.492Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:03:06.173Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:03:06 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:03:06.174Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:03:16.265Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:03:16 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:03:16.265Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:03:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:03:19.498Z"}
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:03:26.358Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:03:26 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:03:26.359Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:03:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:03:34.491Z"}
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:50:10.766Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.797127881Z level=info msg="Executing migration" id="Rename table dashboard_provisioning to dashboard_provisioning_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.799785839Z level=info msg="Migration successfully executed" id="Rename table dashboard_provisioning to dashboard_provisioning_tmp_qwerty - v1" duration=2.657083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.801639047Z level=info msg="Executing migration" id="create dashboard_provisioning v2"
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:03:36.444Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:03:36 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:03:36.444Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:03:46.547Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:03:46 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:03:46.548Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:03:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:03:49.491Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:03:56.633Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:03:56 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:03:56.634Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:04:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:04:04.504Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:04:06.714Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:04:06 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:04:06.715Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:04:16.810Z"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:50:39.689Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-36e6cb21-18f1-472a-bb36-cab263e728cf","leaderId":"kafkajs-36e6cb21-18f1-472a-bb36-cab263e728cf","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28923}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,485] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,486] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,486] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,486] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:04:16 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:04:16.811Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:04:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:04:19.483Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:04:26.932Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:04:26 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:04:26.934Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:04:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:04:34.488Z"}
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:04:37.038Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:04:37 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:04:37.039Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:04:47.107Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:04:47 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:04:47.107Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:04:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:04:49.491Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:04:57.205Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:04:57 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:04:57.205Z"}
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:01.801946881Z level=info msg="Migration successfully executed" id="create dashboard_provisioning v2" duration=307.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.803017089Z level=info msg="Executing migration" id="create index IDX_dashboard_provisioning_dashboard_id - v2"
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:05:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:05:04.486Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:05:07.296Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:05:07 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:05:07.297Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:05:17.429Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:05:17 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:05:17.431Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:05:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:05:19.494Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:05:27.516Z"}
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
kafka-1            | [2026-02-25 11:40:09,487] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:05:27 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:05:27.519Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:05:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:05:34.493Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:05:37.635Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:05:37 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:05:37.636Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:05:47.743Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:05:47 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:05:47.744Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:05:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:05:49.500Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.803326589Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_provisioning_dashboard_id - v2" duration=309.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.804411214Z level=info msg="Executing migration" id="create index IDX_dashboard_provisioning_dashboard_id_name - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.804781006Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_provisioning_dashboard_id_name - v2" duration=369.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.806141922Z level=info msg="Executing migration" id="copy dashboard_provisioning v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.806296256Z level=info msg="Migration successfully executed" id="copy dashboard_provisioning v1 to v2" duration=154.75Âµs
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
grafana-1          | logger=migrator t=2026-02-25T11:40:01.807194922Z level=info msg="Executing migration" id="drop dashboard_provisioning_tmp_qwerty"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.807438881Z level=info msg="Migration successfully executed" id="drop dashboard_provisioning_tmp_qwerty" duration=244.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.808570256Z level=info msg="Executing migration" id="Add check_sum column"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.809574256Z level=info msg="Migration successfully executed" id="Add check_sum column" duration=1.002917ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.810601131Z level=info msg="Executing migration" id="Add index for dashboard_title"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.810881047Z level=info msg="Migration successfully executed" id="Add index for dashboard_title" duration=280.125Âµs
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,488] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,745] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,745] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,745] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.811866756Z level=info msg="Executing migration" id="delete tags for deleted dashboards"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.811934089Z level=info msg="Migration successfully executed" id="delete tags for deleted dashboards" duration=67.584Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.812808672Z level=info msg="Executing migration" id="delete stars for deleted dashboards"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.812870089Z level=info msg="Migration successfully executed" id="delete stars for deleted dashboards" duration=60.625Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:05:57.871Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:05:57 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:05:57.873Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:06:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:06:04.495Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:06:07.987Z"}
kafka-1            | [2026-02-25 11:40:09,745] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,745] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,745] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,745] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
grafana-1          | logger=migrator t=2026-02-25T11:40:01.813866339Z level=info msg="Executing migration" id="Add index for dashboard_is_folder"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.814169839Z level=info msg="Migration successfully executed" id="Add index for dashboard_is_folder" duration=304.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.815289756Z level=info msg="Executing migration" id="Add isPublic for dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.816400756Z level=info msg="Migration successfully executed" id="Add isPublic for dashboard" duration=1.111041ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.818103631Z level=info msg="Executing migration" id="Add deleted for dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.819331714Z level=info msg="Migration successfully executed" id="Add deleted for dashboard" duration=1.227791ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.820523547Z level=info msg="Executing migration" id="Add index for deleted"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.820876172Z level=info msg="Migration successfully executed" id="Add index for deleted" duration=351.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.822043172Z level=info msg="Executing migration" id="Add column dashboard_uid in dashboard_tag"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.823213464Z level=info msg="Migration successfully executed" id="Add column dashboard_uid in dashboard_tag" duration=1.17ms
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð encrypt with Dotenvx: https://dotenvx.com
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:52:11.367Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.824382381Z level=info msg="Executing migration" id="Add column org_id in dashboard_tag"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.825561422Z level=info msg="Migration successfully executed" id="Add column org_id in dashboard_tag" duration=1.179ms
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:06:07 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:06:07.988Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:06:18.082Z"}
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:06:18 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:06:18.083Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:06:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:06:19.482Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:06:28.185Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.826937047Z level=info msg="Executing migration" id="Add missing dashboard_uid and org_id to dashboard_tag"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.827145756Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid and org_id to dashboard_tag" duration=208.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.828124839Z level=info msg="Executing migration" id="Add apiVersion for dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.829263881Z level=info msg="Migration successfully executed" id="Add apiVersion for dashboard" duration=1.13875ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.830547339Z level=info msg="Executing migration" id="Add index for dashboard_uid on dashboard_tag table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.831046589Z level=info msg="Migration successfully executed" id="Add index for dashboard_uid on dashboard_tag table" duration=499Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.832537256Z level=info msg="Executing migration" id="Add missing dashboard_uid and org_id to star"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.832811381Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid and org_id to star" duration=274.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.833755797Z level=info msg="Executing migration" id="create data_source table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.834168881Z level=info msg="Migration successfully executed" id="create data_source table" duration=413.209Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.835348797Z level=info msg="Executing migration" id="add index data_source.account_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.835745214Z level=info msg="Migration successfully executed" id="add index data_source.account_id" duration=396.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.837182089Z level=info msg="Executing migration" id="add unique index data_source.account_id_name"
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
grafana-1          | logger=migrator t=2026-02-25T11:40:01.837581922Z level=info msg="Migration successfully executed" id="add unique index data_source.account_id_name" duration=400.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.838723714Z level=info msg="Executing migration" id="drop index IDX_data_source_account_id - v1"
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:06:28 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:06:28.186Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:06:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:06:34.486Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:06:38.364Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:06:38 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:06:38.365Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:06:48.492Z"}
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:06:48 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:06:48.494Z"}
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,746] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.839019922Z level=info msg="Migration successfully executed" id="drop index IDX_data_source_account_id - v1" duration=296.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.840150214Z level=info msg="Executing migration" id="drop index UQE_data_source_account_id_name - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.840538922Z level=info msg="Migration successfully executed" id="drop index UQE_data_source_account_id_name - v1" duration=389.291Âµs
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,747] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,748] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-13 (state.change.logger)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:06:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:06:49.490Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:06:58.577Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:06:58 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:06:58.577Z"}
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:07:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:07:04.490Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:07:08.697Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:07:08 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:07:08.698Z"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:01.841469214Z level=info msg="Executing migration" id="Rename table data_source to data_source_v1 - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.844855422Z level=info msg="Migration successfully executed" id="Rename table data_source to data_source_v1 - v1" duration=3.385625ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.846103881Z level=info msg="Executing migration" id="create data_source table v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.846576214Z level=info msg="Migration successfully executed" id="create data_source table v2" duration=472.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.847770006Z level=info msg="Executing migration" id="create index IDX_data_source_org_id - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.848116797Z level=info msg="Migration successfully executed" id="create index IDX_data_source_org_id - v2" duration=347.291Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.849309339Z level=info msg="Executing migration" id="create index UQE_data_source_org_id_name - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.849669547Z level=info msg="Migration successfully executed" id="create index UQE_data_source_org_id_name - v2" duration=359.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.850836256Z level=info msg="Executing migration" id="Drop old table data_source_v1 #2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.851076172Z level=info msg="Migration successfully executed" id="Drop old table data_source_v1 #2" duration=241.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.852196506Z level=info msg="Executing migration" id="Add column with_credentials"
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-46 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-9 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.853429672Z level=info msg="Migration successfully executed" id="Add column with_credentials" duration=1.233583ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.854336964Z level=info msg="Executing migration" id="Add secure json data column"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.855498047Z level=info msg="Migration successfully executed" id="Add secure json data column" duration=1.161458ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.856521172Z level=info msg="Executing migration" id="Update data_source table charset"
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:07:18.778Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:07:18 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:07:18.779Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:07:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:07:19.486Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:07:28.893Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:07:28 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:07:28.894Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:07:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:07:34.493Z"}
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-42 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-21 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-17 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-30 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-26 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-5 (state.change.logger)
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:07:39.001Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:07:39 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:07:39.003Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:07:49.096Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:07:49 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:07:49.097Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:07:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:07:49.492Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:07:59.186Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:07:59 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:07:59.187Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:08:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:08:04.484Z"}
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-38 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-34 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-16 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-45 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.856532339Z level=info msg="Migration successfully executed" id="Update data_source table charset" duration=11.459Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.857847506Z level=info msg="Executing migration" id="Update initial version to 1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.857931839Z level=info msg="Migration successfully executed" id="Update initial version to 1" duration=84.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.859047589Z level=info msg="Executing migration" id="Add read_only data column"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.860367506Z level=info msg="Migration successfully executed" id="Add read_only data column" duration=1.319666ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.862031297Z level=info msg="Executing migration" id="Migrate logging ds to loki ds"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.862129256Z level=info msg="Migration successfully executed" id="Migrate logging ds to loki ds" duration=98.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.863285714Z level=info msg="Executing migration" id="Update json_data with nulls"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.863366506Z level=info msg="Migration successfully executed" id="Update json_data with nulls" duration=81.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.864925131Z level=info msg="Executing migration" id="Add uid column"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.866362172Z level=info msg="Migration successfully executed" id="Add uid column" duration=1.438667ms
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:01.867832422Z level=info msg="Executing migration" id="Update uid value"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.867937047Z level=info msg="Migration successfully executed" id="Update uid value" duration=104.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.869360381Z level=info msg="Executing migration" id="Add unique index datasource_org_id_uid"
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:52:11.458Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:52:40.258Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-bcd59c78-5dad-48e8-bcaa-12966634cef2","leaderId":"kafkajs-bcd59c78-5dad-48e8-bcaa-12966634cef2","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28800}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:01.869769631Z level=info msg="Migration successfully executed" id="Add unique index datasource_org_id_uid" duration=408.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.870966672Z level=info msg="Executing migration" id="add unique index datasource_org_id_is_default"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.871377881Z level=info msg="Migration successfully executed" id="add unique index datasource_org_id_is_default" duration=411.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.872468672Z level=info msg="Executing migration" id="Add is_prunable column"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.873815089Z level=info msg="Migration successfully executed" id="Add is_prunable column" duration=1.345417ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.876068464Z level=info msg="Executing migration" id="Add api_version column"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.877448631Z level=info msg="Migration successfully executed" id="Add api_version column" duration=1.38025ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.884950714Z level=info msg="Executing migration" id="Update secure_json_data column to MediumText"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.884970672Z level=info msg="Migration successfully executed" id="Update secure_json_data column to MediumText" duration=21.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.886184964Z level=info msg="Executing migration" id="create api_key table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.886708422Z level=info msg="Migration successfully executed" id="create api_key table" duration=520.708Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:08:09.299Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:08:09 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:08:09.301Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:08:19.380Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:08:19 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:08:19.381Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:08:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:08:19.492Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.888190172Z level=info msg="Executing migration" id="add index api_key.account_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.888689047Z level=info msg="Migration successfully executed" id="add index api_key.account_id" duration=499.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.890700756Z level=info msg="Executing migration" id="add index api_key.key"
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:08:29.500Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:08:29 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:08:29.501Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:08:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:08:34.501Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:08:39.606Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:08:39 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:08:39.607Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:08:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:08:49.487Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:08:49.714Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:08:49 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:08:49.715Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:08:59.810Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:08:59 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:08:59.811Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:09:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:09:04.499Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.891380172Z level=info msg="Migration successfully executed" id="add index api_key.key" duration=680.584Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.893183006Z level=info msg="Executing migration" id="add index api_key.account_id_name"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.893797131Z level=info msg="Migration successfully executed" id="add index api_key.account_id_name" duration=614.583Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:09:09.909Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:09:09 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:09:09.911Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:09:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:09:19.490Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:09:20.114Z"}
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-12 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-41 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-24 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-20 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-49 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-0 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-29 (state.change.logger)
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:09:20 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:09:20.115Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:09:30.235Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:09:30 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:09:30.237Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:09:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:09:34.496Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:09:40.331Z"}
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:09:40 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:09:40.332Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:09:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:09:49.493Z"}
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-25 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-8 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.895927714Z level=info msg="Executing migration" id="drop index IDX_api_key_account_id - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.896500422Z level=info msg="Migration successfully executed" id="drop index IDX_api_key_account_id - v1" duration=573.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.899677381Z level=info msg="Executing migration" id="drop index UQE_api_key_key - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.900515464Z level=info msg="Migration successfully executed" id="drop index UQE_api_key_key - v1" duration=841.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.903487297Z level=info msg="Executing migration" id="drop index UQE_api_key_account_id_name - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.904114714Z level=info msg="Migration successfully executed" id="drop index UQE_api_key_account_id_name - v1" duration=631.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.905725089Z level=info msg="Executing migration" id="Rename table api_key to api_key_v1 - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.910280089Z level=info msg="Migration successfully executed" id="Rename table api_key to api_key_v1 - v1" duration=4.552709ms
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:09:50.396Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:09:50 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:09:50.396Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:10:00.499Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:10:00 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:10:00.499Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:10:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:10:04.484Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:10:10.600Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:10:10 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:10:10.601Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:10:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:10:19.494Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:10:20.747Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:10:20 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:10:20.749Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:10:30.829Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:10:30 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:10:30.830Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:10:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:10:34.496Z"}
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-37 (state.change.logger)
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:10:40.980Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:10:40 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:10:40.981Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:10:49 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:10:49.496Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:10:51.326Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:10:51 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:10:51.329Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:11:01.417Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:11:01 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:11:01.418Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:11:04 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:11:04.491Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:11:11.508Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:11:11 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:11:11.509Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:11:19 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:11:19.491Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:11:21.606Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:11:21 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:11:21.608Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.912335797Z level=info msg="Executing migration" id="create api_key table v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.912873839Z level=info msg="Migration successfully executed" id="create api_key table v2" duration=539.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.914631381Z level=info msg="Executing migration" id="create index IDX_api_key_org_id - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.915444381Z level=info msg="Migration successfully executed" id="create index IDX_api_key_org_id - v2" duration=813.5Âµs
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:11:31.730Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:11:31 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:11:31.731Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.917188964Z level=info msg="Executing migration" id="create index UQE_api_key_key - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.917702797Z level=info msg="Migration successfully executed" id="create index UQE_api_key_key - v2" duration=514.667Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.920214131Z level=info msg="Executing migration" id="create index UQE_api_key_org_id_name - v2"
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-33 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-15 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-48 (state.change.logger)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:12:11:34 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T12:11:34.487Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T12:11:42.886Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:12:11:42 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T12:11:42.890Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:13:39:12 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T13:39:12.438Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T13:39:16.264Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:13:39:16 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T13:39:16.297Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.920888506Z level=info msg="Migration successfully executed" id="create index UQE_api_key_org_id_name - v2" duration=675.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.923225506Z level=info msg="Executing migration" id="copy api_key v1 to v2"
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T13:39:16.305Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":377}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T13:39:26.474Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:13:39:26 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T13:39:26.475Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:13:39:27 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T13:39:27.361Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T13:39:36.861Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:13:39:36 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T13:39:36.865Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:13:39:42 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T13:39:42.378Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T13:39:46.981Z"}
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-11 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-44 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-23 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.923828298Z level=info msg="Migration successfully executed" id="copy api_key v1 to v2" duration=599.834Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.925357131Z level=info msg="Executing migration" id="Drop old table api_key_v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.925814923Z level=info msg="Migration successfully executed" id="Drop old table api_key_v1" duration=458.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.928075256Z level=info msg="Executing migration" id="Update api_key table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.928097006Z level=info msg="Migration successfully executed" id="Update api_key table charset" duration=20.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.931356589Z level=info msg="Executing migration" id="Add expires to api_key table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.933705839Z level=info msg="Migration successfully executed" id="Add expires to api_key table" duration=2.348875ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.941192256Z level=info msg="Executing migration" id="Add service account foreign key"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.942979089Z level=info msg="Migration successfully executed" id="Add service account foreign key" duration=1.788042ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.953438798Z level=info msg="Executing migration" id="set service account foreign key to nil if 0"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.953687423Z level=info msg="Migration successfully executed" id="set service account foreign key to nil if 0" duration=255.417Âµs
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:13:39:46 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T13:39:46.982Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T13:39:57.065Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:13:39:57 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T13:39:57.065Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:13:39:57 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T13:39:57.357Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T13:40:07.183Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:13:40:07 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T13:40:07.184Z"}
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T14:12:54.821Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":388}
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð prevent building .env in docker: https://dotenvx.com/prebuild
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:54:12.180Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:14:12:55 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T14:12:55.104Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T14:13:00.037Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:14:13:00 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T14:13:00.037Z"}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:14:13:10 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T14:13:10.097Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T14:13:10.113Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.956395464Z level=info msg="Executing migration" id="Add last_used_at to api_key table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.958422506Z level=info msg="Migration successfully executed" id="Add last_used_at to api_key table" duration=2.026584ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.959965839Z level=info msg="Executing migration" id="Add is_revoked column to api_key table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.961525298Z level=info msg="Migration successfully executed" id="Add is_revoked column to api_key table" duration=1.559458ms
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:14:13:10 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T14:13:10.114Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T14:13:20.228Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:14:13:20 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T14:13:20.229Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:15:13:57 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T15:13:57.528Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.962780756Z level=info msg="Executing migration" id="create dashboard_snapshot table v4"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.963257506Z level=info msg="Migration successfully executed" id="create dashboard_snapshot table v4" duration=477.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.964909423Z level=info msg="Executing migration" id="drop table dashboard_snapshot_v4 #1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.965379506Z level=info msg="Migration successfully executed" id="drop table dashboard_snapshot_v4 #1" duration=471.125Âµs
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
grafana-1          | logger=migrator t=2026-02-25T11:40:01.966533089Z level=info msg="Executing migration" id="create dashboard_snapshot table v5 #2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.967127423Z level=info msg="Migration successfully executed" id="create dashboard_snapshot table v5 #2" duration=593.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.968434798Z level=info msg="Executing migration" id="create index UQE_dashboard_snapshot_key - v5"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.968850506Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_snapshot_key - v5" duration=415.916Âµs
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-19 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-32 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-28 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-7 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-40 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-3 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-36 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-47 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-14 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-43 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:01.970037048Z level=info msg="Executing migration" id="create index UQE_dashboard_snapshot_delete_key - v5"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.970452173Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_snapshot_delete_key - v5" duration=415.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.971707881Z level=info msg="Executing migration" id="create index IDX_dashboard_snapshot_user_id - v5"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.972102173Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_snapshot_user_id - v5" duration=394.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.973880048Z level=info msg="Executing migration" id="alter dashboard_snapshot to mediumtext v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.973912256Z level=info msg="Migration successfully executed" id="alter dashboard_snapshot to mediumtext v2" duration=34.291Âµs
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T15:13:57.674Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":394}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T15:14:02.738Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:15:14:02 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T15:14:02.739Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.975279173Z level=info msg="Executing migration" id="Update dashboard_snapshot table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.975300089Z level=info msg="Migration successfully executed" id="Update dashboard_snapshot table charset" duration=21.541Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.976987048Z level=info msg="Executing migration" id="Add column external_delete_url to dashboard_snapshots table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.978784173Z level=info msg="Migration successfully executed" id="Add column external_delete_url to dashboard_snapshots table" duration=1.796583ms
grafana-1          | logger=migrator t=2026-02-25T11:40:01.980236464Z level=info msg="Executing migration" id="Add encrypted dashboard json column"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.982548714Z level=info msg="Migration successfully executed" id="Add encrypted dashboard json column" duration=2.309709ms
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:15:14:12 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T15:14:12.534Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T15:14:12.817Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:15:14:12 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T15:14:12.818Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T15:14:22.892Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:15:14:22 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T15:14:22.893Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:16:15:00 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T16:15:00.611Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.985116548Z level=info msg="Executing migration" id="Change dashboard_encrypted column to MEDIUMBLOB"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.985132964Z level=info msg="Migration successfully executed" id="Change dashboard_encrypted column to MEDIUMBLOB" duration=17.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.987049714Z level=info msg="Executing migration" id="create quota table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.987796006Z level=info msg="Migration successfully executed" id="create quota table v1" duration=746.625Âµs
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T16:15:01.081Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":400}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T16:15:06.037Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:16:15:06 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T16:15:06.038Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:16:15:15 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T16:15:15.603Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T16:15:16.129Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:16:15:16 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T16:15:16.130Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T16:15:26.218Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:16:15:26 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T16:15:26.218Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:17:15:43 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T17:15:43.617Z"}
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T17:15:45.004Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":406}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T17:15:49.299Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:17:15:49 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T17:15:49.299Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:01.990987256Z level=info msg="Executing migration" id="create index UQE_quota_org_id_user_id_target - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.991682423Z level=info msg="Migration successfully executed" id="create index UQE_quota_org_id_user_id_target - v1" duration=696.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.994014214Z level=info msg="Executing migration" id="Update quota table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.994033714Z level=info msg="Migration successfully executed" id="Update quota table charset" duration=20.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.995499798Z level=info msg="Executing migration" id="create plugin_setting table"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.996124464Z level=info msg="Migration successfully executed" id="create plugin_setting table" duration=625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:01.997437964Z level=info msg="Executing migration" id="create index UQE_plugin_setting_org_id_plugin_id - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:01.998379506Z level=info msg="Migration successfully executed" id="create index UQE_plugin_setting_org_id_plugin_id - v1" duration=940.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.000600339Z level=info msg="Executing migration" id="Add column plugin_version to plugin_settings"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.003001256Z level=info msg="Migration successfully executed" id="Add column plugin_version to plugin_settings" duration=2.399833ms
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:17:15:58 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T17:15:58.633Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T17:15:59.468Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:17:15:59 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T17:15:59.469Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T17:16:09.569Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:17:16:09 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T17:16:09.570Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T17:49:29.477Z","logger":"kafkajs","message":"[Consumer] Crash: KafkaJSNumberOfRetriesExceeded: Request Fetch(key: 1, version: 11) timed out","groupId":"order-service-group","retryCount":5,"stack":"KafkaJSNonRetriableError\n  Caused by: KafkaJSRequestTimeoutError: Request Fetch(key: 1, version: 11) timed out\n    at SocketRequest.timeoutRequest (/app/node_modules/kafkajs/src/network/requestQueue/socketRequest.js:107:19)\n    at /app/node_modules/kafkajs/src/network/requestQueue/index.js:94:21\n    at Map.forEach (<anonymous>)\n    at Timeout._onTimeout (/app/node_modules/kafkajs/src/network/requestQueue/index.js:92:23)\n    at listOnTimeout (node:internal/timers:569:17)\n    at process.processTimers (node:internal/timers:512:7)"}
order-service-1    | {"level":"INFO","timestamp":"2026-02-25T17:49:29.585Z","logger":"kafkajs","message":"[Consumer] Stopped","groupId":"order-service-group"}
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-10 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-22 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-18 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.012699048Z level=info msg="Executing migration" id="Update plugin_setting table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.012847298Z level=info msg="Migration successfully executed" id="Update plugin_setting table charset" duration=151.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.017347131Z level=info msg="Executing migration" id="update NULL org_id to 1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.017674506Z level=info msg="Migration successfully executed" id="update NULL org_id to 1" duration=328.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.020108048Z level=info msg="Executing migration" id="make org_id NOT NULL and DEFAULT VALUE 1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.026727006Z level=info msg="Migration successfully executed" id="make org_id NOT NULL and DEFAULT VALUE 1" duration=6.617917ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.029020881Z level=info msg="Executing migration" id="create session table"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T17:49:29.586Z","logger":"kafkajs","message":"[Consumer] Restarting the consumer in 6214ms","retryCount":5,"retryTime":6214,"groupId":"order-service-group"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:17:49:29 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T17:49:29.914Z"}
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T17:49:33.301Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":412}
order-service-1    | {"level":"INFO","timestamp":"2026-02-25T17:49:35.801Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"order-service-group"}
order-service-1    | {"level":"INFO","timestamp":"2026-02-25T17:49:35.858Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"order-service-group","memberId":"kafkajs-ae6c393c-7373-481a-b7ed-56bd68030f22","leaderId":"kafkajs-ae6c393c-7373-481a-b7ed-56bd68030f22","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":45}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.029621589Z level=info msg="Migration successfully executed" id="create session table" duration=601.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.031699173Z level=info msg="Executing migration" id="Drop old table playlist table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.031782089Z level=info msg="Migration successfully executed" id="Drop old table playlist table" duration=84Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.033304923Z level=info msg="Executing migration" id="Drop old table playlist_item table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.033367131Z level=info msg="Migration successfully executed" id="Drop old table playlist_item table" duration=63.167Âµs
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-31 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-27 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-39 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-6 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.034698423Z level=info msg="Executing migration" id="create playlist table v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.035174006Z level=info msg="Migration successfully executed" id="create playlist table v2" duration=475.625Âµs
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.037672631Z level=info msg="Executing migration" id="create playlist item table v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.038133381Z level=info msg="Migration successfully executed" id="create playlist item table v2" duration=461.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.040413839Z level=info msg="Executing migration" id="Update playlist table charset"
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:54:12.320Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:54:40.825Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-b675b376-d653-45f2-9b8b-3f6edb2e96a0","leaderId":"kafkajs-b675b376-d653-45f2-9b8b-3f6edb2e96a0","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28504}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:09,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-35 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,751] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-2 (state.change.logger)
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T17:49:35.930Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.040448256Z level=info msg="Migration successfully executed" id="Update playlist table charset" duration=37.084Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.042169756Z level=info msg="Executing migration" id="Update playlist_item table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.042195798Z level=info msg="Migration successfully executed" id="Update playlist_item table charset" duration=28.334Âµs
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:17:49:35 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T17:49:35.931Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:17:49:44 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T17:49:44.909Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T17:49:46.001Z"}
kafka-1            | [2026-02-25 11:40:09,751] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 50 become-leader and 0 become-follower partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,754] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 50 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,762] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 for 50 partitions (state.change.logger)
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:17:49:46 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T17:49:46.001Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T17:49:56.087Z"}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.043542256Z level=info msg="Executing migration" id="Add playlist column created_at"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.046480256Z level=info msg="Migration successfully executed" id="Add playlist column created_at" duration=2.936375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.049608673Z level=info msg="Executing migration" id="Add playlist column updated_at"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.051712923Z level=info msg="Migration successfully executed" id="Add playlist column updated_at" duration=2.104959ms
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:17:49:56 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T17:49:56.087Z"}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.4 - - [25/Feb/2026:17:49:59 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T17:49:59.907Z"}
order-service-1    | {"level":"info","message":"Health check requested","timestamp":"2026-02-25T17:50:06.189Z"}
order-service-1    | {"level":"info","message":"::1 - - [25/Feb/2026:17:50:06 +0000] \"GET /health HTTP/1.1\" 200 15 \"-\" \"-\"","timestamp":"2026-02-25T17:50:06.190Z"}
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Executing (default): SELECT "id", "name", "stock", "reservedStock", "version", "createdAt", "updatedAt" FROM "Products" AS "Product";
kafka-1            | [2026-02-25 11:40:09,764] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.053866839Z level=info msg="Executing migration" id="drop preferences table v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.053934756Z level=info msg="Migration successfully executed" id="drop preferences table v2" duration=68.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.056132881Z level=info msg="Executing migration" id="drop preferences table v3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.056245298Z level=info msg="Migration successfully executed" id="drop preferences table v3" duration=113.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.057977298Z level=info msg="Executing migration" id="create preferences table v3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.058633339Z level=info msg="Migration successfully executed" id="create preferences table v3" duration=656.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.059885798Z level=info msg="Executing migration" id="Update preferences table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.059909173Z level=info msg="Migration successfully executed" id="Update preferences table charset" duration=21.708Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.061321381Z level=info msg="Executing migration" id="Add column team_id in preferences"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.063251214Z level=info msg="Migration successfully executed" id="Add column team_id in preferences" duration=1.929875ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.065029339Z level=info msg="Executing migration" id="Update team_id column values in preferences"
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð prevent committing .env to code: https://dotenvx.com/precommit
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:56:12.568Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.065175548Z level=info msg="Migration successfully executed" id="Update team_id column values in preferences" duration=147.416Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.066972131Z level=info msg="Executing migration" id="Add column week_start in preferences"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.069107173Z level=info msg="Migration successfully executed" id="Add column week_start in preferences" duration=2.13475ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.071153298Z level=info msg="Executing migration" id="Add column preferences.json_data"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.073313923Z level=info msg="Migration successfully executed" id="Add column preferences.json_data" duration=2.158167ms
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.076630589Z level=info msg="Executing migration" id="alter preferences.json_data to mediumtext v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.076649756Z level=info msg="Migration successfully executed" id="alter preferences.json_data to mediumtext v1" duration=20.541Âµs
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.079277923Z level=info msg="Executing migration" id="Add preferences index org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.079947839Z level=info msg="Migration successfully executed" id="Add preferences index org_id" duration=670.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.085882714Z level=info msg="Executing migration" id="Add preferences index user_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.086547339Z level=info msg="Migration successfully executed" id="Add preferences index user_id" duration=665.584Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.087898881Z level=info msg="Executing migration" id="Add home_dashboard_uid column to preferences table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.089775589Z level=info msg="Migration successfully executed" id="Add home_dashboard_uid column to preferences table" duration=1.876625ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.091246131Z level=info msg="Executing migration" id="Add missing dashboard_uid to preferences table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.091495589Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid to preferences table" duration=246.792Âµs
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
kafka-1            | [2026-02-25 11:40:09,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
grafana-1          | logger=migrator t=2026-02-25T11:40:02.093087798Z level=info msg="Executing migration" id="create alert table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.093723298Z level=info msg="Migration successfully executed" id="create alert table v1" duration=635.791Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.095228673Z level=info msg="Executing migration" id="add index alert org_id & id "
grafana-1          | logger=migrator t=2026-02-25T11:40:02.095771589Z level=info msg="Migration successfully executed" id="add index alert org_id & id " duration=542.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.097250589Z level=info msg="Executing migration" id="add index alert state"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.097663673Z level=info msg="Migration successfully executed" id="add index alert state" duration=412.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.099012548Z level=info msg="Executing migration" id="add index alert dashboard_id"
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:56:12.660Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:56:41.519Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-70b34d36-756b-47f1-a472-1a8229a502f3","leaderId":"kafkajs-70b34d36-756b-47f1-a472-1a8229a502f3","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28859}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.099606173Z level=info msg="Migration successfully executed" id="add index alert dashboard_id" duration=594.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.101515048Z level=info msg="Executing migration" id="Create alert_rule_tag table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.102369214Z level=info msg="Migration successfully executed" id="Create alert_rule_tag table v1" duration=856.708Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.104314756Z level=info msg="Executing migration" id="Add unique index alert_rule_tag.alert_id_tag_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.105026173Z level=info msg="Migration successfully executed" id="Add unique index alert_rule_tag.alert_id_tag_id" duration=711.875Âµs
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
kafka-1            | [2026-02-25 11:40:09,770] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,771] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
kafka-1            | [2026-02-25 11:40:09,772] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,772] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,772] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.107565048Z level=info msg="Executing migration" id="drop index UQE_alert_rule_tag_alert_id_tag_id - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.108092214Z level=info msg="Migration successfully executed" id="drop index UQE_alert_rule_tag_alert_id_tag_id - v1" duration=524.792Âµs
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.114405506Z level=info msg="Executing migration" id="Rename table alert_rule_tag to alert_rule_tag_v1 - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.119628756Z level=info msg="Migration successfully executed" id="Rename table alert_rule_tag to alert_rule_tag_v1 - v1" duration=5.223ms
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
grafana-1          | logger=migrator t=2026-02-25T11:40:02.121480048Z level=info msg="Executing migration" id="Create alert_rule_tag table v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.121851089Z level=info msg="Migration successfully executed" id="Create alert_rule_tag table v2" duration=371.667Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.122733006Z level=info msg="Executing migration" id="create index UQE_alert_rule_tag_alert_id_tag_id - Add unique index alert_rule_tag.alert_id_tag_id V2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.123163631Z level=info msg="Migration successfully executed" id="create index UQE_alert_rule_tag_alert_id_tag_id - Add unique index alert_rule_tag.alert_id_tag_id V2" duration=430.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.124238798Z level=info msg="Executing migration" id="copy alert_rule_tag v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.124355964Z level=info msg="Migration successfully executed" id="copy alert_rule_tag v1 to v2" duration=117.5Âµs
kafka-1            | [2026-02-25 11:40:09,772] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,772] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,772] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,772] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.125298339Z level=info msg="Executing migration" id="drop table alert_rule_tag_v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.125652048Z level=info msg="Migration successfully executed" id="drop table alert_rule_tag_v1" duration=350.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.126562798Z level=info msg="Executing migration" id="create alert_notification table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.126939464Z level=info msg="Migration successfully executed" id="create alert_notification table v1" duration=376.75Âµs
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
grafana-1          | logger=migrator t=2026-02-25T11:40:02.127970339Z level=info msg="Executing migration" id="Add column is_default"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.129889131Z level=info msg="Migration successfully executed" id="Add column is_default" duration=1.91875ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.130962923Z level=info msg="Executing migration" id="Add column frequency"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.132796173Z level=info msg="Migration successfully executed" id="Add column frequency" duration=1.831542ms
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
grafana-1          | logger=migrator t=2026-02-25T11:40:02.133992214Z level=info msg="Executing migration" id="Add column send_reminder"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.135757173Z level=info msg="Migration successfully executed" id="Add column send_reminder" duration=1.765083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.136598339Z level=info msg="Executing migration" id="Add column disable_resolve_message"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.138543923Z level=info msg="Migration successfully executed" id="Add column disable_resolve_message" duration=1.944834ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.139878381Z level=info msg="Executing migration" id="add index alert_notification org_id & name"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.140281131Z level=info msg="Migration successfully executed" id="add index alert_notification org_id & name" duration=403Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.141308298Z level=info msg="Executing migration" id="Update alert table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.141321423Z level=info msg="Migration successfully executed" id="Update alert table charset" duration=13.708Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.142334631Z level=info msg="Executing migration" id="Update alert_notification table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.142348131Z level=info msg="Migration successfully executed" id="Update alert_notification table charset" duration=14.083Âµs
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.143470089Z level=info msg="Executing migration" id="create notification_journal table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.143926381Z level=info msg="Migration successfully executed" id="create notification_journal table v1" duration=456.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.145086673Z level=info msg="Executing migration" id="add index notification_journal org_id & alert_id & notifier_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.145511506Z level=info msg="Migration successfully executed" id="add index notification_journal org_id & alert_id & notifier_id" duration=424.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.146581381Z level=info msg="Executing migration" id="drop alert_notification_journal"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.146908173Z level=info msg="Migration successfully executed" id="drop alert_notification_journal" duration=327.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.147908048Z level=info msg="Executing migration" id="create alert_notification_state table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.148250881Z level=info msg="Migration successfully executed" id="create alert_notification_state table v1" duration=342.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.149093423Z level=info msg="Executing migration" id="add index alert_notification_state org_id & alert_id & notifier_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.149472964Z level=info msg="Migration successfully executed" id="add index alert_notification_state org_id & alert_id & notifier_id" duration=379.917Âµs
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.150613464Z level=info msg="Executing migration" id="Add for to alert table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.152539506Z level=info msg="Migration successfully executed" id="Add for to alert table" duration=1.925375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.153527548Z level=info msg="Executing migration" id="Add column uid in alert_notification"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.155477339Z level=info msg="Migration successfully executed" id="Add column uid in alert_notification" duration=1.949708ms
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,773] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.156508839Z level=info msg="Executing migration" id="Update uid column values in alert_notification"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.156609964Z level=info msg="Migration successfully executed" id="Update uid column values in alert_notification" duration=101.5Âµs
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð prevent building .env in docker: https://dotenvx.com/prebuild
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T11:58:13.212Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
grafana-1          | logger=migrator t=2026-02-25T11:40:02.157455881Z level=info msg="Executing migration" id="Add unique index alert_notification_org_id_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.157859339Z level=info msg="Migration successfully executed" id="Add unique index alert_notification_org_id_uid" duration=403.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.158943798Z level=info msg="Executing migration" id="Remove unique index org_id_name"
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.159305881Z level=info msg="Migration successfully executed" id="Remove unique index org_id_name" duration=362.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.160216673Z level=info msg="Executing migration" id="Add column secure_settings in alert_notification"
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:09,774] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,774] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.162190381Z level=info msg="Migration successfully executed" id="Add column secure_settings in alert_notification" duration=1.97275ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.163424798Z level=info msg="Executing migration" id="alter alert.settings to mediumtext"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.163433089Z level=info msg="Migration successfully executed" id="alter alert.settings to mediumtext" duration=8.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.164765089Z level=info msg="Executing migration" id="Add non-unique index alert_notification_state_alert_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.165163339Z level=info msg="Migration successfully executed" id="Add non-unique index alert_notification_state_alert_id" duration=398.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.166302798Z level=info msg="Executing migration" id="Add non-unique index alert_rule_tag_alert_id"
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,806] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.166638006Z level=info msg="Migration successfully executed" id="Add non-unique index alert_rule_tag_alert_id" duration=335.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.167695339Z level=info msg="Executing migration" id="Drop old annotation table v4"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.167729256Z level=info msg="Migration successfully executed" id="Drop old annotation table v4" duration=34.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.168745089Z level=info msg="Executing migration" id="create annotation table v5"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.169200714Z level=info msg="Migration successfully executed" id="create annotation table v5" duration=455.708Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.170583589Z level=info msg="Executing migration" id="add index annotation 0 v3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.170989006Z level=info msg="Migration successfully executed" id="add index annotation 0 v3" duration=405.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.172141798Z level=info msg="Executing migration" id="add index annotation 1 v3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.172502339Z level=info msg="Migration successfully executed" id="add index annotation 1 v3" duration=360.708Âµs
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:58:13.312Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.173623506Z level=info msg="Executing migration" id="add index annotation 2 v3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.173960589Z level=info msg="Migration successfully executed" id="add index annotation 2 v3" duration=337.166Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.175072756Z level=info msg="Executing migration" id="add index annotation 3 v3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.175413548Z level=info msg="Migration successfully executed" id="add index annotation 3 v3" duration=340.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.176564256Z level=info msg="Executing migration" id="add index annotation 4 v3"
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T11:58:42.262Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-ba9fa6a2-cc98-43ff-8d47-2ef05293dc9a","leaderId":"kafkajs-ba9fa6a2-cc98-43ff-8d47-2ef05293dc9a","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28949}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.176893423Z level=info msg="Migration successfully executed" id="add index annotation 4 v3" duration=329.666Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.177998464Z level=info msg="Executing migration" id="Update annotation table charset"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.178010423Z level=info msg="Migration successfully executed" id="Update annotation table charset" duration=12.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.179074214Z level=info msg="Executing migration" id="Add column region_id to annotation table"
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.181244506Z level=info msg="Migration successfully executed" id="Add column region_id to annotation table" duration=2.170666ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.182272881Z level=info msg="Executing migration" id="Drop category_id index"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.182709173Z level=info msg="Migration successfully executed" id="Drop category_id index" duration=436.292Âµs
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
grafana-1          | logger=migrator t=2026-02-25T11:40:02.184130256Z level=info msg="Executing migration" id="Add column tags to annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.186361673Z level=info msg="Migration successfully executed" id="Add column tags to annotation table" duration=2.229709ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.187569839Z level=info msg="Executing migration" id="Create annotation_tag table v2"
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.187886173Z level=info msg="Migration successfully executed" id="Create annotation_tag table v2" duration=316.166Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.188945589Z level=info msg="Executing migration" id="Add unique index annotation_tag.annotation_id_tag_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.189332298Z level=info msg="Migration successfully executed" id="Add unique index annotation_tag.annotation_id_tag_id" duration=386.584Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.190488881Z level=info msg="Executing migration" id="drop index UQE_annotation_tag_annotation_id_tag_id - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.190888798Z level=info msg="Migration successfully executed" id="drop index UQE_annotation_tag_annotation_id_tag_id - v2" duration=400.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.192131673Z level=info msg="Executing migration" id="Rename table annotation_tag to annotation_tag_v2 - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.198013006Z level=info msg="Migration successfully executed" id="Rename table annotation_tag to annotation_tag_v2 - v2" duration=5.881ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.199340631Z level=info msg="Executing migration" id="Create annotation_tag table v3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.199756839Z level=info msg="Migration successfully executed" id="Create annotation_tag table v3" duration=416.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.200897131Z level=info msg="Executing migration" id="create index UQE_annotation_tag_annotation_id_tag_id - Add unique index annotation_tag.annotation_id_tag_id V3"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.201334256Z level=info msg="Migration successfully executed" id="create index UQE_annotation_tag_annotation_id_tag_id - Add unique index annotation_tag.annotation_id_tag_id V3" duration=438.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.202465964Z level=info msg="Executing migration" id="copy annotation_tag v2 to v3"
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
grafana-1          | logger=migrator t=2026-02-25T11:40:02.202595798Z level=info msg="Migration successfully executed" id="copy annotation_tag v2 to v3" duration=129.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.203979256Z level=info msg="Executing migration" id="drop table annotation_tag_v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.204287756Z level=info msg="Migration successfully executed" id="drop table annotation_tag_v2" duration=308.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.205806673Z level=info msg="Executing migration" id="Update alert annotations and set TEXT to empty"
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,807] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.205929589Z level=info msg="Migration successfully executed" id="Update alert annotations and set TEXT to empty" duration=123.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.207085631Z level=info msg="Executing migration" id="Add created time to annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.209544298Z level=info msg="Migration successfully executed" id="Add created time to annotation table" duration=2.457917ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.211387089Z level=info msg="Executing migration" id="Add updated time to annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.213770548Z level=info msg="Migration successfully executed" id="Add updated time to annotation table" duration=2.383709ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.215207381Z level=info msg="Executing migration" id="Add index for created in annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.215845048Z level=info msg="Migration successfully executed" id="Add index for created in annotation table" duration=638Âµs
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
kafka-1            | [2026-02-25 11:40:09,810] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
kafka-1            | [2026-02-25 11:40:09,812] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 1 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,837] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,851] INFO Created log for partition __consumer_offsets-3 in /var/lib/kafka/data/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,853] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.217263798Z level=info msg="Executing migration" id="Add index for updated in annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.217741131Z level=info msg="Migration successfully executed" id="Add index for updated in annotation table" duration=477.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.219247173Z level=info msg="Executing migration" id="Convert existing annotations from seconds to milliseconds"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.219390298Z level=info msg="Migration successfully executed" id="Convert existing annotations from seconds to milliseconds" duration=143.416Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.220786756Z level=info msg="Executing migration" id="Add epoch_end column"
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
grafana-1          | logger=migrator t=2026-02-25T11:40:02.223197964Z level=info msg="Migration successfully executed" id="Add epoch_end column" duration=2.411ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.225296631Z level=info msg="Executing migration" id="Add index for epoch_end"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.225842964Z level=info msg="Migration successfully executed" id="Add index for epoch_end" duration=547.166Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.229216673Z level=info msg="Executing migration" id="Make epoch_end the same as epoch"
kafka-1            | [2026-02-25 11:40:09,855] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,856] INFO [Broker id=1] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,868] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,870] INFO Created log for partition __consumer_offsets-18 in /var/lib/kafka/data/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,870] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,870] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,870] INFO [Broker id=1] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,879] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,879] INFO Created log for partition __consumer_offsets-41 in /var/lib/kafka/data/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,879] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð encrypt with Dotenvx: https://dotenvx.com
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T12:00:14.233Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.229341881Z level=info msg="Migration successfully executed" id="Make epoch_end the same as epoch" duration=125.667Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.231266006Z level=info msg="Executing migration" id="Move region to single row"
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.231481089Z level=info msg="Migration successfully executed" id="Move region to single row" duration=216Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.238666923Z level=info msg="Executing migration" id="Remove index org_id_epoch from annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.239265131Z level=info msg="Migration successfully executed" id="Remove index org_id_epoch from annotation table" duration=599.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.240476339Z level=info msg="Executing migration" id="Remove index org_id_dashboard_id_panel_id_epoch from annotation table"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.240988006Z level=info msg="Migration successfully executed" id="Remove index org_id_dashboard_id_panel_id_epoch from annotation table" duration=512.334Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.242113881Z level=info msg="Executing migration" id="Add index for org_id_dashboard_id_epoch_end_epoch on annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.242657506Z level=info msg="Migration successfully executed" id="Add index for org_id_dashboard_id_epoch_end_epoch on annotation table" duration=543.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.243794423Z level=info msg="Executing migration" id="Add index for org_id_epoch_end_epoch on annotation table"
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
grafana-1          | logger=migrator t=2026-02-25T11:40:02.244287089Z level=info msg="Migration successfully executed" id="Add index for org_id_epoch_end_epoch on annotation table" duration=491.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.245869214Z level=info msg="Executing migration" id="Remove index org_id_epoch_epoch_end from annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.246296298Z level=info msg="Migration successfully executed" id="Remove index org_id_epoch_epoch_end from annotation table" duration=427.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.247670089Z level=info msg="Executing migration" id="Add index for alert_id on annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.248220631Z level=info msg="Migration successfully executed" id="Add index for alert_id on annotation table" duration=550.834Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.249473381Z level=info msg="Executing migration" id="Increase tags column to length 4096"
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
kafka-1            | [2026-02-25 11:40:09,879] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,880] INFO [Broker id=1] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,893] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:00:14.350Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:00:42.994Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-95896526-f3a8-4a67-ab56-ea7c469cbc36","leaderId":"kafkajs-95896526-f3a8-4a67-ab56-ea7c469cbc36","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28643}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.249490298Z level=info msg="Migration successfully executed" id="Increase tags column to length 4096" duration=17.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.251085006Z level=info msg="Executing migration" id="Increase prev_state column to length 40 not null"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.251101506Z level=info msg="Migration successfully executed" id="Increase prev_state column to length 40 not null" duration=17.541Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.252318131Z level=info msg="Executing migration" id="Increase new_state column to length 40 not null"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.252328464Z level=info msg="Migration successfully executed" id="Increase new_state column to length 40 not null" duration=11.209Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.253520881Z level=info msg="Executing migration" id="Add dashboard_uid column to annotation table"
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.255941714Z level=info msg="Migration successfully executed" id="Add dashboard_uid column to annotation table" duration=2.420083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.257103298Z level=info msg="Executing migration" id="Add missing dashboard_uid to annotation table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.257339381Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid to annotation table" duration=238.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.258541339Z level=info msg="Executing migration" id="create test_data table"
kafka-1            | [2026-02-25 11:40:09,895] INFO Created log for partition __consumer_offsets-10 in /var/lib/kafka/data/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,895] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,895] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,895] INFO [Broker id=1] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,907] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,908] INFO Created log for partition __consumer_offsets-33 in /var/lib/kafka/data/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,908] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,908] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:09,908] INFO [Broker id=1] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,917] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,918] INFO Created log for partition __consumer_offsets-48 in /var/lib/kafka/data/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,918] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,918] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,918] INFO [Broker id=1] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,930] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:09,934] INFO Created log for partition __consumer_offsets-19 in /var/lib/kafka/data/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,934] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,934] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.259014839Z level=info msg="Migration successfully executed" id="create test_data table" duration=473.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.260241798Z level=info msg="Executing migration" id="create dashboard_version table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.260729423Z level=info msg="Migration successfully executed" id="create dashboard_version table v1" duration=487.667Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.262251714Z level=info msg="Executing migration" id="add index dashboard_version.dashboard_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.262881631Z level=info msg="Migration successfully executed" id="add index dashboard_version.dashboard_id" duration=630.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.264423631Z level=info msg="Executing migration" id="add unique index dashboard_version.dashboard_id and dashboard_version.version"
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
grafana-1          | logger=migrator t=2026-02-25T11:40:02.265126464Z level=info msg="Migration successfully executed" id="add unique index dashboard_version.dashboard_id and dashboard_version.version" duration=703.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.267311798Z level=info msg="Executing migration" id="Set dashboard version to 1 where 0"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.267503131Z level=info msg="Migration successfully executed" id="Set dashboard version to 1 where 0" duration=192.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.269668923Z level=info msg="Executing migration" id="save existing dashboard data in dashboard_version table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.269985339Z level=info msg="Migration successfully executed" id="save existing dashboard data in dashboard_version table v1" duration=316.125Âµs
kafka-1            | [2026-02-25 11:40:09,934] INFO [Broker id=1] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,942] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,946] INFO Created log for partition __consumer_offsets-34 in /var/lib/kafka/data/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,946] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,946] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.271318173Z level=info msg="Executing migration" id="alter dashboard_version.data to mediumtext v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.271331339Z level=info msg="Migration successfully executed" id="alter dashboard_version.data to mediumtext v1" duration=13.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.272705506Z level=info msg="Executing migration" id="Add apiVersion for dashboard_version"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.275299381Z level=info msg="Migration successfully executed" id="Add apiVersion for dashboard_version" duration=2.592875ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.276907464Z level=info msg="Executing migration" id="create team table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.277325798Z level=info msg="Migration successfully executed" id="create team table" duration=418.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.278999381Z level=info msg="Executing migration" id="add index team.org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.279589673Z level=info msg="Migration successfully executed" id="add index team.org_id" duration=590.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.281308673Z level=info msg="Executing migration" id="add unique index team_org_id_name"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.281871548Z level=info msg="Migration successfully executed" id="add unique index team_org_id_name" duration=563.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.284004881Z level=info msg="Executing migration" id="Add column uid in team"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.286737881Z level=info msg="Migration successfully executed" id="Add column uid in team" duration=2.734916ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.288675006Z level=info msg="Executing migration" id="Update uid column values in team"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.288838089Z level=info msg="Migration successfully executed" id="Update uid column values in team" duration=159.458Âµs
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
kafka-1            | [2026-02-25 11:40:09,946] INFO [Broker id=1] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,951] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
grafana-1          | logger=migrator t=2026-02-25T11:40:02.290052464Z level=info msg="Executing migration" id="Add unique index team_org_id_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.290661839Z level=info msg="Migration successfully executed" id="Add unique index team_org_id_uid" duration=607.708Âµs
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.292252964Z level=info msg="Executing migration" id="Add column external_uid in team"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.294815839Z level=info msg="Migration successfully executed" id="Add column external_uid in team" duration=2.562375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.296240506Z level=info msg="Executing migration" id="Add column is_provisioned in team"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.298712006Z level=info msg="Migration successfully executed" id="Add column is_provisioned in team" duration=2.471166ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.299855673Z level=info msg="Executing migration" id="create team member table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.300362714Z level=info msg="Migration successfully executed" id="create team member table" duration=507.791Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.301546381Z level=info msg="Executing migration" id="add index team_member.org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.302037006Z level=info msg="Migration successfully executed" id="add index team_member.org_id" duration=490.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.303227881Z level=info msg="Executing migration" id="add unique index team_member_org_id_team_id_user_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.303692006Z level=info msg="Migration successfully executed" id="add unique index team_member_org_id_team_id_user_id" duration=464.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.304851631Z level=info msg="Executing migration" id="add index team_member.team_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.305307006Z level=info msg="Migration successfully executed" id="add index team_member.team_id" duration=455.792Âµs
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
grafana-1          | logger=migrator t=2026-02-25T11:40:02.306506339Z level=info msg="Executing migration" id="Add column email to team table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.309117714Z level=info msg="Migration successfully executed" id="Add column email to team table" duration=2.60975ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.310410256Z level=info msg="Executing migration" id="Add column external to team_member table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.312914714Z level=info msg="Migration successfully executed" id="Add column external to team_member table" duration=2.504ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.314364423Z level=info msg="Executing migration" id="Add column permission to team_member table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.316778256Z level=info msg="Migration successfully executed" id="Add column permission to team_member table" duration=2.413166ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.317954214Z level=info msg="Executing migration" id="add unique index team_member_user_id_org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.318344631Z level=info msg="Migration successfully executed" id="add unique index team_member_user_id_org_id" duration=389.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.319593339Z level=info msg="Executing migration" id="create dashboard acl table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.320642381Z level=info msg="Migration successfully executed" id="create dashboard acl table" duration=1.048542ms
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âï¸  suppress all logs with { quiet: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T12:02:14.712Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.321739798Z level=info msg="Executing migration" id="add index dashboard_acl_dashboard_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.322165923Z level=info msg="Migration successfully executed" id="add index dashboard_acl_dashboard_id" duration=425.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.323132173Z level=info msg="Executing migration" id="add unique index dashboard_acl_dashboard_id_user_id"
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.323529006Z level=info msg="Migration successfully executed" id="add unique index dashboard_acl_dashboard_id_user_id" duration=397.084Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.324763381Z level=info msg="Executing migration" id="add unique index dashboard_acl_dashboard_id_team_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.325163048Z level=info msg="Migration successfully executed" id="add unique index dashboard_acl_dashboard_id_team_id" duration=400.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.326232423Z level=info msg="Executing migration" id="add index dashboard_acl_user_id"
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.326631381Z level=info msg="Migration successfully executed" id="add index dashboard_acl_user_id" duration=399Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.327722006Z level=info msg="Executing migration" id="add index dashboard_acl_team_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.328108173Z level=info msg="Migration successfully executed" id="add index dashboard_acl_team_id" duration=386.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.329132048Z level=info msg="Executing migration" id="add index dashboard_acl_org_id_role"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.329506381Z level=info msg="Migration successfully executed" id="add index dashboard_acl_org_id_role" duration=374.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.330940673Z level=info msg="Executing migration" id="add index dashboard_permission"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.331353006Z level=info msg="Migration successfully executed" id="add index dashboard_permission" duration=412.667Âµs
kafka-1            | [2026-02-25 11:40:09,953] INFO Created log for partition __consumer_offsets-4 in /var/lib/kafka/data/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,953] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,953] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,953] INFO [Broker id=1] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,961] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
grafana-1          | logger=migrator t=2026-02-25T11:40:02.332602214Z level=info msg="Executing migration" id="save default acl rules in dashboard_acl table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.332810256Z level=info msg="Migration successfully executed" id="save default acl rules in dashboard_acl table" duration=208.209Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.333862256Z level=info msg="Executing migration" id="delete acl rules for deleted dashboards and folders"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.333963006Z level=info msg="Migration successfully executed" id="delete acl rules for deleted dashboards and folders" duration=103.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.335029131Z level=info msg="Executing migration" id="create tag table"
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:02:14.807Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:02:43.708Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-0ece8492-2c8c-4684-9144-a1d5692ede2f","leaderId":"kafkajs-0ece8492-2c8c-4684-9144-a1d5692ede2f","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28901}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.335343464Z level=info msg="Migration successfully executed" id="create tag table" duration=314.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.336369548Z level=info msg="Executing migration" id="add index tag.key_value"
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.336769381Z level=info msg="Migration successfully executed" id="add index tag.key_value" duration=400Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.337888214Z level=info msg="Executing migration" id="create login attempt table"
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
kafka-1            | [2026-02-25 11:40:09,964] INFO Created log for partition __consumer_offsets-11 in /var/lib/kafka/data/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,964] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,964] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,964] INFO [Broker id=1] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,973] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.338200673Z level=info msg="Migration successfully executed" id="create login attempt table" duration=312.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.339258839Z level=info msg="Executing migration" id="add index login_attempt.username"
kafka-1            | [2026-02-25 11:40:09,974] INFO Created log for partition __consumer_offsets-26 in /var/lib/kafka/data/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,974] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,974] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,975] INFO [Broker id=1] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,980] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,981] INFO Created log for partition __consumer_offsets-49 in /var/lib/kafka/data/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,981] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,981] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,981] INFO [Broker id=1] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,988] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:09,990] INFO Created log for partition __consumer_offsets-39 in /var/lib/kafka/data/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,990] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,990] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,990] INFO [Broker id=1] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:09,995] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
kafka-1            | [2026-02-25 11:40:09,995] INFO Created log for partition __consumer_offsets-9 in /var/lib/kafka/data/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:09,996] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,996] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:09,996] INFO [Broker id=1] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,001] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,007] INFO Created log for partition __consumer_offsets-24 in /var/lib/kafka/data/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,007] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.339690089Z level=info msg="Migration successfully executed" id="add index login_attempt.username" duration=431.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.340901589Z level=info msg="Executing migration" id="drop index IDX_login_attempt_username - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.341275339Z level=info msg="Migration successfully executed" id="drop index IDX_login_attempt_username - v1" duration=373.875Âµs
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.342550256Z level=info msg="Executing migration" id="Rename table login_attempt to login_attempt_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.349829006Z level=info msg="Migration successfully executed" id="Rename table login_attempt to login_attempt_tmp_qwerty - v1" duration=7.278042ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.350733714Z level=info msg="Executing migration" id="create login_attempt v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.351048839Z level=info msg="Migration successfully executed" id="create login_attempt v2" duration=314.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.352737131Z level=info msg="Executing migration" id="create index IDX_login_attempt_username - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.353239256Z level=info msg="Migration successfully executed" id="create index IDX_login_attempt_username - v2" duration=502.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.354577006Z level=info msg="Executing migration" id="copy login_attempt v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.354697131Z level=info msg="Migration successfully executed" id="copy login_attempt v1 to v2" duration=119.125Âµs
kafka-1            | [2026-02-25 11:40:10,007] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,008] INFO [Broker id=1] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,016] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,020] INFO Created log for partition __consumer_offsets-31 in /var/lib/kafka/data/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,020] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,020] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,020] INFO [Broker id=1] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.355795381Z level=info msg="Executing migration" id="drop login_attempt_tmp_qwerty"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.356117506Z level=info msg="Migration successfully executed" id="drop login_attempt_tmp_qwerty" duration=322.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.357355589Z level=info msg="Executing migration" id="increase login_attempt.ip_address column length for IPv6 support"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.357366089Z level=info msg="Migration successfully executed" id="increase login_attempt.ip_address column length for IPv6 support" duration=10.834Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.358495214Z level=info msg="Executing migration" id="alter table login_attempt alter column created type to bigint"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.358503173Z level=info msg="Migration successfully executed" id="alter table login_attempt alter column created type to bigint" duration=8.291Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.359416631Z level=info msg="Executing migration" id="create user auth table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.359799756Z level=info msg="Migration successfully executed" id="create user auth table" duration=382.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.360921298Z level=info msg="Executing migration" id="create index IDX_user_auth_auth_module_auth_id - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.361307506Z level=info msg="Migration successfully executed" id="create index IDX_user_auth_auth_module_auth_id - v1" duration=386.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.362373339Z level=info msg="Executing migration" id="alter user_auth.auth_id to length 190"
kafka-1            | [2026-02-25 11:40:10,026] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,026] INFO Created log for partition __consumer_offsets-46 in /var/lib/kafka/data/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,026] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,026] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,027] INFO [Broker id=1] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,032] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,035] INFO Created log for partition __consumer_offsets-1 in /var/lib/kafka/data/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,035] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
grafana-1          | logger=migrator t=2026-02-25T11:40:02.362382048Z level=info msg="Migration successfully executed" id="alter user_auth.auth_id to length 190" duration=10.166Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.363421381Z level=info msg="Executing migration" id="Add OAuth access token to user_auth"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.366086464Z level=info msg="Migration successfully executed" id="Add OAuth access token to user_auth" duration=2.664708ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.367204714Z level=info msg="Executing migration" id="Add OAuth refresh token to user_auth"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.369852006Z level=info msg="Migration successfully executed" id="Add OAuth refresh token to user_auth" duration=2.647209ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.373798714Z level=info msg="Executing migration" id="Add OAuth token type to user_auth"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.376668589Z level=info msg="Migration successfully executed" id="Add OAuth token type to user_auth" duration=2.86975ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.378099964Z level=info msg="Executing migration" id="Add OAuth expiry to user_auth"
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âï¸  write to custom object with { processEnv: myObject }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T12:04:15.560Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
grafana-1          | logger=migrator t=2026-02-25T11:40:02.380906006Z level=info msg="Migration successfully executed" id="Add OAuth expiry to user_auth" duration=2.806ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.382088881Z level=info msg="Executing migration" id="Add index to user_id column in user_auth"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.382514506Z level=info msg="Migration successfully executed" id="Add index to user_id column in user_auth" duration=425.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.383646423Z level=info msg="Executing migration" id="Add OAuth ID token to user_auth"
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.386475298Z level=info msg="Migration successfully executed" id="Add OAuth ID token to user_auth" duration=2.828416ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.387593839Z level=info msg="Executing migration" id="Add user_unique_id to user_auth"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.390377298Z level=info msg="Migration successfully executed" id="Add user_unique_id to user_auth" duration=2.782375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.391564381Z level=info msg="Executing migration" id="create server_lock table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.391913214Z level=info msg="Migration successfully executed" id="create server_lock table" duration=348.917Âµs
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
kafka-1            | [2026-02-25 11:40:10,035] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,035] INFO [Broker id=1] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,041] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,042] INFO Created log for partition __consumer_offsets-16 in /var/lib/kafka/data/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:10,042] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,042] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,042] INFO [Broker id=1] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,048] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,049] INFO Created log for partition __consumer_offsets-2 in /var/lib/kafka/data/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,049] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,049] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.392995548Z level=info msg="Executing migration" id="add index server_lock.operation_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.393440964Z level=info msg="Migration successfully executed" id="add index server_lock.operation_uid" duration=445.666Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.394633256Z level=info msg="Executing migration" id="create user auth token table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.395047714Z level=info msg="Migration successfully executed" id="create user auth token table" duration=414.666Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.396188256Z level=info msg="Executing migration" id="add unique index user_auth_token.auth_token"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.396638089Z level=info msg="Migration successfully executed" id="add unique index user_auth_token.auth_token" duration=451.125Âµs
kafka-1            | [2026-02-25 11:40:10,049] INFO [Broker id=1] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,055] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,056] INFO Created log for partition __consumer_offsets-25 in /var/lib/kafka/data/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,056] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,056] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,056] INFO [Broker id=1] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.397932048Z level=info msg="Executing migration" id="add unique index user_auth_token.prev_auth_token"
kafka-1            | [2026-02-25 11:40:10,066] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,067] INFO Created log for partition __consumer_offsets-40 in /var/lib/kafka/data/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 11:40:10,067] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.398330923Z level=info msg="Migration successfully executed" id="add unique index user_auth_token.prev_auth_token" duration=399.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.399396214Z level=info msg="Executing migration" id="add index user_auth_token.user_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.399811048Z level=info msg="Migration successfully executed" id="add index user_auth_token.user_id" duration=414.209Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.400887714Z level=info msg="Executing migration" id="Add revoked_at to the user auth token"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.403797173Z level=info msg="Migration successfully executed" id="Add revoked_at to the user auth token" duration=2.908917ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.405489214Z level=info msg="Executing migration" id="add index user_auth_token.revoked_at"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.405947131Z level=info msg="Migration successfully executed" id="add index user_auth_token.revoked_at" duration=457.708Âµs
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:04:15.667Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:04:44.416Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-87d84a3a-47f5-4a88-9081-27b6e4260cbe","leaderId":"kafkajs-87d84a3a-47f5-4a88-9081-27b6e4260cbe","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28749}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.407302839Z level=info msg="Executing migration" id="add external_session_id to user_auth_token"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.410170298Z level=info msg="Migration successfully executed" id="add external_session_id to user_auth_token" duration=2.86725ms
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
kafka-1            | [2026-02-25 11:40:10,067] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,067] INFO [Broker id=1] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
kafka-1            | [2026-02-25 11:40:10,076] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,076] INFO Created log for partition __consumer_offsets-47 in /var/lib/kafka/data/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,077] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.411612881Z level=info msg="Executing migration" id="create cache_data table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.412031006Z level=info msg="Migration successfully executed" id="create cache_data table" duration=417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.413708548Z level=info msg="Executing migration" id="add unique index cache_data.cache_key"
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at runNextTicks (node:internal/process/task_queues:60:5)
product-service-1  |     at process.processTimers (node:internal/timers:509:9)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð add secrets lifecycle management: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T12:06:16.288Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
grafana-1          | logger=migrator t=2026-02-25T11:40:02.414217339Z level=info msg="Migration successfully executed" id="add unique index cache_data.cache_key" duration=509.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.415895589Z level=info msg="Executing migration" id="create short_url table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.416288423Z level=info msg="Migration successfully executed" id="create short_url table v1" duration=394.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.417948089Z level=info msg="Executing migration" id="add index short_url.org_id-uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.418398506Z level=info msg="Migration successfully executed" id="add index short_url.org_id-uid" duration=450.209Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.419548381Z level=info msg="Executing migration" id="alter table short_url alter column created_by type to bigint"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.419585173Z level=info msg="Migration successfully executed" id="alter table short_url alter column created_by type to bigint" duration=37.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.420618173Z level=info msg="Executing migration" id="delete alert_definition table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.420656839Z level=info msg="Migration successfully executed" id="delete alert_definition table" duration=40.459Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.421782548Z level=info msg="Executing migration" id="recreate alert_definition table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.422234048Z level=info msg="Migration successfully executed" id="recreate alert_definition table" duration=451.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.423978173Z level=info msg="Executing migration" id="add index in alert_definition on org_id and title columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.424470673Z level=info msg="Migration successfully executed" id="add index in alert_definition on org_id and title columns" duration=492.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.425804381Z level=info msg="Executing migration" id="add index in alert_definition on org_id and uid columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.426254381Z level=info msg="Migration successfully executed" id="add index in alert_definition on org_id and uid columns" duration=449.667Âµs
kafka-1            | [2026-02-25 11:40:10,077] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,077] INFO [Broker id=1] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,086] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,094] INFO Created log for partition __consumer_offsets-17 in /var/lib/kafka/data/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,094] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,094] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,094] INFO [Broker id=1] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,103] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,104] INFO Created log for partition __consumer_offsets-32 in /var/lib/kafka/data/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,104] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,104] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.427903798Z level=info msg="Executing migration" id="alter alert_definition table data column to mediumtext in mysql"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.427913006Z level=info msg="Migration successfully executed" id="alter alert_definition table data column to mediumtext in mysql" duration=9.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.429609631Z level=info msg="Executing migration" id="drop index in alert_definition on org_id and title columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.430088339Z level=info msg="Migration successfully executed" id="drop index in alert_definition on org_id and title columns" duration=478.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.431805006Z level=info msg="Executing migration" id="drop index in alert_definition on org_id and uid columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.432387006Z level=info msg="Migration successfully executed" id="drop index in alert_definition on org_id and uid columns" duration=580.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.434113173Z level=info msg="Executing migration" id="add unique index in alert_definition on org_id and title columns"
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
kafka-1            | [2026-02-25 11:40:10,104] INFO [Broker id=1] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,112] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,114] INFO Created log for partition __consumer_offsets-37 in /var/lib/kafka/data/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,115] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,115] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,115] INFO [Broker id=1] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,120] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,120] INFO Created log for partition __consumer_offsets-7 in /var/lib/kafka/data/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,120] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,120] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 11:40:10,120] INFO [Broker id=1] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,126] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,126] INFO Created log for partition __consumer_offsets-22 in /var/lib/kafka/data/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,127] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.434708631Z level=info msg="Migration successfully executed" id="add unique index in alert_definition on org_id and title columns" duration=595.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.436123464Z level=info msg="Executing migration" id="add unique index in alert_definition on org_id and uid columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.436609423Z level=info msg="Migration successfully executed" id="add unique index in alert_definition on org_id and uid columns" duration=486.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.437874214Z level=info msg="Executing migration" id="Add column paused in alert_definition"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.441026798Z level=info msg="Migration successfully executed" id="Add column paused in alert_definition" duration=3.151417ms
kafka-1            | [2026-02-25 11:40:10,127] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,127] INFO [Broker id=1] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,131] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,132] INFO Created log for partition __consumer_offsets-29 in /var/lib/kafka/data/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,133] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,133] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,133] INFO [Broker id=1] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,139] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,139] INFO Created log for partition __consumer_offsets-44 in /var/lib/kafka/data/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,140] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,140] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,140] INFO [Broker id=1] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,146] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | [2026-02-25 11:40:10,148] INFO Created log for partition __consumer_offsets-14 in /var/lib/kafka/data/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,148] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,148] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,149] INFO [Broker id=1] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,178] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.442508464Z level=info msg="Executing migration" id="drop alert_definition table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.443076256Z level=info msg="Migration successfully executed" id="drop alert_definition table" duration=568Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.444566714Z level=info msg="Executing migration" id="delete alert_definition_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.444625964Z level=info msg="Migration successfully executed" id="delete alert_definition_version table" duration=59.791Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.445717214Z level=info msg="Executing migration" id="recreate alert_definition_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.446202798Z level=info msg="Migration successfully executed" id="recreate alert_definition_version table" duration=485.041Âµs
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:06:16.418Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:06:45.290Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-9eccc8c6-83ad-4f15-8624-550d1925e2f4","leaderId":"kafkajs-9eccc8c6-83ad-4f15-8624-550d1925e2f4","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28871}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:10,179] INFO Created log for partition __consumer_offsets-23 in /var/lib/kafka/data/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,179] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
kafka-1            | [2026-02-25 11:40:10,179] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,179] INFO [Broker id=1] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,185] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,187] INFO Created log for partition __consumer_offsets-38 in /var/lib/kafka/data/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,187] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,187] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,187] INFO [Broker id=1] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.448053631Z level=info msg="Executing migration" id="add index in alert_definition_version table on alert_definition_id and version columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.448540548Z level=info msg="Migration successfully executed" id="add index in alert_definition_version table on alert_definition_id and version columns" duration=486.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.450680298Z level=info msg="Executing migration" id="add index in alert_definition_version table on alert_definition_uid and version columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.451338798Z level=info msg="Migration successfully executed" id="add index in alert_definition_version table on alert_definition_uid and version columns" duration=659.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.453218006Z level=info msg="Executing migration" id="alter alert_definition_version table data column to mediumtext in mysql"
kafka-1            | [2026-02-25 11:40:10,194] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,195] INFO Created log for partition __consumer_offsets-8 in /var/lib/kafka/data/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,195] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,195] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,195] INFO [Broker id=1] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,200] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,202] INFO Created log for partition __consumer_offsets-45 in /var/lib/kafka/data/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,202] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,202] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  |   [cause]: undefined
product-service-1  | }
kafka-1            | [2026-02-25 11:40:10,202] INFO [Broker id=1] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,207] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,208] INFO Created log for partition __consumer_offsets-15 in /var/lib/kafka/data/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,208] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,208] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,208] INFO [Broker id=1] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð encrypt with Dotenvx: https://dotenvx.com
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T12:08:17.162Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
kafka-1            | [2026-02-25 11:40:10,213] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,214] INFO Created log for partition __consumer_offsets-30 in /var/lib/kafka/data/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,214] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,214] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,214] INFO [Broker id=1] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.453253006Z level=info msg="Migration successfully executed" id="alter alert_definition_version table data column to mediumtext in mysql" duration=35.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.455039006Z level=info msg="Executing migration" id="drop alert_definition_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.455570798Z level=info msg="Migration successfully executed" id="drop alert_definition_version table" duration=531.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.457522673Z level=info msg="Executing migration" id="create alert_instance table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.458257881Z level=info msg="Migration successfully executed" id="create alert_instance table" duration=735.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.459480673Z level=info msg="Executing migration" id="add index in alert_instance table on def_org_id, def_uid and current_state columns"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.459958173Z level=info msg="Migration successfully executed" id="add index in alert_instance table on def_org_id, def_uid and current_state columns" duration=477.916Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.461033798Z level=info msg="Executing migration" id="add index in alert_instance table on def_org_id, current_state columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.461461589Z level=info msg="Migration successfully executed" id="add index in alert_instance table on def_org_id, current_state columns" duration=426.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.462549131Z level=info msg="Executing migration" id="add column current_state_end to alert_instance"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.465678714Z level=info msg="Migration successfully executed" id="add column current_state_end to alert_instance" duration=3.129083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.467136089Z level=info msg="Executing migration" id="remove index def_org_id, def_uid, current_state on alert_instance"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.467818256Z level=info msg="Migration successfully executed" id="remove index def_org_id, def_uid, current_state on alert_instance" duration=683.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.468960839Z level=info msg="Executing migration" id="remove index def_org_id, current_state on alert_instance"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.469493714Z level=info msg="Migration successfully executed" id="remove index def_org_id, current_state on alert_instance" duration=533.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.470644923Z level=info msg="Executing migration" id="rename def_org_id to rule_org_id in alert_instance"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.485525798Z level=info msg="Migration successfully executed" id="rename def_org_id to rule_org_id in alert_instance" duration=14.879ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.487161714Z level=info msg="Executing migration" id="rename def_uid to rule_uid in alert_instance"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.502316298Z level=info msg="Migration successfully executed" id="rename def_uid to rule_uid in alert_instance" duration=15.152709ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.503606964Z level=info msg="Executing migration" id="add index rule_org_id, rule_uid, current_state on alert_instance"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.504149714Z level=info msg="Migration successfully executed" id="add index rule_org_id, rule_uid, current_state on alert_instance" duration=545.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.505384006Z level=info msg="Executing migration" id="add index rule_org_id, current_state on alert_instance"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.505856881Z level=info msg="Migration successfully executed" id="add index rule_org_id, current_state on alert_instance" duration=473.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.507007214Z level=info msg="Executing migration" id="add current_reason column related to current_state"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.510047881Z level=info msg="Migration successfully executed" id="add current_reason column related to current_state" duration=3.040541ms
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.511537381Z level=info msg="Executing migration" id="add result_fingerprint column to alert_instance"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.514642839Z level=info msg="Migration successfully executed" id="add result_fingerprint column to alert_instance" duration=3.102333ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.515730798Z level=info msg="Executing migration" id="create alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.516331006Z level=info msg="Migration successfully executed" id="create alert_rule table" duration=600.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.517630506Z level=info msg="Executing migration" id="add index in alert_rule on org_id and title columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.518141923Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id and title columns" duration=509.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.519353173Z level=info msg="Executing migration" id="add index in alert_rule on org_id and uid columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.519793631Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id and uid columns" duration=440.292Âµs
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.521071756Z level=info msg="Executing migration" id="add index in alert_rule on org_id, namespace_uid, group_uid columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.521484464Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id, namespace_uid, group_uid columns" duration=412.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.522598923Z level=info msg="Executing migration" id="alter alert_rule table data column to mediumtext in mysql"
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.522607714Z level=info msg="Migration successfully executed" id="alter alert_rule table data column to mediumtext in mysql" duration=9.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.523628839Z level=info msg="Executing migration" id="add column for to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.526959089Z level=info msg="Migration successfully executed" id="add column for to alert_rule" duration=3.32975ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.528327298Z level=info msg="Executing migration" id="add column annotations to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.531762506Z level=info msg="Migration successfully executed" id="add column annotations to alert_rule" duration=3.43475ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.532931048Z level=info msg="Executing migration" id="add column labels to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.536230006Z level=info msg="Migration successfully executed" id="add column labels to alert_rule" duration=3.298625ms
kafka-1            | [2026-02-25 11:40:10,219] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,220] INFO Created log for partition __consumer_offsets-0 in /var/lib/kafka/data/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,220] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,220] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,220] INFO [Broker id=1] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.537465423Z level=info msg="Executing migration" id="remove unique index from alert_rule on org_id, title columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.538095631Z level=info msg="Migration successfully executed" id="remove unique index from alert_rule on org_id, title columns" duration=630.667Âµs
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | [2026-02-25 11:40:10,227] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,228] INFO Created log for partition __consumer_offsets-35 in /var/lib/kafka/data/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,228] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,228] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,228] INFO [Broker id=1] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,241] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,242] INFO Created log for partition __consumer_offsets-5 in /var/lib/kafka/data/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,242] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,242] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,242] INFO [Broker id=1] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,247] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,248] INFO Created log for partition __consumer_offsets-20 in /var/lib/kafka/data/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,248] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
kafka-1            | [2026-02-25 11:40:10,248] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,248] INFO [Broker id=1] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,255] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,256] INFO Created log for partition __consumer_offsets-27 in /var/lib/kafka/data/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,256] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.539283131Z level=info msg="Executing migration" id="add index in alert_rule on org_id, namespase_uid and title columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.539820548Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id, namespase_uid and title columns" duration=537.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.540922923Z level=info msg="Executing migration" id="add dashboard_uid column to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.544134714Z level=info msg="Migration successfully executed" id="add dashboard_uid column to alert_rule" duration=3.211541ms
kafka-1            | [2026-02-25 11:40:10,256] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,256] INFO [Broker id=1] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,261] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,262] INFO Created log for partition __consumer_offsets-42 in /var/lib/kafka/data/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,262] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,263] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,263] INFO [Broker id=1] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:08:17.276Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:08:45.907Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-19fff72d-85ca-49e7-9e70-67c175845fd1","leaderId":"kafkajs-19fff72d-85ca-49e7-9e70-67c175845fd1","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28630}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
grafana-1          | logger=migrator t=2026-02-25T11:40:02.545106714Z level=info msg="Executing migration" id="add panel_id column to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.548302131Z level=info msg="Migration successfully executed" id="add panel_id column to alert_rule" duration=3.19425ms
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.549978131Z level=info msg="Executing migration" id="add index in alert_rule on org_id, dashboard_uid and panel_id columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.550734089Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id, dashboard_uid and panel_id columns" duration=755.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.552144548Z level=info msg="Executing migration" id="add rule_group_idx column to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.555492714Z level=info msg="Migration successfully executed" id="add rule_group_idx column to alert_rule" duration=3.347125ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.556692339Z level=info msg="Executing migration" id="add is_paused column to alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.560039839Z level=info msg="Migration successfully executed" id="add is_paused column to alert_rule table" duration=3.346333ms
kafka-1            | [2026-02-25 11:40:10,271] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,273] INFO Created log for partition __consumer_offsets-12 in /var/lib/kafka/data/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,273] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,273] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,273] INFO [Broker id=1] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,277] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.561465589Z level=info msg="Executing migration" id="fix is_paused column for alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.561476839Z level=info msg="Migration successfully executed" id="fix is_paused column for alert_rule table" duration=11.791Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.562777339Z level=info msg="Executing migration" id="alter table alert_rule alter column rule_group_idx type to bigint"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.562791131Z level=info msg="Migration successfully executed" id="alter table alert_rule alter column rule_group_idx type to bigint" duration=14.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.564281839Z level=info msg="Executing migration" id="create alert_rule_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.564869631Z level=info msg="Migration successfully executed" id="create alert_rule_version table" duration=588.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.566165589Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_org_id, rule_uid and version columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.566820756Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_org_id, rule_uid and version columns" duration=655.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.568131631Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_org_id, rule_namespace_uid and rule_group columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.568812256Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_org_id, rule_namespace_uid and rule_group columns" duration=680.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.571272214Z level=info msg="Executing migration" id="alter alert_rule_version table data column to mediumtext in mysql"
kafka-1            | [2026-02-25 11:40:10,280] INFO Created log for partition __consumer_offsets-21 in /var/lib/kafka/data/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,280] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,280] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,280] INFO [Broker id=1] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,285] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,285] INFO Created log for partition __consumer_offsets-36 in /var/lib/kafka/data/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,285] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,285] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,285] INFO [Broker id=1] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,291] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,293] INFO Created log for partition __consumer_offsets-6 in /var/lib/kafka/data/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,293] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð encrypt with Dotenvx: https://dotenvx.com
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T12:10:17.620Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
grafana-1          | logger=migrator t=2026-02-25T11:40:02.571293798Z level=info msg="Migration successfully executed" id="alter alert_rule_version table data column to mediumtext in mysql" duration=22.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.573265298Z level=info msg="Executing migration" id="add column for to alert_rule_version"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.577295714Z level=info msg="Migration successfully executed" id="add column for to alert_rule_version" duration=4.030208ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.578811839Z level=info msg="Executing migration" id="add column annotations to alert_rule_version"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.582236881Z level=info msg="Migration successfully executed" id="add column annotations to alert_rule_version" duration=3.424375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.583461589Z level=info msg="Executing migration" id="add column labels to alert_rule_version"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.586822714Z level=info msg="Migration successfully executed" id="add column labels to alert_rule_version" duration=3.361458ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.588174714Z level=info msg="Executing migration" id="add rule_group_idx column to alert_rule_version"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.591357548Z level=info msg="Migration successfully executed" id="add rule_group_idx column to alert_rule_version" duration=3.181959ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.592344756Z level=info msg="Executing migration" id="add is_paused column to alert_rule_versions table"
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.595702506Z level=info msg="Migration successfully executed" id="add is_paused column to alert_rule_versions table" duration=3.357375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.596908839Z level=info msg="Executing migration" id="fix is_paused column for alert_rule_version table"
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.596917548Z level=info msg="Migration successfully executed" id="fix is_paused column for alert_rule_version table" duration=8.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.598001631Z level=info msg="Executing migration" id="alter table alert_rule_version alter column rule_group_idx type to bigint"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.598010923Z level=info msg="Migration successfully executed" id="alter table alert_rule_version alter column rule_group_idx type to bigint" duration=9.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.599294256Z level=info msg="Executing migration" id=create_alert_configuration_table
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.599779631Z level=info msg="Migration successfully executed" id=create_alert_configuration_table duration=485.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.600951881Z level=info msg="Executing migration" id="Add column default in alert_configuration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.604202339Z level=info msg="Migration successfully executed" id="Add column default in alert_configuration" duration=3.25ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.605259006Z level=info msg="Executing migration" id="alert alert_configuration alertmanager_configuration column from TEXT to MEDIUMTEXT if mysql"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.605267673Z level=info msg="Migration successfully executed" id="alert alert_configuration alertmanager_configuration column from TEXT to MEDIUMTEXT if mysql" duration=8.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.606273506Z level=info msg="Executing migration" id="add column org_id in alert_configuration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.609482006Z level=info msg="Migration successfully executed" id="add column org_id in alert_configuration" duration=3.208375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.610574089Z level=info msg="Executing migration" id="add index in alert_configuration table on org_id column"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.611016923Z level=info msg="Migration successfully executed" id="add index in alert_configuration table on org_id column" duration=442.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.612159131Z level=info msg="Executing migration" id="add configuration_hash column to alert_configuration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.615409589Z level=info msg="Migration successfully executed" id="add configuration_hash column to alert_configuration" duration=3.250584ms
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
grafana-1          | logger=migrator t=2026-02-25T11:40:02.616401339Z level=info msg="Executing migration" id=create_ngalert_configuration_table
grafana-1          | logger=migrator t=2026-02-25T11:40:02.616759964Z level=info msg="Migration successfully executed" id=create_ngalert_configuration_table duration=358.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.617835339Z level=info msg="Executing migration" id="add index in ngalert_configuration on org_id column"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.618280673Z level=info msg="Migration successfully executed" id="add index in ngalert_configuration on org_id column" duration=445.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.619420506Z level=info msg="Executing migration" id="add column send_alerts_to in ngalert_configuration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.622679923Z level=info msg="Migration successfully executed" id="add column send_alerts_to in ngalert_configuration" duration=3.259501ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.623497048Z level=info msg="Executing migration" id="create provenance_type table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.623856173Z level=info msg="Migration successfully executed" id="create provenance_type table" duration=358.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.624924965Z level=info msg="Executing migration" id="add index to uniquify (record_key, record_type, org_id) columns"
kafka-1            | [2026-02-25 11:40:10,293] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,293] INFO [Broker id=1] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,301] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:10,303] INFO Created log for partition __consumer_offsets-43 in /var/lib/kafka/data/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,303] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,303] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,303] INFO [Broker id=1] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,308] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 11:40:10,309] INFO Created log for partition __consumer_offsets-13 in /var/lib/kafka/data/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,309] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,309] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,309] INFO [Broker id=1] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,319] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 11:40:10,324] INFO Created log for partition __consumer_offsets-28 in /var/lib/kafka/data/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2026-02-25 11:40:10,324] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:10:17.734Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
kafka-1            | [2026-02-25 11:40:10,324] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2026-02-25 11:40:10,324] INFO [Broker id=1] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,326] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.62534784Z level=info msg="Migration successfully executed" id="add index to uniquify (record_key, record_type, org_id) columns" duration=422.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.626644173Z level=info msg="Executing migration" id="create alert_image table"
kafka-1            | [2026-02-25 11:40:10,326] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,326] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.627042965Z level=info msg="Migration successfully executed" id="create alert_image table" duration=398.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.628326923Z level=info msg="Executing migration" id="add unique index on token to alert_image table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.628805798Z level=info msg="Migration successfully executed" id="add unique index on token to alert_image table" duration=479.083Âµs
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.629896173Z level=info msg="Executing migration" id="support longer URLs in alert_image table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.62990409Z level=info msg="Migration successfully executed" id="support longer URLs in alert_image table" duration=8.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.630865756Z level=info msg="Executing migration" id=create_alert_configuration_history_table
grafana-1          | logger=migrator t=2026-02-25T11:40:02.631284673Z level=info msg="Migration successfully executed" id=create_alert_configuration_history_table duration=419.084Âµs
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.632431881Z level=info msg="Executing migration" id="drop non-unique orgID index on alert_configuration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.632883298Z level=info msg="Migration successfully executed" id="drop non-unique orgID index on alert_configuration" duration=451.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.633707965Z level=info msg="Executing migration" id="drop unique orgID index on alert_configuration if exists"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.633895048Z level=warn msg="Skipping migration: Already executed, but not recorded in migration log" id="drop unique orgID index on alert_configuration if exists"
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.634934381Z level=info msg="Executing migration" id="extract alertmanager configuration history to separate table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.635060756Z level=info msg="Migration successfully executed" id="extract alertmanager configuration history to separate table" duration=126.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.636070923Z level=info msg="Executing migration" id="add unique index on orgID to alert_configuration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.636513381Z level=info msg="Migration successfully executed" id="add unique index on orgID to alert_configuration" duration=442.791Âµs
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T12:10:46.651Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-24b218aa-edcc-4077-b598-b0ae75a47b97","leaderId":"kafkajs-24b218aa-edcc-4077-b598-b0ae75a47b97","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":28916}
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
grafana-1          | logger=migrator t=2026-02-25T11:40:02.637483798Z level=info msg="Executing migration" id="add last_applied column to alert_configuration_history"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.640971006Z level=info msg="Migration successfully executed" id="add last_applied column to alert_configuration_history" duration=3.487333ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.641765381Z level=info msg="Executing migration" id="create library_element table v1"
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.64222959Z level=info msg="Migration successfully executed" id="create library_element table v1" duration=465.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.64328609Z level=info msg="Executing migration" id="add index library_element org_id-folder_id-name-kind"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.643735715Z level=info msg="Migration successfully executed" id="add index library_element org_id-folder_id-name-kind" duration=449.792Âµs
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T13:39:16.262Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":12}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.644710673Z level=info msg="Executing migration" id="create library_element_connection table v1"
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,327] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,330] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,336] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.645054506Z level=info msg="Migration successfully executed" id="create library_element_connection table v1" duration=343.834Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.64606359Z level=info msg="Executing migration" id="add index library_element_connection element_id-kind-connection_id"
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | 
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ð ï¸  run anywhere with `dotenvx run -- yourcommand`
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T13:39:41.971Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.646568965Z level=info msg="Migration successfully executed" id="add index library_element_connection element_id-kind-connection_id" duration=505.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.647701506Z level=info msg="Executing migration" id="add unique index library_element org_id_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.648133423Z level=info msg="Migration successfully executed" id="add unique index library_element org_id_uid" duration=432.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.649147881Z level=info msg="Executing migration" id="increase max description length to 2048"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.649161673Z level=info msg="Migration successfully executed" id="increase max description length to 2048" duration=13.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.650204465Z level=info msg="Executing migration" id="alter library_element model to mediumtext"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.650213798Z level=info msg="Migration successfully executed" id="alter library_element model to mediumtext" duration=9.541Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.651141673Z level=info msg="Executing migration" id="add library_element folder uid"
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.65480384Z level=info msg="Migration successfully executed" id="add library_element folder uid" duration=3.662333ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.655833506Z level=info msg="Executing migration" id="populate library_element folder_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.655993506Z level=info msg="Migration successfully executed" id="populate library_element folder_uid" duration=160.333Âµs
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.657116298Z level=info msg="Executing migration" id="add index library_element org_id-folder_uid-name-kind"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.657563798Z level=info msg="Migration successfully executed" id="add index library_element org_id-folder_uid-name-kind" duration=447.541Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.65864484Z level=info msg="Executing migration" id="clone move dashboard alerts to unified alerting"
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.65878309Z level=info msg="Migration successfully executed" id="clone move dashboard alerts to unified alerting" duration=138.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.65976234Z level=info msg="Executing migration" id="create data_keys table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.660160006Z level=info msg="Migration successfully executed" id="create data_keys table" duration=398.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.661239881Z level=info msg="Executing migration" id="create secrets table"
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.661586006Z level=info msg="Migration successfully executed" id="create secrets table" duration=346Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.662561256Z level=info msg="Executing migration" id="rename data_keys name column to id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.681218006Z level=info msg="Migration successfully executed" id="rename data_keys name column to id" duration=18.666333ms
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,390] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
grafana-1          | logger=migrator t=2026-02-25T11:40:02.682540048Z level=info msg="Executing migration" id="add name column into data_keys"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.686205548Z level=info msg="Migration successfully executed" id="add name column into data_keys" duration=3.665291ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.68717659Z level=info msg="Executing migration" id="copy data_keys id column values into name"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.687230131Z level=info msg="Migration successfully executed" id="copy data_keys id column values into name" duration=53.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.688030798Z level=info msg="Executing migration" id="rename data_keys name column to label"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.707116965Z level=info msg="Migration successfully executed" id="rename data_keys name column to label" duration=19.085542ms
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.70829509Z level=info msg="Executing migration" id="rename data_keys id column back to name"
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.727000173Z level=info msg="Migration successfully executed" id="rename data_keys id column back to name" duration=18.70375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.728189006Z level=info msg="Executing migration" id="create kv_store table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.728577756Z level=info msg="Migration successfully executed" id="create kv_store table v1" duration=388.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.729634923Z level=info msg="Executing migration" id="add index kv_store.org_id-namespace-key"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.730086715Z level=info msg="Migration successfully executed" id="add index kv_store.org_id-namespace-key" duration=451.916Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.731142006Z level=info msg="Executing migration" id="update dashboard_uid and panel_id from existing annotations"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.731219131Z level=info msg="Migration successfully executed" id="update dashboard_uid and panel_id from existing annotations" duration=77.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.732250548Z level=info msg="Executing migration" id="create permission table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.732596298Z level=info msg="Migration successfully executed" id="create permission table" duration=345.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.73360184Z level=info msg="Executing migration" id="add unique index permission.role_id"
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.734028798Z level=info msg="Migration successfully executed" id="add unique index permission.role_id" duration=427.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.735092215Z level=info msg="Executing migration" id="add unique index role_id_action_scope"
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 11:40:10,391] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T13:39:42.516Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T13:40:06.894Z","logger":"kafkajs","message":"[ConsumerGroup] Consumer has joined the group","groupId":"product-group","memberId":"kafkajs-d35adf07-45d6-45fe-8111-5c9f395944bb","leaderId":"kafkajs-d35adf07-45d6-45fe-8111-5c9f395944bb","isLeader":true,"memberAssignment":{"order-events":[0]},"groupProtocol":"RoundRobinAssigner","duration":24376}
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.735531548Z level=info msg="Migration successfully executed" id="add unique index role_id_action_scope" duration=439.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.736795548Z level=info msg="Executing migration" id="create role table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.737178506Z level=info msg="Migration successfully executed" id="create role table" duration=382.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.738195465Z level=info msg="Executing migration" id="add column display_name"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.742069631Z level=info msg="Migration successfully executed" id="add column display_name" duration=3.873958ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.743167465Z level=info msg="Executing migration" id="add column group_name"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.746903173Z level=info msg="Migration successfully executed" id="add column group_name" duration=3.735834ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.747912423Z level=info msg="Executing migration" id="add index role.org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.748340381Z level=info msg="Migration successfully executed" id="add index role.org_id" duration=428.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.749388881Z level=info msg="Executing migration" id="add unique index role_org_id_name"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.74986959Z level=info msg="Migration successfully executed" id="add unique index role_org_id_name" duration=481Âµs
product-service-1  | Attempt 1 to start Kafka client failed, retrying in 1000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 2 to start Kafka client failed, retrying in 2000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 3 to start Kafka client failed, retrying in 4000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T14:12:54.707Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":0}
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 4 to start Kafka client failed, retrying in 8000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 5 to start Kafka client failed, retrying in 16000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 6 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.750881215Z level=info msg="Executing migration" id="add index role_org_id_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.751333715Z level=info msg="Migration successfully executed" id="add index role_org_id_uid" duration=452.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.752473006Z level=info msg="Executing migration" id="create team role table"
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T15:13:57.498Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":6}
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 7 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
grafana-1          | logger=migrator t=2026-02-25T11:40:02.75283609Z level=info msg="Migration successfully executed" id="create team role table" duration=363.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.753947673Z level=info msg="Executing migration" id="add index team_role.org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.754389173Z level=info msg="Migration successfully executed" id="add index team_role.org_id" duration=441.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.755574006Z level=info msg="Executing migration" id="add unique index team_role_org_id_team_id_role_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.75604609Z level=info msg="Migration successfully executed" id="add unique index team_role_org_id_team_id_role_id" duration=472.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.75700134Z level=info msg="Executing migration" id="add index team_role.team_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.757442423Z level=info msg="Migration successfully executed" id="add index team_role.team_id" duration=441.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.758465215Z level=info msg="Executing migration" id="create user role table"
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.758820506Z level=info msg="Migration successfully executed" id="create user role table" duration=355.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.759762215Z level=info msg="Executing migration" id="add index user_role.org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.760196965Z level=info msg="Migration successfully executed" id="add index user_role.org_id" duration=434.708Âµs
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T16:15:00.921Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":12}
product-service-1  | Kafka producer connected for product-service
product-service-1  | Attempt 8 to start Kafka client failed, retrying in 30000ms { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Failed to start Kafka consumer after retries { error: 'Cannot subscribe to topic while consumer is running' }
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.761279798Z level=info msg="Executing migration" id="add unique index user_role_org_id_user_id_role_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.761749756Z level=info msg="Migration successfully executed" id="add unique index user_role_org_id_user_id_role_id" duration=470.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.76278509Z level=info msg="Executing migration" id="add index user_role.user_id"
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.763232256Z level=info msg="Migration successfully executed" id="add index user_role.user_id" duration=447.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.764215798Z level=info msg="Executing migration" id="create builtin role table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.76456084Z level=info msg="Migration successfully executed" id="create builtin role table" duration=345.333Âµs
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.765612548Z level=info msg="Executing migration" id="add index builtin_role.role_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.766111298Z level=info msg="Migration successfully executed" id="add index builtin_role.role_id" duration=500.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.767183881Z level=info msg="Executing migration" id="add index builtin_role.name"
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.767682381Z level=info msg="Migration successfully executed" id="add index builtin_role.name" duration=498.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.768732673Z level=info msg="Executing migration" id="Add column org_id to builtin_role table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.772708131Z level=info msg="Migration successfully executed" id="Add column org_id to builtin_role table" duration=3.975375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.77373534Z level=info msg="Executing migration" id="add index builtin_role.org_id"
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,392] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,393] INFO [Broker id=1] Finished LeaderAndIsr request in 630ms correlationId 3 from controller 1 for 50 partitions (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,398] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=88z2T5O8T7KV0KsEEMBkmQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=13, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=46, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=9, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=42, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=21, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=17, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=30, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=26, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=5, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=38, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=1, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=34, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=16, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=45, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=12, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=41, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=24, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=20, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=49, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=29, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=25, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=8, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=37, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=4, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=33, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=15, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=48, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=11, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=44, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=23, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=19, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=32, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=28, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=7, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=40, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=3, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=36, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=47, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=14, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=43, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=10, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=22, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=18, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=31, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=27, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=39, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=6, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=35, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=2, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 3 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,409] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 67 milliseconds for epoch 0, of which 57 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,413] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.774168673Z level=info msg="Migration successfully executed" id="add index builtin_role.org_id" duration=433.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.77518784Z level=info msg="Executing migration" id="add unique index builtin_role_org_id_role_id_role"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.775640923Z level=info msg="Migration successfully executed" id="add unique index builtin_role_org_id_role_id_role" duration=453.125Âµs
kafka-1            | [2026-02-25 11:40:10,414] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,414] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,414] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,414] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,414] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  |   [cause]: undefined
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError: Cannot subscribe to topic while consumer is running
grafana-1          | logger=migrator t=2026-02-25T11:40:02.776606756Z level=info msg="Executing migration" id="Remove unique index role_org_id_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.777071381Z level=info msg="Migration successfully executed" id="Remove unique index role_org_id_uid" duration=464.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.777862048Z level=info msg="Executing migration" id="add unique index role.uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.778285048Z level=info msg="Migration successfully executed" id="add unique index role.uid" duration=423.209Âµs
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  |     at Object.subscribe (/app/node_modules/kafkajs/src/consumer/index.js:136:13)
product-service-1  |     at startKafkaConsumer (/app/index.js:498:24)
product-service-1  |     at async start (/app/index.js:538:5) {
product-service-1  |   retriable: false,
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.779381798Z level=info msg="Executing migration" id="create seed assignment table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.779712298Z level=info msg="Migration successfully executed" id="create seed assignment table" duration=330.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.780715256Z level=info msg="Executing migration" id="add unique index builtin_role_role_name"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.781181548Z level=info msg="Migration successfully executed" id="add unique index builtin_role_role_name" duration=466.458Âµs
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  |   helpUrl: undefined,
product-service-1  |   [cause]: undefined
product-service-1  | }
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.782187048Z level=info msg="Executing migration" id="add column hidden to role table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.786214715Z level=info msg="Migration successfully executed" id="add column hidden to role table" duration=4.027792ms
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.787375756Z level=info msg="Executing migration" id="permission kind migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.791478256Z level=info msg="Migration successfully executed" id="permission kind migration" duration=4.10275ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.792285256Z level=info msg="Executing migration" id="permission attribute migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.796285756Z level=info msg="Migration successfully executed" id="permission attribute migration" duration=4.0005ms
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
kafka-1            | [2026-02-25 11:40:10,418] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 28 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.797325756Z level=info msg="Executing migration" id="permission identifier migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.801375465Z level=info msg="Migration successfully executed" id="permission identifier migration" duration=4.049042ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.80244534Z level=info msg="Executing migration" id="add permission identifier index"
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.80291309Z level=info msg="Migration successfully executed" id="add permission identifier index" duration=468Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.80402784Z level=info msg="Executing migration" id="add permission action scope role_id index"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.804498506Z level=info msg="Migration successfully executed" id="add permission action scope role_id index" duration=470.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.805690548Z level=info msg="Executing migration" id="remove permission role_id action scope index"
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.80613884Z level=info msg="Migration successfully executed" id="remove permission role_id action scope index" duration=448.416Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.807136423Z level=info msg="Executing migration" id="add group mapping UID column to user_role table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.811125298Z level=info msg="Migration successfully executed" id="add group mapping UID column to user_role table" duration=3.988958ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.811969298Z level=info msg="Executing migration" id="add user_role org ID, user ID, role ID, group mapping UID index"
kafka-1            | [2026-02-25 11:40:10,418] INFO [Broker id=1] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,418] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.812460423Z level=info msg="Migration successfully executed" id="add user_role org ID, user ID, role ID, group mapping UID index" duration=491.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.813382298Z level=info msg="Executing migration" id="remove user_role org ID, user ID, role ID index"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.813869965Z level=info msg="Migration successfully executed" id="remove user_role org ID, user ID, role ID index" duration=487.875Âµs
kafka-1            | [2026-02-25 11:40:10,418] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,418] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,418] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: â audit secrets and track compliance: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T16:15:26.804Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Product Service running on port 3002
product-service-1  | Executing (default): SELECT 1+1 AS result
grafana-1          | logger=migrator t=2026-02-25T11:40:02.81495909Z level=info msg="Executing migration" id="add permission role_id action index"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.81541809Z level=info msg="Migration successfully executed" id="add permission role_id action index" duration=459.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.816592923Z level=info msg="Executing migration" id="Remove permission role_id index"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.817087173Z level=info msg="Migration successfully executed" id="Remove permission role_id index" duration=494.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.818184256Z level=info msg="Executing migration" id="create query_history table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.818588465Z level=info msg="Migration successfully executed" id="create query_history table v1" duration=403.208Âµs
kafka-1            | [2026-02-25 11:40:10,418] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 28 milliseconds for epoch 0, of which 28 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.819410506Z level=info msg="Executing migration" id="add index query_history.org_id-created_by-datasource_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.819917131Z level=info msg="Migration successfully executed" id="add index query_history.org_id-created_by-datasource_uid" duration=506.875Âµs
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 11:40:10,419] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,419] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,419] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 29 milliseconds for epoch 0, of which 29 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,420] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,420] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,420] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,420] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,420] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,420] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,421] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 31 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.820949965Z level=info msg="Executing migration" id="alter table query_history alter column created_by type to bigint"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.820958548Z level=info msg="Migration successfully executed" id="alter table query_history alter column created_by type to bigint" duration=8.791Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.822037673Z level=info msg="Executing migration" id="create query_history_details table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.822362756Z level=info msg="Migration successfully executed" id="create query_history_details table v1" duration=325.125Âµs
kafka-1            | [2026-02-25 11:40:10,421] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 4 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2026-02-25 11:40:10,421] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,421] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
kafka-1            | [2026-02-25 11:40:10,421] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 31 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.823357881Z level=info msg="Executing migration" id="rbac disabled migrator"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.82338084Z level=info msg="Migration successfully executed" id="rbac disabled migrator" duration=24Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.82439709Z level=info msg="Executing migration" id="teams permissions migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.824573673Z level=info msg="Migration successfully executed" id="teams permissions migration" duration=176.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.825730173Z level=info msg="Executing migration" id="dashboard permissions"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.826015048Z level=info msg="Migration successfully executed" id="dashboard permissions" duration=284.166Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.827159923Z level=info msg="Executing migration" id="dashboard permissions uid scopes"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.827376756Z level=info msg="Migration successfully executed" id="dashboard permissions uid scopes" duration=216.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.828524048Z level=info msg="Executing migration" id="drop managed folder create actions"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.82861284Z level=info msg="Migration successfully executed" id="drop managed folder create actions" duration=88.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.82941909Z level=info msg="Executing migration" id="alerting notification permissions"
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 31 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 30 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,423] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 31 milliseconds for epoch 0, of which 30 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 32 milliseconds for epoch 0, of which 31 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.829626798Z level=info msg="Migration successfully executed" id="alerting notification permissions" duration=207.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.830664715Z level=info msg="Executing migration" id="create query_history_star table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.830994548Z level=info msg="Migration successfully executed" id="create query_history_star table v1" duration=329.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.832238506Z level=info msg="Executing migration" id="add index query_history.user_id-query_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.832743048Z level=info msg="Migration successfully executed" id="add index query_history.user_id-query_uid" duration=504.667Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.83378959Z level=info msg="Executing migration" id="add column org_id in query_history_star"
kafka-1            | [2026-02-25 11:40:10,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 32 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 32 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 32 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 32 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 32 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.837945048Z level=info msg="Migration successfully executed" id="add column org_id in query_history_star" duration=4.155667ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.839079423Z level=info msg="Executing migration" id="alter table query_history_star_mig column user_id type to bigint"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.839087131Z level=info msg="Migration successfully executed" id="alter table query_history_star_mig column user_id type to bigint" duration=8.792Âµs
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=migrator t=2026-02-25T11:40:02.840081215Z level=info msg="Executing migration" id="create correlation table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.840493173Z level=info msg="Migration successfully executed" id="create correlation table v1" duration=412.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.841548506Z level=info msg="Executing migration" id="add index correlations.uid"
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
kafka-1            | [2026-02-25 11:40:10,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 32 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 33 milliseconds for epoch 0, of which 32 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.842042048Z level=info msg="Migration successfully executed" id="add index correlations.uid" duration=493.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.843131215Z level=info msg="Executing migration" id="add index correlations.source_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.843614006Z level=info msg="Migration successfully executed" id="add index correlations.source_uid" duration=482.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.844644048Z level=info msg="Executing migration" id="add correlation config column"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.84886784Z level=info msg="Migration successfully executed" id="add correlation config column" duration=4.223541ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.849913506Z level=info msg="Executing migration" id="drop index IDX_correlation_uid - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.850367965Z level=info msg="Migration successfully executed" id="drop index IDX_correlation_uid - v1" duration=454.708Âµs
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2026-02-25 11:40:10,426] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 33 milliseconds for epoch 0, of which 33 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | Kafka producer connected for product-service
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T16:15:26.908Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:15:43.233Z","logger":"kafkajs","message":"[Consumer] Crash: KafkaJSRequestTimeoutError: Request JoinGroup(key: 11, version: 5) timed out","groupId":"product-group","stack":"KafkaJSRequestTimeoutError: Request JoinGroup(key: 11, version: 5) timed out\n    at SocketRequest.timeoutRequest (/app/node_modules/kafkajs/src/network/requestQueue/socketRequest.js:107:19)\n    at /app/node_modules/kafkajs/src/network/requestQueue/index.js:94:21\n    at Map.forEach (<anonymous>)\n    at Timeout._onTimeout (/app/node_modules/kafkajs/src/network/requestQueue/index.js:92:23)\n    at listOnTimeout (node:internal/timers:569:17)\n    at process.processTimers (node:internal/timers:512:7)"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:15:43.234Z","logger":"kafkajs","message":"[Consumer] Stopped","groupId":"product-group"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:15:43.235Z","logger":"kafkajs","message":"[Consumer] Restarting the consumer in 300ms","retryTime":300,"groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:15:43.245Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:15:43.535Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T17:16:04.440Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":4}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:16:04.447Z","logger":"kafkajs","message":"[Consumer] Crash: KafkaJSConnectionClosedError: Closed connection","groupId":"product-group","stack":"KafkaJSConnectionClosedError: Closed connection\n    at Socket.onEnd (/app/node_modules/kafkajs/src/network/connection.js:197:13)\n    at Socket.emit (node:events:529:35)\n    at endReadableNT (node:internal/streams/readable:1400:12)\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:16:04.448Z","logger":"kafkajs","message":"[Consumer] Stopped","groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:16:04.449Z","logger":"kafkajs","message":"[Consumer] Stopped","groupId":"product-group"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:16:04.449Z","logger":"kafkajs","message":"[Consumer] Crash: KafkaJSGroupCoordinatorNotFound: Failed to find group coordinator","groupId":"product-group","stack":"KafkaJSGroupCoordinatorNotFound: Failed to find group coordinator\n    at Cluster.findGroupCoordinatorMetadata (/app/node_modules/kafkajs/src/cluster/index.js:420:11)\n    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\n    at async /app/node_modules/kafkajs/src/cluster/index.js:346:33\n    at async [private:ConsumerGroup:join] (/app/node_modules/kafkajs/src/consumer/consumerGroup.js:167:24)\n    at async /app/node_modules/kafkajs/src/consumer/consumerGroup.js:335:9\n    at async Runner.start (/app/node_modules/kafkajs/src/consumer/runner.js:84:7)\n    at async start (/app/node_modules/kafkajs/src/consumer/index.js:243:7)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:16:04.449Z","logger":"kafkajs","message":"[Consumer] Restarting the consumer in 300ms","retryTime":300,"groupId":"product-group"}
product-service-1  | Kafka consumer started successfully
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T17:16:04.450Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":8}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.85132309Z level=info msg="Executing migration" id="drop index IDX_correlation_source_uid - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.851789256Z level=info msg="Migration successfully executed" id="drop index IDX_correlation_source_uid - v1" duration=466.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.85363909Z level=info msg="Executing migration" id="Rename table correlation to correlation_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.86726384Z level=info msg="Migration successfully executed" id="Rename table correlation to correlation_tmp_qwerty - v1" duration=13.617583ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.868469465Z level=info msg="Executing migration" id="create correlation v2"
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T17:16:04.454Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":9}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:16:04.750Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:49:29.468Z","logger":"kafkajs","message":"[Consumer] Crash: KafkaJSRequestTimeoutError: Request JoinGroup(key: 11, version: 5) timed out","groupId":"product-group","stack":"KafkaJSRequestTimeoutError: Request JoinGroup(key: 11, version: 5) timed out\n    at SocketRequest.timeoutRequest (/app/node_modules/kafkajs/src/network/requestQueue/socketRequest.js:107:19)\n    at /app/node_modules/kafkajs/src/network/requestQueue/index.js:94:21\n    at Map.forEach (<anonymous>)\n    at Timeout._onTimeout (/app/node_modules/kafkajs/src/network/requestQueue/index.js:92:23)\n    at listOnTimeout (node:internal/timers:569:17)\n    at process.processTimers (node:internal/timers:512:7)"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:49:29.469Z","logger":"kafkajs","message":"[Consumer] Stopped","groupId":"product-group"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:49:29.469Z","logger":"kafkajs","message":"[Consumer] Restarting the consumer in 300ms","retryTime":300,"groupId":"product-group"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:49:29.769Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.86892409Z level=info msg="Migration successfully executed" id="create correlation v2" duration=454.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.87006709Z level=info msg="Executing migration" id="create index IDX_correlation_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.870528381Z level=info msg="Migration successfully executed" id="create index IDX_correlation_uid - v2" duration=461.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.871605465Z level=info msg="Executing migration" id="create index IDX_correlation_source_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.872066923Z level=info msg="Migration successfully executed" id="create index IDX_correlation_source_uid - v2" duration=461.625Âµs
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T17:49:50.781Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":4}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:49:50.786Z","logger":"kafkajs","message":"[Consumer] Crash: KafkaJSConnectionClosedError: Closed connection","groupId":"product-group","stack":"KafkaJSConnectionClosedError: Closed connection\n    at Socket.onEnd (/app/node_modules/kafkajs/src/network/connection.js:197:13)\n    at Socket.emit (node:events:529:35)\n    at endReadableNT (node:internal/streams/readable:1400:12)\n    at process.processTicksAndRejections (node:internal/process/task_queues:82:21)"}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:49:50.786Z","logger":"kafkajs","message":"[Consumer] Stopped","groupId":"product-group"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T17:49:50.786Z","logger":"kafkajs","message":"[Consumer] Restarting the consumer in 300ms","retryTime":300,"groupId":"product-group"}
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T17:49:50.805Z","logger":"kafkajs","message":"[RequestQueue] Response without match","clientId":"kafkajs","broker":"kafka:9092","correlationId":10}
product-service-1  | {"level":"INFO","timestamp":"2026-02-25T17:49:51.087Z","logger":"kafkajs","message":"[Consumer] Starting","groupId":"product-group"}
grafana-1          | logger=migrator t=2026-02-25T11:40:02.873148756Z level=info msg="Executing migration" id="create index IDX_correlation_org_id - v2"
kafka-1            | [2026-02-25 11:40:10,659] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group order-service-group in Empty state. Created a new member id kafkajs-cdcde567-88ff-4efd-a00b-1f159868e7d4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,659] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Empty state. Created a new member id kafkajs-a56c28ad-49ed-4a21-bb33-efe803b10721 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,715] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 0 (__consumer_offsets-45) (reason: Adding new member kafkajs-a56c28ad-49ed-4a21-bb33-efe803b10721 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.873685548Z level=info msg="Migration successfully executed" id="create index IDX_correlation_org_id - v2" duration=536.916Âµs
kafka-1            | [2026-02-25 11:40:10,715] INFO [GroupCoordinator 1]: Preparing to rebalance group order-service-group in state PreparingRebalance with old generation 0 (__consumer_offsets-32) (reason: Adding new member kafkajs-cdcde567-88ff-4efd-a00b-1f159868e7d4 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,730] INFO [GroupCoordinator 1]: Stabilized group order-service-group generation 1 (__consumer_offsets-32) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,740] INFO [GroupCoordinator 1]: Stabilized group product-group generation 1 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,791] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-a56c28ad-49ed-4a21-bb33-efe803b10721 for group product-group for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:40:10,791] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-cdcde567-88ff-4efd-a00b-1f159868e7d4 for group order-service-group for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:41:42,945] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-af4bca03-bc62-4960-9a51-67c698402a91 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:41:42,958] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 1 (__consumer_offsets-45) (reason: Adding new member kafkajs-af4bca03-bc62-4960-9a51-67c698402a91 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:42:11,663] INFO [GroupCoordinator 1]: Member kafkajs-a56c28ad-49ed-4a21-bb33-efe803b10721 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:42:11,670] INFO [GroupCoordinator 1]: Stabilized group product-group generation 2 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:42:11,692] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-af4bca03-bc62-4960-9a51-67c698402a91 for group product-group for generation 2. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:43:34,200] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:43:34,203] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:43:34,225] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.874680923Z level=info msg="Executing migration" id="copy correlation v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.874778215Z level=info msg="Migration successfully executed" id="copy correlation v1 to v2" duration=97.417Âµs
kafka-1            | [2026-02-25 11:43:34,257] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:43:44,011] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-007ab1b0-33d5-4e54-9932-c74912242268 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:43:44,019] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 2 (__consumer_offsets-45) (reason: Adding new member kafkajs-007ab1b0-33d5-4e54-9932-c74912242268 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:44:07,677] INFO [GroupCoordinator 1]: Member kafkajs-af4bca03-bc62-4960-9a51-67c698402a91 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.876198881Z level=info msg="Executing migration" id="drop correlation_tmp_qwerty"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.876518965Z level=info msg="Migration successfully executed" id="drop correlation_tmp_qwerty" duration=321.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.877519006Z level=info msg="Executing migration" id="add provisioning column"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.881846715Z level=info msg="Migration successfully executed" id="add provisioning column" duration=4.327625ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.883121798Z level=info msg="Executing migration" id="add type column"
kafka-1            | [2026-02-25 11:44:07,695] INFO [GroupCoordinator 1]: Stabilized group product-group generation 3 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:44:07,732] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-007ab1b0-33d5-4e54-9932-c74912242268 for group product-group for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:44:12,769] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-461c0a3b-2a4a-45b2-809c-6ebdeeeac3c1 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.887396381Z level=info msg="Migration successfully executed" id="add type column" duration=4.273625ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.888657881Z level=info msg="Executing migration" id="create entity_events table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.889120006Z level=info msg="Migration successfully executed" id="create entity_events table" duration=462.125Âµs
kafka-1            | [2026-02-25 11:44:12,776] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 3 (__consumer_offsets-45) (reason: Adding new member kafkajs-461c0a3b-2a4a-45b2-809c-6ebdeeeac3c1 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:44:37,751] INFO [GroupCoordinator 1]: Member kafkajs-007ab1b0-33d5-4e54-9932-c74912242268 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:44:37,764] INFO [GroupCoordinator 1]: Stabilized group product-group generation 4 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.890054298Z level=info msg="Executing migration" id="create dashboard public config v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.890530631Z level=info msg="Migration successfully executed" id="create dashboard public config v1" duration=476.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.891697131Z level=info msg="Executing migration" id="drop index UQE_dashboard_public_config_uid - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.891923381Z level=warn msg="Skipping migration: Already executed, but not recorded in migration log" id="drop index UQE_dashboard_public_config_uid - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.892884965Z level=info msg="Executing migration" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.893123048Z level=warn msg="Skipping migration: Already executed, but not recorded in migration log" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v1"
kafka-1            | [2026-02-25 11:44:37,794] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-461c0a3b-2a4a-45b2-809c-6ebdeeeac3c1 for group product-group for generation 4. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:46:09,781] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-87d3b683-ad23-4c0b-bf51-d2e676f8fbbf and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:46:09,787] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 4 (__consumer_offsets-45) (reason: Adding new member kafkajs-87d3b683-ad23-4c0b-bf51-d2e676f8fbbf with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.89393959Z level=info msg="Executing migration" id="Drop old dashboard public config table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.894316006Z level=info msg="Migration successfully executed" id="Drop old dashboard public config table" duration=375.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.895334298Z level=info msg="Executing migration" id="recreate dashboard public config v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.895788465Z level=info msg="Migration successfully executed" id="recreate dashboard public config v1" duration=454.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.896822173Z level=info msg="Executing migration" id="create index UQE_dashboard_public_config_uid - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.897404631Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_public_config_uid - v1" duration=582.208Âµs
kafka-1            | [2026-02-25 11:46:38,298] INFO [GroupCoordinator 1]: Member kafkajs-461c0a3b-2a4a-45b2-809c-6ebdeeeac3c1 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:46:38,303] INFO [GroupCoordinator 1]: Stabilized group product-group generation 5 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.898575298Z level=info msg="Executing migration" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.899062756Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v1" duration=487.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.900147173Z level=info msg="Executing migration" id="drop index UQE_dashboard_public_config_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.900637256Z level=info msg="Migration successfully executed" id="drop index UQE_dashboard_public_config_uid - v2" duration=489.834Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.901751256Z level=info msg="Executing migration" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.902218965Z level=info msg="Migration successfully executed" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v2" duration=467.917Âµs
kafka-1            | [2026-02-25 11:46:38,325] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-87d3b683-ad23-4c0b-bf51-d2e676f8fbbf for group product-group for generation 5. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:48:10,171] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-af7c0c7b-481f-4c09-827f-32dbfb29a571 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:48:10,183] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 5 (__consumer_offsets-45) (reason: Adding new member kafkajs-af7c0c7b-481f-4c09-827f-32dbfb29a571 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:48:34,268] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:48:34,269] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:48:34,285] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.903022548Z level=info msg="Executing migration" id="Drop public config table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.903338756Z level=info msg="Migration successfully executed" id="Drop public config table" duration=316.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.904370631Z level=info msg="Executing migration" id="Recreate dashboard public config v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.904850715Z level=info msg="Migration successfully executed" id="Recreate dashboard public config v2" duration=479.708Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.905886173Z level=info msg="Executing migration" id="create index UQE_dashboard_public_config_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.906364881Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_public_config_uid - v2" duration=479.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.907436923Z level=info msg="Executing migration" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.907942381Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v2" duration=505.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.908722881Z level=info msg="Executing migration" id="create index UQE_dashboard_public_config_access_token - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.909181048Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_public_config_access_token - v2" duration=458.084Âµs
kafka-1            | [2026-02-25 11:48:34,285] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:48:38,829] INFO [GroupCoordinator 1]: Member kafkajs-87d3b683-ad23-4c0b-bf51-d2e676f8fbbf in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:48:38,844] INFO [GroupCoordinator 1]: Stabilized group product-group generation 6 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:48:38,891] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-af7c0c7b-481f-4c09-827f-32dbfb29a571 for group product-group for generation 6. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.910275715Z level=info msg="Executing migration" id="Rename table dashboard_public_config to dashboard_public - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.923065131Z level=info msg="Migration successfully executed" id="Rename table dashboard_public_config to dashboard_public - v2" duration=12.789083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.924082173Z level=info msg="Executing migration" id="add annotations_enabled column"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.928543715Z level=info msg="Migration successfully executed" id="add annotations_enabled column" duration=4.461583ms
kafka-1            | [2026-02-25 11:50:10,771] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-36e6cb21-18f1-472a-bb36-cab263e728cf and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:50:10,781] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 6 (__consumer_offsets-45) (reason: Adding new member kafkajs-36e6cb21-18f1-472a-bb36-cab263e728cf with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:50:39,638] INFO [GroupCoordinator 1]: Member kafkajs-af7c0c7b-481f-4c09-827f-32dbfb29a571 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:50:39,652] INFO [GroupCoordinator 1]: Stabilized group product-group generation 7 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.92947584Z level=info msg="Executing migration" id="add time_selection_enabled column"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.933832423Z level=info msg="Migration successfully executed" id="add time_selection_enabled column" duration=4.356458ms
kafka-1            | [2026-02-25 11:50:39,679] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-36e6cb21-18f1-472a-bb36-cab263e728cf for group product-group for generation 7. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:52:11,462] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-bcd59c78-5dad-48e8-bcaa-12966634cef2 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.934818756Z level=info msg="Executing migration" id="delete orphaned public dashboards"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.93493259Z level=info msg="Migration successfully executed" id="delete orphaned public dashboards" duration=113.667Âµs
kafka-1            | [2026-02-25 11:52:11,468] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 7 (__consumer_offsets-45) (reason: Adding new member kafkajs-bcd59c78-5dad-48e8-bcaa-12966634cef2 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:52:40,209] INFO [GroupCoordinator 1]: Member kafkajs-36e6cb21-18f1-472a-bb36-cab263e728cf in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:52:40,222] INFO [GroupCoordinator 1]: Stabilized group product-group generation 8 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:52:40,250] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-bcd59c78-5dad-48e8-bcaa-12966634cef2 for group product-group for generation 8. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:53:34,295] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:53:34,299] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:53:34,323] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:53:34,324] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:54:12,329] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-b675b376-d653-45f2-9b8b-3f6edb2e96a0 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:54:12,338] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 8 (__consumer_offsets-45) (reason: Adding new member kafkajs-b675b376-d653-45f2-9b8b-3f6edb2e96a0 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.935934715Z level=info msg="Executing migration" id="add share column"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.940519715Z level=info msg="Migration successfully executed" id="add share column" duration=4.583708ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.943266298Z level=info msg="Executing migration" id="backfill empty share column fields with default of public"
kafka-1            | [2026-02-25 11:54:40,767] INFO [GroupCoordinator 1]: Member kafkajs-bcd59c78-5dad-48e8-bcaa-12966634cef2 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:54:40,783] INFO [GroupCoordinator 1]: Stabilized group product-group generation 9 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:54:40,810] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-b675b376-d653-45f2-9b8b-3f6edb2e96a0 for group product-group for generation 9. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:56:12,664] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-70b34d36-756b-47f1-a472-1a8229a502f3 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:56:12,670] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 9 (__consumer_offsets-45) (reason: Adding new member kafkajs-70b34d36-756b-47f1-a472-1a8229a502f3 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:56:41,497] INFO [GroupCoordinator 1]: Member kafkajs-b675b376-d653-45f2-9b8b-3f6edb2e96a0 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:56:41,502] INFO [GroupCoordinator 1]: Stabilized group product-group generation 10 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:56:41,512] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-70b34d36-756b-47f1-a472-1a8229a502f3 for group product-group for generation 10. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:58:13,318] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-ba9fa6a2-cc98-43ff-8d47-2ef05293dc9a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:58:13,325] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 10 (__consumer_offsets-45) (reason: Adding new member kafkajs-ba9fa6a2-cc98-43ff-8d47-2ef05293dc9a with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.943353715Z level=info msg="Migration successfully executed" id="backfill empty share column fields with default of public" duration=87.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.944488381Z level=info msg="Executing migration" id="create file table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.945088965Z level=info msg="Migration successfully executed" id="create file table" duration=600.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.946420465Z level=info msg="Executing migration" id="file table idx: path natural pk"
kafka-1            | [2026-02-25 11:58:34,336] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:58:34,338] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:58:34,353] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:58:34,354] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 11:58:42,205] INFO [GroupCoordinator 1]: Member kafkajs-70b34d36-756b-47f1-a472-1a8229a502f3 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:58:42,218] INFO [GroupCoordinator 1]: Stabilized group product-group generation 11 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 11:58:42,250] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-ba9fa6a2-cc98-43ff-8d47-2ef05293dc9a for group product-group for generation 11. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:00:14,356] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-95896526-f3a8-4a67-ab56-ea7c469cbc36 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:00:14,366] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 11 (__consumer_offsets-45) (reason: Adding new member kafkajs-95896526-f3a8-4a67-ab56-ea7c469cbc36 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:00:42,825] INFO [GroupCoordinator 1]: Member kafkajs-ba9fa6a2-cc98-43ff-8d47-2ef05293dc9a in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:00:42,834] INFO [GroupCoordinator 1]: Stabilized group product-group generation 12 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:00:42,920] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-95896526-f3a8-4a67-ab56-ea7c469cbc36 for group product-group for generation 12. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:02:14,812] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-0ece8492-2c8c-4684-9144-a1d5692ede2f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:02:14,818] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 12 (__consumer_offsets-45) (reason: Adding new member kafkajs-0ece8492-2c8c-4684-9144-a1d5692ede2f with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:02:43,628] INFO [GroupCoordinator 1]: Member kafkajs-95896526-f3a8-4a67-ab56-ea7c469cbc36 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:02:43,640] INFO [GroupCoordinator 1]: Stabilized group product-group generation 13 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:02:43,679] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-0ece8492-2c8c-4684-9144-a1d5692ede2f for group product-group for generation 13. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:03:34,361] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 12:03:34,362] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 12:03:34,374] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 12:03:34,375] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.94743659Z level=info msg="Migration successfully executed" id="file table idx: path natural pk" duration=1.01625ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.948785881Z level=info msg="Executing migration" id="file table idx: parent_folder_path_hash fast folder retrieval"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.94932909Z level=info msg="Migration successfully executed" id="file table idx: parent_folder_path_hash fast folder retrieval" duration=543.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.950680756Z level=info msg="Executing migration" id="create file_meta table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.951124465Z level=info msg="Migration successfully executed" id="create file_meta table" duration=443.584Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.952266173Z level=info msg="Executing migration" id="file table idx: path key"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.952798173Z level=info msg="Migration successfully executed" id="file table idx: path key" duration=532.084Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.953806631Z level=info msg="Executing migration" id="set path collation in file table"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.953815715Z level=info msg="Migration successfully executed" id="set path collation in file table" duration=9.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.954882298Z level=info msg="Executing migration" id="migrate contents column to mediumblob for MySQL"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.95489209Z level=info msg="Migration successfully executed" id="migrate contents column to mediumblob for MySQL" duration=10.291Âµs
kafka-1            | [2026-02-25 12:04:15,672] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-87d84a3a-47f5-4a88-9081-27b6e4260cbe and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:04:15,679] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 13 (__consumer_offsets-45) (reason: Adding new member kafkajs-87d84a3a-47f5-4a88-9081-27b6e4260cbe with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:04:44,356] INFO [GroupCoordinator 1]: Member kafkajs-0ece8492-2c8c-4684-9144-a1d5692ede2f in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:04:44,367] INFO [GroupCoordinator 1]: Stabilized group product-group generation 14 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:04:44,392] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-87d84a3a-47f5-4a88-9081-27b6e4260cbe for group product-group for generation 14. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:06:16,425] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-9eccc8c6-83ad-4f15-8624-550d1925e2f4 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:06:16,433] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 14 (__consumer_offsets-45) (reason: Adding new member kafkajs-9eccc8c6-83ad-4f15-8624-550d1925e2f4 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:06:45,255] INFO [GroupCoordinator 1]: Member kafkajs-87d84a3a-47f5-4a88-9081-27b6e4260cbe in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:06:45,261] INFO [GroupCoordinator 1]: Stabilized group product-group generation 15 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:06:45,282] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-9eccc8c6-83ad-4f15-8624-550d1925e2f4 for group product-group for generation 15. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:08:17,281] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-19fff72d-85ca-49e7-9e70-67c175845fd1 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.955902131Z level=info msg="Executing migration" id="managed permissions migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.956147048Z level=info msg="Migration successfully executed" id="managed permissions migration" duration=245.125Âµs
kafka-1            | [2026-02-25 12:08:17,287] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 15 (__consumer_offsets-45) (reason: Adding new member kafkajs-19fff72d-85ca-49e7-9e70-67c175845fd1 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:08:34,386] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 12:08:34,387] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 12:08:34,399] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 12:08:34,400] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 12:08:45,845] INFO [GroupCoordinator 1]: Member kafkajs-9eccc8c6-83ad-4f15-8624-550d1925e2f4 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:08:45,850] INFO [GroupCoordinator 1]: Stabilized group product-group generation 16 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:08:45,898] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-19fff72d-85ca-49e7-9e70-67c175845fd1 for group product-group for generation 16. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:10:17,739] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-24b218aa-edcc-4077-b598-b0ae75a47b97 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:10:17,748] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 16 (__consumer_offsets-45) (reason: Adding new member kafkajs-24b218aa-edcc-4077-b598-b0ae75a47b97 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:10:46,580] INFO [GroupCoordinator 1]: Member kafkajs-19fff72d-85ca-49e7-9e70-67c175845fd1 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:10:46,593] INFO [GroupCoordinator 1]: Stabilized group product-group generation 17 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 12:10:46,633] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-24b218aa-edcc-4077-b598-b0ae75a47b97 for group product-group for generation 17. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 13:39:43,198] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-d35adf07-45d6-45fe-8111-5c9f395944bb and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 13:39:43,225] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 17 (__consumer_offsets-45) (reason: Adding new member kafkajs-d35adf07-45d6-45fe-8111-5c9f395944bb with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.956990673Z level=info msg="Executing migration" id="managed folder permissions alert actions migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.957071423Z level=info msg="Migration successfully executed" id="managed folder permissions alert actions migration" duration=82.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.958309215Z level=info msg="Executing migration" id="RBAC action name migrator"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.958846965Z level=info msg="Migration successfully executed" id="RBAC action name migrator" duration=538.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.96001609Z level=info msg="Executing migration" id="Add UID column to playlist"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.964652173Z level=info msg="Migration successfully executed" id="Add UID column to playlist" duration=4.636125ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.965783173Z level=info msg="Executing migration" id="Update uid column values in playlist"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.965841298Z level=info msg="Migration successfully executed" id="Update uid column values in playlist" duration=58.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.966996381Z level=info msg="Executing migration" id="Add index for uid in playlist"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.96749259Z level=info msg="Migration successfully executed" id="Add index for uid in playlist" duration=496.291Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.968499215Z level=info msg="Executing migration" id="update group index for alert rules"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.968710006Z level=info msg="Migration successfully executed" id="update group index for alert rules" duration=210.708Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.969724923Z level=info msg="Executing migration" id="managed folder permissions alert actions repeated migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.969818048Z level=info msg="Migration successfully executed" id="managed folder permissions alert actions repeated migration" duration=93.417Âµs
kafka-1            | [2026-02-25 13:40:06,803] INFO [GroupCoordinator 1]: Member kafkajs-24b218aa-edcc-4077-b598-b0ae75a47b97 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 13:40:06,821] INFO [GroupCoordinator 1]: Stabilized group product-group generation 18 (__consumer_offsets-45) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 13:40:06,865] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-d35adf07-45d6-45fe-8111-5c9f395944bb for group product-group for generation 18. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 15:14:12,451] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 15:14:12,451] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 15:14:12,468] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 15:14:12,468] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2026-02-25 16:15:26,913] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-421eafb0-6693-49d7-9f73-97da6c7e3d3f and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 16:15:26,919] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 18 (__consumer_offsets-45) (reason: Adding new member kafkajs-421eafb0-6693-49d7-9f73-97da6c7e3d3f with group instance id None) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:15:43,250] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in PreparingRebalance state. Created a new member id kafkajs-3c08da6b-7704-48c9-9700-92191c875307 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:16:04,417] INFO [GroupCoordinator 1]: Member kafkajs-d35adf07-45d6-45fe-8111-5c9f395944bb in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:16:04,432] INFO [GroupCoordinator 1]: Stabilized group product-group generation 19 (__consumer_offsets-45) with 2 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:16:04,760] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in CompletingRebalance state. Created a new member id kafkajs-23bb4581-da99-47d7-ac38-3d4fa18f353a and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:16:04,789] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 19 (__consumer_offsets-45) (reason: Adding new member kafkajs-23bb4581-da99-47d7-ac38-3d4fa18f353a with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.970868465Z level=info msg="Executing migration" id="admin only folder/dashboard permission"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.97111484Z level=info msg="Migration successfully executed" id="admin only folder/dashboard permission" duration=246.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.971915965Z level=info msg="Executing migration" id="add action column to seed_assignment"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.976414173Z level=info msg="Migration successfully executed" id="add action column to seed_assignment" duration=4.497542ms
grafana-1          | logger=migrator t=2026-02-25T11:40:02.977466215Z level=info msg="Executing migration" id="add scope column to seed_assignment"
kafka-1            | [2026-02-25 17:49:29,540] INFO [GroupCoordinator 1]: Preparing to rebalance group order-service-group in state PreparingRebalance with old generation 1 (__consumer_offsets-32) (reason: Removing member kafkajs-cdcde567-88ff-4efd-a00b-1f159868e7d4 on LeaveGroup) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:29,542] INFO [GroupCoordinator 1]: Group order-service-group with generation 2 is now empty (__consumer_offsets-32) (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:29,555] INFO [GroupCoordinator 1]: Member MemberMetadata(memberId=kafkajs-cdcde567-88ff-4efd-a00b-1f159868e7d4, groupInstanceId=None, clientId=kafkajs, clientHost=/172.18.0.9, sessionTimeoutMs=30000, rebalanceTimeoutMs=60000, supportedProtocols=List(RoundRobinAssigner)) has left group order-service-group through explicit `LeaveGroup` request (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.981967048Z level=info msg="Migration successfully executed" id="add scope column to seed_assignment" duration=4.500833ms
kafka-1            | [2026-02-25 17:49:29,781] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in PreparingRebalance state. Created a new member id kafkajs-87e271ff-8855-4ce2-96f2-e7a6920b36a6 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:35,818] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group order-service-group in Empty state. Created a new member id kafkajs-ae6c393c-7373-481a-b7ed-56bd68030f22 and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:35,830] INFO [GroupCoordinator 1]: Preparing to rebalance group order-service-group in state PreparingRebalance with old generation 2 (__consumer_offsets-32) (reason: Adding new member kafkajs-ae6c393c-7373-481a-b7ed-56bd68030f22 with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:02.982953798Z level=info msg="Executing migration" id="remove unique index builtin_role_role_name before nullable update"
grafana-1          | logger=migrator t=2026-02-25T11:40:02.983428173Z level=info msg="Migration successfully executed" id="remove unique index builtin_role_role_name before nullable update" duration=475.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:02.984463423Z level=info msg="Executing migration" id="update seed_assignment role_name column to nullable"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.034286256Z level=info msg="Migration successfully executed" id="update seed_assignment role_name column to nullable" duration=49.819875ms
kafka-1            | [2026-02-25 17:49:35,841] INFO [GroupCoordinator 1]: Stabilized group order-service-group generation 3 (__consumer_offsets-32) with 1 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:35,853] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-ae6c393c-7373-481a-b7ed-56bd68030f22 for group order-service-group for generation 3. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:50,743] INFO [GroupCoordinator 1]: Member kafkajs-421eafb0-6693-49d7-9f73-97da6c7e3d3f in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:50,747] INFO [GroupCoordinator 1]: Member kafkajs-3c08da6b-7704-48c9-9700-92191c875307 in group product-group has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:03.035953798Z level=info msg="Executing migration" id="add unique index builtin_role_name back"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.036578923Z level=info msg="Migration successfully executed" id="add unique index builtin_role_name back" duration=625.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.038610215Z level=info msg="Executing migration" id="add unique index builtin_role_action_scope"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.03918709Z level=info msg="Migration successfully executed" id="add unique index builtin_role_action_scope" duration=576.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.040626631Z level=info msg="Executing migration" id="add primary key to seed_assigment"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.05615784Z level=info msg="Migration successfully executed" id="add primary key to seed_assigment" duration=15.530083ms
kafka-1            | [2026-02-25 17:49:50,756] INFO [GroupCoordinator 1]: Stabilized group product-group generation 20 (__consumer_offsets-45) with 2 members (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:50,787] INFO [GroupCoordinator 1]: Assignment received from leader kafkajs-87e271ff-8855-4ce2-96f2-e7a6920b36a6 for group product-group for generation 20. The group has 2 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:50,792] WARN [GroupCoordinator 1]: Setting empty assignments for members Set(kafkajs-87e271ff-8855-4ce2-96f2-e7a6920b36a6) of product-group for generation 20 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:50,801] WARN [GroupCoordinator 1]: Sending empty assignment to member kafkajs-87e271ff-8855-4ce2-96f2-e7a6920b36a6 of product-group for generation 20 with no errors (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:03.057917173Z level=info msg="Executing migration" id="add origin column to seed_assignment"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.062622006Z level=info msg="Migration successfully executed" id="add origin column to seed_assignment" duration=4.704541ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.06423059Z level=info msg="Executing migration" id="add origin to plugin seed_assignment"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.06437359Z level=info msg="Migration successfully executed" id="add origin to plugin seed_assignment" duration=143.292Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.066421548Z level=info msg="Executing migration" id="prevent seeding OnCall access"
kafka-1            | [2026-02-25 17:49:51,105] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group product-group in Stable state. Created a new member id kafkajs-6c4587bb-f0b4-412b-b91d-f7d90884a7af and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2026-02-25 17:49:51,124] INFO [GroupCoordinator 1]: Preparing to rebalance group product-group in state PreparingRebalance with old generation 20 (__consumer_offsets-45) (reason: Adding new member kafkajs-6c4587bb-f0b4-412b-b91d-f7d90884a7af with group instance id None) (kafka.coordinator.group.GroupCoordinator)
grafana-1          | logger=migrator t=2026-02-25T11:40:03.066520006Z level=info msg="Migration successfully executed" id="prevent seeding OnCall access" duration=98.834Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.068317756Z level=info msg="Executing migration" id="managed folder permissions alert actions repeated fixed migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.06840334Z level=info msg="Migration successfully executed" id="managed folder permissions alert actions repeated fixed migration" duration=85.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.069858923Z level=info msg="Executing migration" id="managed folder permissions library panel actions migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.069986131Z level=info msg="Migration successfully executed" id="managed folder permissions library panel actions migration" duration=126.084Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.071350506Z level=info msg="Executing migration" id="migrate external alertmanagers to datsourcse"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.071453048Z level=info msg="Migration successfully executed" id="migrate external alertmanagers to datsourcse" duration=102.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.072630006Z level=info msg="Executing migration" id="create folder table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.073136715Z level=info msg="Migration successfully executed" id="create folder table" duration=517.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.074813298Z level=info msg="Executing migration" id="Add index for parent_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.075393715Z level=info msg="Migration successfully executed" id="Add index for parent_uid" duration=580.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.077436923Z level=info msg="Executing migration" id="Add unique index for folder.uid and folder.org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.078026006Z level=info msg="Migration successfully executed" id="Add unique index for folder.uid and folder.org_id" duration=589.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.080367548Z level=info msg="Executing migration" id="Update folder title length"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.080384506Z level=info msg="Migration successfully executed" id="Update folder title length" duration=17.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.08164884Z level=info msg="Executing migration" id="Add unique index for folder.title and folder.parent_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.082312631Z level=info msg="Migration successfully executed" id="Add unique index for folder.title and folder.parent_uid" duration=664.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.083809215Z level=info msg="Executing migration" id="Remove unique index for folder.title and folder.parent_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.084383423Z level=info msg="Migration successfully executed" id="Remove unique index for folder.title and folder.parent_uid" duration=574.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.086521631Z level=info msg="Executing migration" id="Add unique index for title, parent_uid, and org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.087118798Z level=info msg="Migration successfully executed" id="Add unique index for title, parent_uid, and org_id" duration=597.084Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.088505423Z level=info msg="Executing migration" id="Sync dashboard and folder table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.088713506Z level=info msg="Migration successfully executed" id="Sync dashboard and folder table" duration=208.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.089806506Z level=info msg="Executing migration" id="Remove ghost folders from the folder table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.089931131Z level=info msg="Migration successfully executed" id="Remove ghost folders from the folder table" duration=127.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.091954256Z level=info msg="Executing migration" id="Remove unique index UQE_folder_uid_org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.092504673Z level=info msg="Migration successfully executed" id="Remove unique index UQE_folder_uid_org_id" duration=551.041Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.093871631Z level=info msg="Executing migration" id="Add unique index UQE_folder_org_id_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.094523173Z level=info msg="Migration successfully executed" id="Add unique index UQE_folder_org_id_uid" duration=651.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.09587084Z level=info msg="Executing migration" id="Remove unique index UQE_folder_title_parent_uid_org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.096482506Z level=info msg="Migration successfully executed" id="Remove unique index UQE_folder_title_parent_uid_org_id" duration=611.916Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.097625506Z level=info msg="Executing migration" id="Add unique index UQE_folder_org_id_parent_uid_title"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.098203715Z level=info msg="Migration successfully executed" id="Add unique index UQE_folder_org_id_parent_uid_title" duration=578.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.100508715Z level=info msg="Executing migration" id="Remove index IDX_folder_parent_uid_org_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.101105715Z level=info msg="Migration successfully executed" id="Remove index IDX_folder_parent_uid_org_id" duration=596.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.102640048Z level=info msg="Executing migration" id="Remove unique index UQE_folder_org_id_parent_uid_title"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.103172673Z level=info msg="Migration successfully executed" id="Remove unique index UQE_folder_org_id_parent_uid_title" duration=532.667Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.104983215Z level=info msg="Executing migration" id="Add index IDX_folder_org_id_parent_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.105527381Z level=info msg="Migration successfully executed" id="Add index IDX_folder_org_id_parent_uid" duration=540Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.106608548Z level=info msg="Executing migration" id="create anon_device table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.107039506Z level=info msg="Migration successfully executed" id="create anon_device table" duration=431.084Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.108109298Z level=info msg="Executing migration" id="add unique index anon_device.device_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.10866184Z level=info msg="Migration successfully executed" id="add unique index anon_device.device_id" duration=552.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.109712173Z level=info msg="Executing migration" id="add index anon_device.updated_at"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.11023684Z level=info msg="Migration successfully executed" id="add index anon_device.updated_at" duration=524.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.111307006Z level=info msg="Executing migration" id="create signing_key table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.111780298Z level=info msg="Migration successfully executed" id="create signing_key table" duration=473.416Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.112841006Z level=info msg="Executing migration" id="add unique index signing_key.key_id"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.113356798Z level=info msg="Migration successfully executed" id="add unique index signing_key.key_id" duration=515.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.114495548Z level=info msg="Executing migration" id="set legacy alert migration status in kvstore"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.115073923Z level=info msg="Migration successfully executed" id="set legacy alert migration status in kvstore" duration=578.458Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.116126173Z level=info msg="Executing migration" id="migrate record of created folders during legacy migration to kvstore"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.11624509Z level=info msg="Migration successfully executed" id="migrate record of created folders during legacy migration to kvstore" duration=119.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.117093173Z level=info msg="Executing migration" id="Add folder_uid for dashboard"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.122153298Z level=info msg="Migration successfully executed" id="Add folder_uid for dashboard" duration=5.060042ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.123415256Z level=info msg="Executing migration" id="Populate dashboard folder_uid column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.123793923Z level=info msg="Migration successfully executed" id="Populate dashboard folder_uid column" duration=379.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.124922798Z level=info msg="Executing migration" id="Add unique index for dashboard_org_id_folder_uid_title"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.12493309Z level=info msg="Migration successfully executed" id="Add unique index for dashboard_org_id_folder_uid_title" duration=10.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.12611759Z level=info msg="Executing migration" id="Delete unique index for dashboard_org_id_folder_id_title"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.126731215Z level=info msg="Migration successfully executed" id="Delete unique index for dashboard_org_id_folder_id_title" duration=613.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.12768309Z level=info msg="Executing migration" id="Delete unique index for dashboard_org_id_folder_uid_title"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.127696798Z level=info msg="Migration successfully executed" id="Delete unique index for dashboard_org_id_folder_uid_title" duration=14.875Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.128810423Z level=info msg="Executing migration" id="Add unique index for dashboard_org_id_folder_uid_title_is_folder"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.129352423Z level=info msg="Migration successfully executed" id="Add unique index for dashboard_org_id_folder_uid_title_is_folder" duration=542.291Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.130289923Z level=info msg="Executing migration" id="Restore index for dashboard_org_id_folder_id_title"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.13083534Z level=info msg="Migration successfully executed" id="Restore index for dashboard_org_id_folder_id_title" duration=545.541Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.131994631Z level=info msg="Executing migration" id="Remove unique index for dashboard_org_id_folder_uid_title_is_folder"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.132492506Z level=info msg="Migration successfully executed" id="Remove unique index for dashboard_org_id_folder_uid_title_is_folder" duration=497.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.133856881Z level=info msg="Executing migration" id="create sso_setting table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.134308756Z level=info msg="Migration successfully executed" id="create sso_setting table" duration=452.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.135456506Z level=info msg="Executing migration" id="copy kvstore migration status to each org"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.135726631Z level=info msg="Migration successfully executed" id="copy kvstore migration status to each org" duration=270.583Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.136538256Z level=info msg="Executing migration" id="add back entry for orgid=0 migrated status"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.13666084Z level=info msg="Migration successfully executed" id="add back entry for orgid=0 migrated status" duration=122.75Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.137638715Z level=info msg="Executing migration" id="managed dashboard permissions annotation actions migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.137916048Z level=info msg="Migration successfully executed" id="managed dashboard permissions annotation actions migration" duration=277.416Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.139174715Z level=info msg="Executing migration" id="create cloud_migration table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.139602048Z level=info msg="Migration successfully executed" id="create cloud_migration table v1" duration=424.542Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.140575756Z level=info msg="Executing migration" id="create cloud_migration_run table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.14101134Z level=info msg="Migration successfully executed" id="create cloud_migration_run table v1" duration=440.709Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.14211559Z level=info msg="Executing migration" id="add stack_id column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.147062673Z level=info msg="Migration successfully executed" id="add stack_id column" duration=4.947083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.148218756Z level=info msg="Executing migration" id="add region_slug column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.153628423Z level=info msg="Migration successfully executed" id="add region_slug column" duration=5.407125ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.154669381Z level=info msg="Executing migration" id="add cluster_slug column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.15963234Z level=info msg="Migration successfully executed" id="add cluster_slug column" duration=4.960917ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.160906131Z level=info msg="Executing migration" id="add migration uid column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.165818298Z level=info msg="Migration successfully executed" id="add migration uid column" duration=4.912167ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.166781548Z level=info msg="Executing migration" id="Update uid column values for migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.166843423Z level=info msg="Migration successfully executed" id="Update uid column values for migration" duration=63.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.167872881Z level=info msg="Executing migration" id="Add unique index migration_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.16841384Z level=info msg="Migration successfully executed" id="Add unique index migration_uid" duration=540.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.16957959Z level=info msg="Executing migration" id="add migration run uid column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.174380298Z level=info msg="Migration successfully executed" id="add migration run uid column" duration=4.799458ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.175522673Z level=info msg="Executing migration" id="Update uid column values for migration run"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.175599506Z level=info msg="Migration successfully executed" id="Update uid column values for migration run" duration=77.167Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.176619173Z level=info msg="Executing migration" id="Add unique index migration_run_uid"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.17716534Z level=info msg="Migration successfully executed" id="Add unique index migration_run_uid" duration=546.209Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.178510756Z level=info msg="Executing migration" id="Rename table cloud_migration to cloud_migration_session_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.19250634Z level=info msg="Migration successfully executed" id="Rename table cloud_migration to cloud_migration_session_tmp_qwerty - v1" duration=13.994375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.193614381Z level=info msg="Executing migration" id="create cloud_migration_session v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.194054756Z level=info msg="Migration successfully executed" id="create cloud_migration_session v2" duration=440.625Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.195220965Z level=info msg="Executing migration" id="create index UQE_cloud_migration_session_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.195818715Z level=info msg="Migration successfully executed" id="create index UQE_cloud_migration_session_uid - v2" duration=597.666Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.196989965Z level=info msg="Executing migration" id="copy cloud_migration_session v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.197120798Z level=info msg="Migration successfully executed" id="copy cloud_migration_session v1 to v2" duration=130.792Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.198127798Z level=info msg="Executing migration" id="drop cloud_migration_session_tmp_qwerty"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.198503881Z level=info msg="Migration successfully executed" id="drop cloud_migration_session_tmp_qwerty" duration=377.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.199388173Z level=info msg="Executing migration" id="Rename table cloud_migration_run to cloud_migration_snapshot_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.213333756Z level=info msg="Migration successfully executed" id="Rename table cloud_migration_run to cloud_migration_snapshot_tmp_qwerty - v1" duration=13.945375ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.21459959Z level=info msg="Executing migration" id="create cloud_migration_snapshot v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.21501059Z level=info msg="Migration successfully executed" id="create cloud_migration_snapshot v2" duration=410.916Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.216144673Z level=info msg="Executing migration" id="create index UQE_cloud_migration_snapshot_uid - v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.216681215Z level=info msg="Migration successfully executed" id="create index UQE_cloud_migration_snapshot_uid - v2" duration=536.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.21770809Z level=info msg="Executing migration" id="copy cloud_migration_snapshot v1 to v2"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.217820631Z level=info msg="Migration successfully executed" id="copy cloud_migration_snapshot v1 to v2" duration=112.833Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.218714756Z level=info msg="Executing migration" id="drop cloud_migration_snapshot_tmp_qwerty"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.219093756Z level=info msg="Migration successfully executed" id="drop cloud_migration_snapshot_tmp_qwerty" duration=378.666Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.220289131Z level=info msg="Executing migration" id="add snapshot upload_url column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.225247131Z level=info msg="Migration successfully executed" id="add snapshot upload_url column" duration=4.957ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.226421965Z level=info msg="Executing migration" id="add snapshot status column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.231427381Z level=info msg="Migration successfully executed" id="add snapshot status column" duration=5.005ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.232627548Z level=info msg="Executing migration" id="add snapshot local_directory column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.237741631Z level=info msg="Migration successfully executed" id="add snapshot local_directory column" duration=5.113542ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.23908909Z level=info msg="Executing migration" id="add snapshot gms_snapshot_uid column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.244509256Z level=info msg="Migration successfully executed" id="add snapshot gms_snapshot_uid column" duration=5.419958ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.245667131Z level=info msg="Executing migration" id="add snapshot encryption_key column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.25115384Z level=info msg="Migration successfully executed" id="add snapshot encryption_key column" duration=5.485584ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.254310506Z level=info msg="Executing migration" id="add snapshot error_string column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.25944584Z level=info msg="Migration successfully executed" id="add snapshot error_string column" duration=5.137666ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.260877923Z level=info msg="Executing migration" id="create cloud_migration_resource table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.261398173Z level=info msg="Migration successfully executed" id="create cloud_migration_resource table v1" duration=520.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.262280548Z level=info msg="Executing migration" id="delete cloud_migration_snapshot.result column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.284063673Z level=info msg="Migration successfully executed" id="delete cloud_migration_snapshot.result column" duration=21.780167ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.285646965Z level=info msg="Executing migration" id="add cloud_migration_resource.name column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.290830173Z level=info msg="Migration successfully executed" id="add cloud_migration_resource.name column" duration=5.183167ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.291970965Z level=info msg="Executing migration" id="add cloud_migration_resource.parent_name column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.297112131Z level=info msg="Migration successfully executed" id="add cloud_migration_resource.parent_name column" duration=5.140792ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.298385506Z level=info msg="Executing migration" id="add cloud_migration_session.org_id column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.304567673Z level=info msg="Migration successfully executed" id="add cloud_migration_session.org_id column" duration=6.151708ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.305819048Z level=info msg="Executing migration" id="add cloud_migration_resource.error_code column"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.311548506Z level=info msg="Migration successfully executed" id="add cloud_migration_resource.error_code column" duration=5.726333ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.313227548Z level=info msg="Executing migration" id="increase resource_uid column length"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.313241798Z level=info msg="Migration successfully executed" id="increase resource_uid column length" duration=15.375Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.314575631Z level=info msg="Executing migration" id="create cloud_migration_snapshot_partition table v1"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.315183256Z level=info msg="Migration successfully executed" id="create cloud_migration_snapshot_partition table v1" duration=607.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.316327006Z level=info msg="Executing migration" id="add cloud_migration_snapshot_partition srp_unique index"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.317075006Z level=info msg="Migration successfully executed" id="add cloud_migration_snapshot_partition srp_unique index" duration=748.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.318170298Z level=info msg="Executing migration" id="add resource_storage_type column to cloud_migration_snapshot table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.323255423Z level=info msg="Migration successfully executed" id="add resource_storage_type column to cloud_migration_snapshot table" duration=5.084458ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.324494423Z level=info msg="Executing migration" id="add encryption_algo column to cloud_migration_snapshot table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.329623923Z level=info msg="Migration successfully executed" id="add encryption_algo column to cloud_migration_snapshot table" duration=5.128208ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.330905507Z level=info msg="Executing migration" id="add metadata column to cloud_migration_snapshot table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.336127923Z level=info msg="Migration successfully executed" id="add metadata column to cloud_migration_snapshot table" duration=5.220791ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.33733334Z level=info msg="Executing migration" id="add public_key column to cloud_migration_snapshot table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.34235884Z level=info msg="Migration successfully executed" id="add public_key column to cloud_migration_snapshot table" duration=5.028583ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.343384673Z level=info msg="Executing migration" id="alter kv_store.value to longtext"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.343393715Z level=info msg="Migration successfully executed" id="alter kv_store.value to longtext" duration=9.334Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.344509882Z level=info msg="Executing migration" id="add notification_settings column to alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.349845798Z level=info msg="Migration successfully executed" id="add notification_settings column to alert_rule table" duration=5.332583ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.351266923Z level=info msg="Executing migration" id="add notification_settings column to alert_rule_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.356549132Z level=info msg="Migration successfully executed" id="add notification_settings column to alert_rule_version table" duration=5.279917ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.358036548Z level=info msg="Executing migration" id="removing scope from alert.instances:read action migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.358368382Z level=info msg="Migration successfully executed" id="removing scope from alert.instances:read action migration" duration=332.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.36013784Z level=info msg="Executing migration" id="managed folder permissions alerting silences actions migration"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.360390715Z level=info msg="Migration successfully executed" id="managed folder permissions alerting silences actions migration" duration=254.083Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.361998882Z level=info msg="Executing migration" id="add record column to alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.368711257Z level=info msg="Migration successfully executed" id="add record column to alert_rule table" duration=6.7105ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.370042257Z level=info msg="Executing migration" id="add record column to alert_rule_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.375886882Z level=info msg="Migration successfully executed" id="add record column to alert_rule_version table" duration=5.824208ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.378125048Z level=info msg="Executing migration" id="add resolved_at column to alert_instance table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.386880007Z level=info msg="Migration successfully executed" id="add resolved_at column to alert_instance table" duration=8.750167ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.388988173Z level=info msg="Executing migration" id="add last_sent_at column to alert_instance table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.395646132Z level=info msg="Migration successfully executed" id="add last_sent_at column to alert_instance table" duration=6.639333ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.397259715Z level=info msg="Executing migration" id="Add scope to alert.notifications.receivers:read and alert.notifications.receivers.secrets:read"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.397593173Z level=info msg="Migration successfully executed" id="Add scope to alert.notifications.receivers:read and alert.notifications.receivers.secrets:read" duration=333.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.398608382Z level=info msg="Executing migration" id="add metadata column to alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.404070715Z level=info msg="Migration successfully executed" id="add metadata column to alert_rule table" duration=5.461709ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.405355382Z level=info msg="Executing migration" id="add metadata column to alert_rule_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.411064632Z level=info msg="Migration successfully executed" id="add metadata column to alert_rule_version table" duration=5.705666ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.412602423Z level=info msg="Executing migration" id="delete orphaned service account permissions"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.412805007Z level=info msg="Migration successfully executed" id="delete orphaned service account permissions" duration=203.208Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.413933965Z level=info msg="Executing migration" id="adding action set permissions"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.414230798Z level=info msg="Migration successfully executed" id="adding action set permissions" duration=296.916Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.415526173Z level=info msg="Executing migration" id="create user_external_session table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.41639209Z level=info msg="Migration successfully executed" id="create user_external_session table" duration=867.084Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.417758632Z level=info msg="Executing migration" id="increase name_id column length to 1024"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.417778173Z level=info msg="Migration successfully executed" id="increase name_id column length to 1024" duration=19.958Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.418769965Z level=info msg="Executing migration" id="increase session_id column length to 1024"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.418781715Z level=info msg="Migration successfully executed" id="increase session_id column length to 1024" duration=12.166Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.419604715Z level=info msg="Executing migration" id="remove scope from alert.notifications.receivers:create"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.419822548Z level=info msg="Migration successfully executed" id="remove scope from alert.notifications.receivers:create" duration=214.291Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.420777673Z level=info msg="Executing migration" id="add created_by column to alert_rule_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.426315673Z level=info msg="Migration successfully executed" id="add created_by column to alert_rule_version table" duration=5.536083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.427754132Z level=info msg="Executing migration" id="add updated_by column to alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.433048632Z level=info msg="Migration successfully executed" id="add updated_by column to alert_rule table" duration=5.294ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.434334715Z level=info msg="Executing migration" id="add alert_rule_state table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.434900007Z level=info msg="Migration successfully executed" id="add alert_rule_state table" duration=566Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.43596259Z level=info msg="Executing migration" id="add index to alert_rule_state on org_id and rule_uid columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.436604548Z level=info msg="Migration successfully executed" id="add index to alert_rule_state on org_id and rule_uid columns" duration=639.25Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.437671048Z level=info msg="Executing migration" id="add guid column to alert_rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.443161882Z level=info msg="Migration successfully executed" id="add guid column to alert_rule table" duration=5.46225ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.444666715Z level=info msg="Executing migration" id="add rule_guid column to alert_rule_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.450055798Z level=info msg="Migration successfully executed" id="add rule_guid column to alert_rule_version table" duration=5.388083ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.451318882Z level=info msg="Executing migration" id="cleanup alert_rule_version table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.451333757Z level=info msg="Rule version record limit is not set, fallback to 100" limit=0
grafana-1          | logger=migrator t=2026-02-25T11:40:03.451461382Z level=info msg="Cleaning up table `alert_rule_version`" batchSize=50 batches=0 keepVersions=100
grafana-1          | logger=migrator t=2026-02-25T11:40:03.451479798Z level=info msg="Migration successfully executed" id="cleanup alert_rule_version table" duration=161.417Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.452614007Z level=info msg="Executing migration" id="populate rule guid in alert rule table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.452945965Z level=info msg="Migration successfully executed" id="populate rule guid in alert rule table" duration=332.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.453935673Z level=info msg="Executing migration" id="drop index in alert_rule_version table on rule_org_id, rule_uid and version columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.454535382Z level=info msg="Migration successfully executed" id="drop index in alert_rule_version table on rule_org_id, rule_uid and version columns" duration=599.125Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.455451215Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_org_id, rule_uid, rule_guid and version columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.456076507Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_org_id, rule_uid, rule_guid and version columns" duration=625.333Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.457378298Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_guid and version columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.458036632Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_guid and version columns" duration=659.042Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.459114798Z level=info msg="Executing migration" id="add index in alert_rule table on guid columns"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.45984734Z level=info msg="Migration successfully executed" id="add index in alert_rule table on guid columns" duration=733.959Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.461022382Z level=info msg="Executing migration" id="add keep_firing_for column to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.466515007Z level=info msg="Migration successfully executed" id="add keep_firing_for column to alert_rule" duration=5.491208ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.468002215Z level=info msg="Executing migration" id="add keep_firing_for column to alert_rule_version"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.47346759Z level=info msg="Migration successfully executed" id="add keep_firing_for column to alert_rule_version" duration=5.463542ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.474968423Z level=info msg="Executing migration" id="add missing_series_evals_to_resolve column to alert_rule"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.48021009Z level=info msg="Migration successfully executed" id="add missing_series_evals_to_resolve column to alert_rule" duration=5.241417ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.48124009Z level=info msg="Executing migration" id="add missing_series_evals_to_resolve column to alert_rule_version"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.486384465Z level=info msg="Migration successfully executed" id="add missing_series_evals_to_resolve column to alert_rule_version" duration=5.143125ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.487667673Z level=info msg="Executing migration" id="remove the datasources:drilldown action"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.487797048Z level=info msg="Removed 0 datasources:drilldown permissions"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.487805048Z level=info msg="Migration successfully executed" id="remove the datasources:drilldown action" duration=137.5Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.489011215Z level=info msg="Executing migration" id="remove title in folder unique index"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.489668465Z level=info msg="Migration successfully executed" id="remove title in folder unique index" duration=657.917Âµs
grafana-1          | logger=migrator t=2026-02-25T11:40:03.49075809Z level=info msg="Executing migration" id="add fired_at column to alert_instance table"
grafana-1          | logger=migrator t=2026-02-25T11:40:03.49584884Z level=info msg="Migration successfully executed" id="add fired_at column to alert_instance table" duration=5.090625ms
grafana-1          | logger=migrator t=2026-02-25T11:40:03.496846423Z level=info msg="migrations completed" performed=674 skipped=0 duration=2.033365793s
grafana-1          | logger=migrator t=2026-02-25T11:40:03.497226507Z level=info msg="Unlocking database"
grafana-1          | logger=sqlstore t=2026-02-25T11:40:03.506187048Z level=info msg="Created default admin" user=admin
grafana-1          | logger=sqlstore t=2026-02-25T11:40:03.506329382Z level=info msg="Created default organization"
grafana-1          | logger=secrets t=2026-02-25T11:40:03.508199423Z level=info msg="Envelope encryption state" enabled=true currentprovider=secretKey.v1
grafana-1          | logger=plugin.angulardetectorsprovider.dynamic t=2026-02-25T11:40:03.553497007Z level=info msg="Restored cache from database" duration=419.125Âµs
grafana-1          | logger=resource-db t=2026-02-25T11:40:03.55486309Z level=info msg="Using database section" db_type=sqlite3
grafana-1          | logger=resource-db t=2026-02-25T11:40:03.554936715Z level=info msg="Initializing Resource DB" db_type=sqlite3 open_conn=0 in_use_conn=0 idle_conn=0 max_open_conn=0
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.560164882Z level=info msg="Locking database"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.560186507Z level=info msg="Starting DB migrations"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.565954923Z level=info msg="Executing migration" id="create resource_migration_log table"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.566582757Z level=info msg="Migration successfully executed" id="create resource_migration_log table" duration=628Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.568443257Z level=info msg="Executing migration" id="Initialize resource tables"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.568460132Z level=info msg="Migration successfully executed" id="Initialize resource tables" duration=19.417Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.56979409Z level=info msg="Executing migration" id="drop table resource"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.569889007Z level=info msg="Migration successfully executed" id="drop table resource" duration=95.75Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.571116965Z level=info msg="Executing migration" id="create table resource"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.57198384Z level=info msg="Migration successfully executed" id="create table resource" duration=866.75Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.573464465Z level=info msg="Executing migration" id="create table resource, index: 0"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.574364715Z level=info msg="Migration successfully executed" id="create table resource, index: 0" duration=900.334Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.575940798Z level=info msg="Executing migration" id="drop table resource_history"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.576018382Z level=info msg="Migration successfully executed" id="drop table resource_history" duration=77.792Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.577323132Z level=info msg="Executing migration" id="create table resource_history"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.578094965Z level=info msg="Migration successfully executed" id="create table resource_history" duration=772.25Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.57926884Z level=info msg="Executing migration" id="create table resource_history, index: 0"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.57992034Z level=info msg="Migration successfully executed" id="create table resource_history, index: 0" duration=650.958Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.581095965Z level=info msg="Executing migration" id="create table resource_history, index: 1"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.581726923Z level=info msg="Migration successfully executed" id="create table resource_history, index: 1" duration=627.542Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.582928048Z level=info msg="Executing migration" id="drop table resource_version"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.58298734Z level=info msg="Migration successfully executed" id="drop table resource_version" duration=59.583Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.584157298Z level=info msg="Executing migration" id="create table resource_version"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.584644507Z level=info msg="Migration successfully executed" id="create table resource_version" duration=487Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.585916465Z level=info msg="Executing migration" id="create table resource_version, index: 0"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.586644757Z level=info msg="Migration successfully executed" id="create table resource_version, index: 0" duration=728Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.587781298Z level=info msg="Executing migration" id="drop table resource_blob"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.587846257Z level=info msg="Migration successfully executed" id="drop table resource_blob" duration=64.667Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.588967882Z level=info msg="Executing migration" id="create table resource_blob"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.589607757Z level=info msg="Migration successfully executed" id="create table resource_blob" duration=639.584Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.590745632Z level=info msg="Executing migration" id="create table resource_blob, index: 0"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.591344548Z level=info msg="Migration successfully executed" id="create table resource_blob, index: 0" duration=598.875Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.592597298Z level=info msg="Executing migration" id="create table resource_blob, index: 1"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.593172423Z level=info msg="Migration successfully executed" id="create table resource_blob, index: 1" duration=574.958Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.594654548Z level=info msg="Executing migration" id="Add column previous_resource_version in resource_history"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.60036534Z level=info msg="Migration successfully executed" id="Add column previous_resource_version in resource_history" duration=5.709709ms
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.601685173Z level=info msg="Executing migration" id="Add column previous_resource_version in resource"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.60727334Z level=info msg="Migration successfully executed" id="Add column previous_resource_version in resource" duration=5.581667ms
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.608697298Z level=info msg="Executing migration" id="Add index to resource_history for polling"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.609431215Z level=info msg="Migration successfully executed" id="Add index to resource_history for polling" duration=733.958Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.610598632Z level=info msg="Executing migration" id="Add index to resource for loading"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.611190548Z level=info msg="Migration successfully executed" id="Add index to resource for loading" duration=591.75Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.612449382Z level=info msg="Executing migration" id="Add column folder in resource_history"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.618130007Z level=info msg="Migration successfully executed" id="Add column folder in resource_history" duration=5.679833ms
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.619276048Z level=info msg="Executing migration" id="Add column folder in resource"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.624941757Z level=info msg="Migration successfully executed" id="Add column folder in resource" duration=5.6665ms
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.626176798Z level=info msg="Executing migration" id="Migrate DeletionMarkers to real Resource objects"
grafana-1          | logger=deletion-marker-migrator t=2026-02-25T11:40:03.626196715Z level=info msg="finding any deletion markers"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.626411882Z level=info msg="Migration successfully executed" id="Migrate DeletionMarkers to real Resource objects" duration=234.25Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.627606923Z level=info msg="Executing migration" id="Add index to resource_history for get trash"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.628267215Z level=info msg="Migration successfully executed" id="Add index to resource_history for get trash" duration=660.083Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.629358632Z level=info msg="Executing migration" id="Add generation to resource history"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.634946423Z level=info msg="Migration successfully executed" id="Add generation to resource history" duration=5.587709ms
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.636208632Z level=info msg="Executing migration" id="Add generation index to resource history"
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.636876965Z level=info msg="Migration successfully executed" id="Add generation index to resource history" duration=668.791Âµs
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.637956007Z level=info msg="migrations completed" performed=26 skipped=0 duration=72.041375ms
grafana-1          | logger=resource-migrator t=2026-02-25T11:40:03.638269798Z level=info msg="Unlocking database"
grafana-1          | t=2026-02-25T11:40:03.638447632Z level=info caller=logger.go:214 time=2026-02-25T11:40:03.638428923Z msg="Using channel notifier" logger=sql-resource-server
grafana-1          | logger=plugin.store t=2026-02-25T11:40:03.64475584Z level=info msg="Loading plugins..."
grafana-1          | logger=plugin.store t=2026-02-25T11:40:03.673843132Z level=info msg="Plugins loaded" count=52 duration=29.088583ms
grafana-1          | logger=query_data t=2026-02-25T11:40:03.677525548Z level=info msg="Query Service initialization"
grafana-1          | logger=live.push_http t=2026-02-25T11:40:03.679933048Z level=info msg="Live Push Gateway initialization"
grafana-1          | logger=ngalert.notifier component=alertmanager orgID=1 t=2026-02-25T11:40:03.688077715Z level=info msg="Applying new configuration to Alertmanager" configHash=d2c56faca6af2a5772ff4253222f7386
grafana-1          | logger=ngalert.writer t=2026-02-25T11:40:03.691626507Z level=info msg="Setting up remote write using data sources" timeout=30s default_datasource_uid=
grafana-1          | logger=ngalert t=2026-02-25T11:40:03.691684465Z level=info msg="Using protobuf-based alert instance store"
grafana-1          | logger=ngalert.state.manager.persist t=2026-02-25T11:40:03.691695673Z level=info msg="Using rule state persister"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.69336559Z level=info msg="Locking database"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.69337909Z level=info msg="Starting DB migrations"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.693883423Z level=info msg="Executing migration" id="create secret_migration_log table"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.694524965Z level=info msg="Migration successfully executed" id="create secret_migration_log table" duration=641.25Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.696175632Z level=info msg="Executing migration" id="Initialize secrets tables"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.696186757Z level=info msg="Migration successfully executed" id="Initialize secrets tables" duration=11.709Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.697315965Z level=info msg="Executing migration" id="drop table secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.697353132Z level=info msg="Migration successfully executed" id="drop table secret_secure_value" duration=37.333Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.698309548Z level=info msg="Executing migration" id="create table secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.699054298Z level=info msg="Migration successfully executed" id="create table secret_secure_value" duration=744.875Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.700293507Z level=info msg="Executing migration" id="create table secret_secure_value, index: 0"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.700963798Z level=info msg="Migration successfully executed" id="create table secret_secure_value, index: 0" duration=670.5Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.702089798Z level=info msg="Executing migration" id="create table secret_secure_value, index: 1"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.702758632Z level=info msg="Migration successfully executed" id="create table secret_secure_value, index: 1" duration=669.083Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.703851632Z level=info msg="Executing migration" id="drop table secret_keeper"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.703889923Z level=info msg="Migration successfully executed" id="drop table secret_keeper" duration=38.875Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.704871757Z level=info msg="Executing migration" id="create table secret_keeper"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.705394548Z level=info msg="Migration successfully executed" id="create table secret_keeper" duration=522.833Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.706377548Z level=info msg="Executing migration" id="create table secret_keeper, index: 0"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.706972673Z level=info msg="Migration successfully executed" id="create table secret_keeper, index: 0" duration=594.292Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.707990215Z level=info msg="Executing migration" id="drop table secret_data_key"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.708022298Z level=info msg="Migration successfully executed" id="drop table secret_data_key" duration=33.833Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.708947132Z level=info msg="Executing migration" id="create table secret_data_key"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.709475257Z level=info msg="Migration successfully executed" id="create table secret_data_key" duration=527.792Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.710620465Z level=info msg="Executing migration" id="drop table secret_encrypted_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.710653632Z level=info msg="Migration successfully executed" id="drop table secret_encrypted_value" duration=33.292Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.711676715Z level=info msg="Executing migration" id="create table secret_encrypted_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.712067715Z level=info msg="Migration successfully executed" id="create table secret_encrypted_value" duration=391Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.713163465Z level=info msg="Executing migration" id="create table secret_encrypted_value, index: 0"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.713790715Z level=info msg="Migration successfully executed" id="create table secret_encrypted_value, index: 0" duration=627.708Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.714916257Z level=info msg="Executing migration" id="create index for list on secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.715490465Z level=info msg="Migration successfully executed" id="create index for list on secret_secure_value" duration=574.291Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.716636382Z level=info msg="Executing migration" id="create index for list and read current on secret_data_key"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.717193798Z level=info msg="Migration successfully executed" id="create index for list and read current on secret_data_key" duration=557.583Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.71822384Z level=info msg="Executing migration" id="add owner_reference_api_group column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.724029798Z level=info msg="Migration successfully executed" id="add owner_reference_api_group column to secret_secure_value" duration=5.805958ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.724934548Z level=info msg="Executing migration" id="add owner_reference_api_version column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.73066184Z level=info msg="Migration successfully executed" id="add owner_reference_api_version column to secret_secure_value" duration=5.727292ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.731603923Z level=info msg="Executing migration" id="add owner_reference_kind column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.737474257Z level=info msg="Migration successfully executed" id="add owner_reference_kind column to secret_secure_value" duration=5.870417ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.738523632Z level=info msg="Executing migration" id="add owner_reference_name column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.744213298Z level=info msg="Migration successfully executed" id="add owner_reference_name column to secret_secure_value" duration=5.689292ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.745279632Z level=info msg="Executing migration" id="add lease_token column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.751491882Z level=info msg="Migration successfully executed" id="add lease_token column to secret_secure_value" duration=6.208833ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.752822423Z level=info msg="Executing migration" id="add lease_token index to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.753606798Z level=info msg="Migration successfully executed" id="add lease_token index to secret_secure_value" duration=784.833Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.754743215Z level=info msg="Executing migration" id="add lease_created column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.76080084Z level=info msg="Migration successfully executed" id="add lease_created column to secret_secure_value" duration=6.055166ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.762742507Z level=info msg="Executing migration" id="add lease_created index to secret_secure_value"
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.764253798Z level=info msg="Migration successfully executed" id="add lease_created index to secret_secure_value" duration=1.511125ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.767115965Z level=info msg="migrations completed" performed=24 skipped=0 duration=73.253458ms
grafana-1          | logger=secret-migrator t=2026-02-25T11:40:03.767914757Z level=info msg="Unlocking database"
grafana-1          | logger=infra.usagestats.collector t=2026-02-25T11:40:03.768431798Z level=info msg="registering usage stat providers" usageStatsProvidersLen=2
grafana-1          | logger=grafanaStorageLogger t=2026-02-25T11:40:03.768756173Z level=info msg="Storage starting"
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:03.768863048Z level=info msg="Installing plugin" pluginId=grafana-lokiexplore-app version=
grafana-1          | logger=ngalert.state.manager t=2026-02-25T11:40:03.769491715Z level=info msg="Warming state cache for startup"
grafana-1          | logger=ngalert.multiorg.alertmanager t=2026-02-25T11:40:03.769721298Z level=info msg="Starting MultiOrg Alertmanager"
grafana-1          | logger=http.server t=2026-02-25T11:40:03.785298507Z level=info msg="HTTP Server Listen" address=[::]:3000 protocol=http subUrl= socket=
grafana-1          | logger=provisioning.alerting t=2026-02-25T11:40:03.840610882Z level=info msg="starting to provision alerting"
grafana-1          | logger=provisioning.alerting t=2026-02-25T11:40:03.840640923Z level=info msg="finished to provision alerting"
grafana-1          | logger=provisioning.dashboard t=2026-02-25T11:40:03.847115673Z level=info msg="starting to provision dashboards"
grafana-1          | logger=provisioning.dashboard t=2026-02-25T11:40:03.847133215Z level=info msg="finished to provision dashboards"
grafana-1          | logger=ngalert.state.manager t=2026-02-25T11:40:03.850987048Z level=info msg="State cache has been initialized" states=0 duration=81.495ms
grafana-1          | logger=ngalert.scheduler t=2026-02-25T11:40:03.85103809Z level=info msg="Starting scheduler" tickInterval=10s maxAttempts=3
grafana-1          | logger=ngalert.scheduler t=2026-02-25T11:40:03.85108684Z level=info msg=starting component=ticker first_tick=2026-02-25T11:40:10Z
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.918201798Z level=info msg="Adding GroupVersion folder.grafana.app v1beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.919394673Z level=info msg="Adding GroupVersion iam.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.91973184Z level=info msg="Adding GroupVersion userstorage.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.919854882Z level=info msg="Adding GroupVersion features.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.920981465Z level=info msg="Adding GroupVersion notifications.alerting.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.924896882Z level=info msg="Adding GroupVersion dashboard.grafana.app v1beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.925116048Z level=info msg="Adding GroupVersion dashboard.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.925358757Z level=info msg="Adding GroupVersion dashboard.grafana.app v2beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.925573298Z level=info msg="Adding GroupVersion dashboard.grafana.app v2alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.926121215Z level=info msg="Adding GroupVersion playlist.grafana.app v0alpha1 to ResourceManager"
grafana-1          | t=2026-02-25T11:40:03.926160007Z level=info caller=logger.go:214 time=2026-02-25T11:40:03.926157215Z msg="Installed APIs for app" app=playlist
grafana-1          | logger=grafana-apiserver t=2026-02-25T11:40:03.926232798Z level=info msg="Skipping API plugins.grafana.app/v0alpha1 because it has no resources."
grafana-1          | t=2026-02-25T11:40:03.926263715Z level=info caller=logger.go:214 time=2026-02-25T11:40:03.926261715Z msg="Installed APIs for app" app=plugins
grafana-1          | logger=plugins.update.checker t=2026-02-25T11:40:03.95459959Z level=info msg="Update check succeeded" duration=185.224666ms
grafana-1          | logger=grafana.update.checker t=2026-02-25T11:40:03.954812382Z level=info msg="Update check succeeded" duration=185.185334ms
grafana-1          | logger=app-registry t=2026-02-25T11:40:03.961011507Z level=info msg="app registry initialized"
grafana-1          | t=2026-02-25T11:40:03.961166798Z level=info caller=logger.go:214 time=2026-02-25T11:40:03.961163548Z msg="App initialized" app=plugins
grafana-1          | t=2026-02-25T11:40:03.961168257Z level=info caller=logger.go:214 time=2026-02-25T11:40:03.961164923Z msg="App initialized" app=playlist
grafana-1          | logger=plugin.angulardetectorsprovider.dynamic t=2026-02-25T11:40:04.01759559Z level=info msg="Patterns update finished" duration=170.454166ms
grafana-1          | logger=plugin.installer t=2026-02-25T11:40:05.196238757Z level=info msg="Installing plugin" pluginId=grafana-lokiexplore-app version=
grafana-1          | logger=installer.fs t=2026-02-25T11:40:05.388331132Z level=info msg="Downloaded and extracted grafana-lokiexplore-app v1.0.37 zip successfully to /var/lib/grafana/plugins/grafana-lokiexplore-app"
grafana-1          | logger=plugins.registration t=2026-02-25T11:40:05.419294258Z level=info msg="Plugin registered" pluginId=grafana-lokiexplore-app
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:05.419332841Z level=info msg="Plugin successfully installed" pluginId=grafana-lokiexplore-app version= duration=1.650446293s
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:05.419356799Z level=info msg="Installing plugin" pluginId=grafana-pyroscope-app version=
grafana-1          | logger=plugin.installer t=2026-02-25T11:40:05.872700549Z level=info msg="Installing plugin" pluginId=grafana-pyroscope-app version=
grafana-1          | logger=installer.fs t=2026-02-25T11:40:05.941586424Z level=info msg="Downloaded and extracted grafana-pyroscope-app v1.17.0 zip successfully to /var/lib/grafana/plugins/grafana-pyroscope-app"
grafana-1          | logger=plugins.registration t=2026-02-25T11:40:05.956769966Z level=info msg="Plugin registered" pluginId=grafana-pyroscope-app
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:05.956812758Z level=info msg="Plugin successfully installed" pluginId=grafana-pyroscope-app version= duration=537.452417ms
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:05.956846091Z level=info msg="Installing plugin" pluginId=grafana-exploretraces-app version=
grafana-1          | logger=plugin.installer t=2026-02-25T11:40:06.4241443Z level=info msg="Installing plugin" pluginId=grafana-exploretraces-app version=
grafana-1          | logger=installer.fs t=2026-02-25T11:40:06.49921155Z level=info msg="Downloaded and extracted grafana-exploretraces-app v1.3.2 zip successfully to /var/lib/grafana/plugins/grafana-exploretraces-app"
grafana-1          | logger=plugins.registration t=2026-02-25T11:40:06.514927966Z level=info msg="Plugin registered" pluginId=grafana-exploretraces-app
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:06.514974425Z level=info msg="Plugin successfully installed" pluginId=grafana-exploretraces-app version= duration=558.122625ms
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:06.515004758Z level=info msg="Installing plugin" pluginId=grafana-metricsdrilldown-app version=
grafana-1          | logger=plugin.installer t=2026-02-25T11:40:07.059792758Z level=info msg="Installing plugin" pluginId=grafana-metricsdrilldown-app version=
grafana-1          | logger=installer.fs t=2026-02-25T11:40:07.171639633Z level=info msg="Downloaded and extracted grafana-metricsdrilldown-app v1.0.31 zip successfully to /var/lib/grafana/plugins/grafana-metricsdrilldown-app"
grafana-1          | logger=plugins.registration t=2026-02-25T11:40:07.19736405Z level=info msg="Plugin registered" pluginId=grafana-metricsdrilldown-app
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T11:40:07.197420092Z level=info msg="Plugin successfully installed" pluginId=grafana-metricsdrilldown-app version= duration=682.402792ms
grafana-1          | logger=infra.usagestats t=2026-02-25T11:41:37.781459384Z level=info msg="Usage stats are ready to report"
grafana-1          | logger=cleanup t=2026-02-25T11:50:03.840051965Z level=info msg="Completed cleanup jobs" duration=63.76925ms
grafana-1          | logger=sqlstore.transactions t=2026-02-25T11:50:03.847487382Z level=info msg="Database locked, sleeping then retrying" error="database is locked" retry=0
grafana-1          | logger=plugins.update.checker t=2026-02-25T11:50:04.186782007Z level=info msg="Update check succeeded" duration=229.162ms
grafana-1          | logger=sqlstore.transactions t=2026-02-25T11:55:03.70329134Z level=info msg="Database locked, sleeping then retrying" error="database is locked" retry=0
grafana-1          | logger=cleanup t=2026-02-25T12:00:03.838505465Z level=info msg="Completed cleanup jobs" duration=64.070666ms
grafana-1          | logger=plugins.update.checker t=2026-02-25T12:00:04.126001757Z level=info msg="Update check succeeded" duration=166.841291ms
grafana-1          | logger=cleanup t=2026-02-25T12:10:03.831849257Z level=info msg="Completed cleanup jobs" duration=58.29775ms
grafana-1          | logger=plugins.update.checker t=2026-02-25T12:10:04.190492965Z level=info msg="Update check succeeded" duration=230.216708ms
grafana-1          | logger=infra.usagestats t=2026-02-25T12:11:37.796225758Z level=info msg="Usage stats are ready to report"
