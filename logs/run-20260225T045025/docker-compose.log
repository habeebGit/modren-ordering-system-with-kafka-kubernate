time="2026-02-25T04:50:25-06:00" level=warning msg="/Users/habeebmohammed/software/modren-ordering-system-with-kafka-kubernate/modren-ordering-system-with-kafka/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion"
products_db-1  | 
products_db-1  | PostgreSQL Database directory appears to contain a database; Skipping initialization
products_db-1  | 
products_db-1  | 2026-02-25 10:39:25.974 UTC [1] LOG:  starting PostgreSQL 14.21 (Debian 14.21-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
products_db-1  | 2026-02-25 10:39:25.974 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
products_db-1  | 2026-02-25 10:39:25.974 UTC [1] LOG:  listening on IPv6 address "::", port 5432
products_db-1  | 2026-02-25 10:39:25.978 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
products_db-1  | 2026-02-25 10:39:25.984 UTC [26] LOG:  database system shutdown was interrupted; last known up at 2025-11-07 16:43:37 UTC
products_db-1  | 2026-02-25 10:39:26.485 UTC [26] LOG:  database system was not properly shut down; automatic recovery in progress
products_db-1  | 2026-02-25 10:39:26.486 UTC [26] LOG:  redo starts at 0/18332F8
products_db-1  | 2026-02-25 10:39:26.486 UTC [26] LOG:  invalid record length at 0/18333E0: wanted 24, got 0
products_db-1  | 2026-02-25 10:39:26.486 UTC [26] LOG:  redo done at 0/18333A8 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
products_db-1  | 2026-02-25 10:39:26.495 UTC [1] LOG:  database system is ready to accept connections
zookeeper-1    | ===> User
zookeeper-1    | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
zookeeper-1    | ===> Configuring ...
zookeeper-1    | ===> Running preflight checks ... 
zookeeper-1    | ===> Check if /var/lib/zookeeper/data is writable ...
zookeeper-1    | ===> Check if /var/lib/zookeeper/log is writable ...
zookeeper-1    | ===> Launching ... 
zookeeper-1    | ===> Launching zookeeper ... 
zookeeper-1    | [2025-11-07 15:48:32,671] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,106] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,106] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,107] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,107] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,131] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1    | [2025-11-07 15:48:34,131] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1    | [2025-11-07 15:48:34,131] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1    | [2025-11-07 15:48:34,131] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
zookeeper-1    | [2025-11-07 15:48:34,145] INFO Log4j 1.2 jmx support found and enabled. (org.apache.zookeeper.jmx.ManagedUtil)
zookeeper-1    | [2025-11-07 15:48:34,417] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,419] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,419] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,419] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,419] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1    | [2025-11-07 15:48:34,419] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
zookeeper-1    | [2025-11-07 15:48:34,553] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@52102734 (org.apache.zookeeper.server.ServerMetrics)
zookeeper-1    | [2025-11-07 15:48:34,566] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,581] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,584] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,588] INFO Server environment:host.name=db3d5cbacf85 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,588] INFO Server environment:java.version=11.0.13 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-json-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-raft-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/netty-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/connect-transforms-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-shell-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-clients-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/trogdor-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.68.Final.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.68.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.19.3.jar:/usr/bin/../share/java/kafka/kafka-streams-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.68.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/connect-mirror-7.0.1-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:os.memory.free=492MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1    | [2025-11-07 15:48:34,589] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
grafana-1      | logger=settings t=2025-11-07T15:47:38.861115544Z level=info msg="Starting Grafana" version=12.2.1 commit=563109b696e9c1cbaf345f2ab7a11f7f78422982 branch=release-12.2.1 compiled=2025-11-07T15:47:38Z
my-ordering-app-1  | [1G[0K\[1G[0K
orders_db-1        | 
orders_db-1        | PostgreSQL Database directory appears to contain a database; Skipping initialization
orders_db-1        | 
grafana-1          | logger=settings t=2025-11-07T15:47:38.861802544Z level=info msg="Config loaded from" file=/usr/share/grafana/conf/defaults.ini
grafana-1          | logger=settings t=2025-11-07T15:47:38.861817461Z level=info msg="Config loaded from" file=/etc/grafana/grafana.ini
grafana-1          | logger=settings t=2025-11-07T15:47:38.861819794Z level=info msg="Config overridden from command line" arg="default.paths.data=/var/lib/grafana"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861821586Z level=info msg="Config overridden from command line" arg="default.paths.logs=/var/log/grafana"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861823294Z level=info msg="Config overridden from command line" arg="default.paths.plugins=/var/lib/grafana/plugins"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861825211Z level=info msg="Config overridden from command line" arg="default.paths.provisioning=/etc/grafana/provisioning"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861826752Z level=info msg="Config overridden from command line" arg="default.log.mode=console"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861828294Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_DATA=/var/lib/grafana"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861829961Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_LOGS=/var/log/grafana"
my-ordering-app-1  | > my-ordering-app@0.1.0 start
grafana-1          | logger=settings t=2025-11-07T15:47:38.861831419Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PLUGINS=/var/lib/grafana/plugins"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861832794Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PROVISIONING=/etc/grafana/provisioning"
grafana-1          | logger=settings t=2025-11-07T15:47:38.861834502Z level=info msg=Target target=[all]
my-ordering-app-1  | > react-scripts start
my-ordering-app-1  | 
my-ordering-app-1  | [1G[0K\[1G[0K[baseline-browser-mapping] The data in this module is over two months old.  To ensure accurate Baseline data, please update: `npm i baseline-browser-mapping@latest -D`
my-ordering-app-1  | (node:25) [DEP_WEBPACK_DEV_SERVER_ON_AFTER_SETUP_MIDDLEWARE] DeprecationWarning: 'onAfterSetupMiddleware' option is deprecated. Please use the 'setupMiddlewares' option.
my-ordering-app-1  | (Use `node --trace-deprecation ...` to show where the warning was created)
my-ordering-app-1  | (node:25) [DEP_WEBPACK_DEV_SERVER_ON_BEFORE_SETUP_MIDDLEWARE] DeprecationWarning: 'onBeforeSetupMiddleware' option is deprecated. Please use the 'setupMiddlewares' option.
my-ordering-app-1  | [2J[3J[H[36mStarting the development server...[39m
my-ordering-app-1  | [36m[39m
orders_db-1        | 2026-02-25 10:39:25.997 UTC [1] LOG:  starting PostgreSQL 14.21 (Debian 14.21-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit
orders_db-1        | 2026-02-25 10:39:25.997 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
orders_db-1        | 2026-02-25 10:39:25.997 UTC [1] LOG:  listening on IPv6 address "::", port 5432
orders_db-1        | 2026-02-25 10:39:26.001 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
orders_db-1        | 2026-02-25 10:39:26.010 UTC [25] LOG:  database system shutdown was interrupted; last known up at 2025-11-07 16:43:37 UTC
orders_db-1        | 2026-02-25 10:39:26.443 UTC [25] LOG:  database system was not properly shut down; automatic recovery in progress
my-ordering-app-1  | [2J[3J[H[32mCompiled successfully![39m
my-ordering-app-1  | 
orders_db-1        | 2026-02-25 10:39:26.444 UTC [25] LOG:  redo starts at 0/17BFFE8
orders_db-1        | 2026-02-25 10:39:26.444 UTC [25] LOG:  invalid record length at 0/17C00E8: wanted 24, got 0
my-ordering-app-1  | You can now view [1mmy-ordering-app[22m in the browser.
my-ordering-app-1  | 
my-ordering-app-1  |   [1mLocal:[22m            http://localhost:[1m3000[22m
my-ordering-app-1  |   [1mOn Your Network:[22m  http://172.18.0.10:[1m3000[22m
my-ordering-app-1  | 
my-ordering-app-1  | Note that the development build is not optimized.
my-ordering-app-1  | To create a production build, use [36mnpm run build[39m.
my-ordering-app-1  | 
my-ordering-app-1  | webpack compiled [1m[32msuccessfully[39m[22m
orders_db-1        | 2026-02-25 10:39:26.444 UTC [25] LOG:  redo done at 0/17C00B0 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
orders_db-1        | 2026-02-25 10:39:26.458 UTC [1] LOG:  database system is ready to accept connections
my-ordering-app-1  | [2J[3J[HCompiling...
my-ordering-app-1  | [2J[3J[H[32mCompiled successfully![39m
my-ordering-app-1  | 
my-ordering-app-1  | You can now view [1mmy-ordering-app[22m in the browser.
my-ordering-app-1  | 
my-ordering-app-1  |   [1mLocal:[22m            http://localhost:[1m3000[22m
my-ordering-app-1  |   [1mOn Your Network:[22m  http://172.18.0.10:[1m3000[22m
my-ordering-app-1  | 
my-ordering-app-1  | Note that the development build is not optimized.
my-ordering-app-1  | To create a production build, use [36mnpm run build[39m.
my-ordering-app-1  | 
my-ordering-app-1  | webpack compiled [1m[32msuccessfully[39m[22m
prometheus-1       | time=2025-11-07T15:47:35.638Z level=INFO source=main.go:1549 msg="updated GOGC" old=100 new=75
prometheus-1       | time=2025-11-07T15:47:35.670Z level=INFO source=main.go:680 msg="Leaving GOMAXPROCS=12: CPU quota undefined" component=automaxprocs
prometheus-1       | time=2025-11-07T15:47:35.671Z level=INFO source=memlimit.go:198 msg="GOMEMLIMIT is updated" component=automemlimit package=github.com/KimMachineGun/automemlimit/memlimit GOMEMLIMIT=7395976396 previous=9223372036854775807
prometheus-1       | time=2025-11-07T15:47:35.671Z level=INFO source=main.go:722 msg="No time or size retention was set so using the default time retention" duration=15d
prometheus-1       | time=2025-11-07T15:47:35.671Z level=INFO source=main.go:773 msg="Starting Prometheus Server" mode=server version="(version=3.7.1, branch=HEAD, revision=0aeb4fddc93b64e4e95104d5e8ea8b55ad36fb61)"
prometheus-1       | time=2025-11-07T15:47:35.671Z level=INFO source=main.go:778 msg="operational information" build_context="(go=go1.25.3, platform=linux/arm64, user=root@54bf11233185, date=20251017-06:35:17, tags=netgo,builtinassets)" host_details="(Linux 6.10.14-linuxkit #1 SMP Sat May 17 08:28:57 UTC 2025 aarch64 826fd5223daa (none))" fd_limits="(soft=1048576, hard=1048576)" vm_limits="(soft=unlimited, hard=unlimited)"
prometheus-1       | time=2025-11-07T15:47:35.695Z level=INFO source=web.go:660 msg="Start listening for connections" component=web address=0.0.0.0:9090
prometheus-1       | time=2025-11-07T15:47:35.758Z level=INFO source=main.go:1293 msg="Starting TSDB ..."
prometheus-1       | time=2025-11-07T15:47:35.764Z level=INFO source=tls_config.go:346 msg="Listening on" component=web address=[::]:9090
prometheus-1       | time=2025-11-07T15:47:35.764Z level=INFO source=tls_config.go:349 msg="TLS is disabled." component=web http2=false address=[::]:9090
prometheus-1       | time=2025-11-07T15:47:35.790Z level=INFO source=head.go:669 msg="Replaying on-disk memory mappable chunks if any" component=tsdb
prometheus-1       | time=2025-11-07T15:47:35.791Z level=INFO source=head.go:755 msg="On-disk memory mappable chunks replay completed" component=tsdb duration=1.167Âµs
prometheus-1       | time=2025-11-07T15:47:35.791Z level=INFO source=head.go:763 msg="Replaying WAL, this may take a while" component=tsdb
prometheus-1       | time=2025-11-07T15:47:35.795Z level=INFO source=head.go:836 msg="WAL segment loaded" component=tsdb segment=0 maxSegment=0 duration=3.764209ms
prometheus-1       | time=2025-11-07T15:47:35.795Z level=INFO source=head.go:873 msg="WAL replay completed" component=tsdb checkpoint_replay_duration=30.125Âµs wal_replay_duration=3.778042ms wbl_replay_duration=42ns chunk_snapshot_load_duration=0s mmap_chunk_replay_duration=1.167Âµs total_replay_duration=3.824542ms
prometheus-1       | time=2025-11-07T15:47:35.801Z level=INFO source=main.go:1314 msg="filesystem information" fs_type=EXT4_SUPER_MAGIC
prometheus-1       | time=2025-11-07T15:47:35.801Z level=INFO source=main.go:1317 msg="TSDB started"
prometheus-1       | time=2025-11-07T15:47:35.801Z level=INFO source=main.go:1502 msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
prometheus-1       | time=2025-11-07T15:47:35.824Z level=INFO source=main.go:1542 msg="Completed loading of configuration file" db_storage=917ns remote_storage=708ns web_handler=416ns query_engine=5.584Âµs scrape=417.292Âµs scrape_sd=28.5Âµs notify=584ns notify_sd=458ns rules=875ns tracing=3.167Âµs filename=/etc/prometheus/prometheus.yml totalDuration=23.690792ms
prometheus-1       | time=2025-11-07T15:47:35.825Z level=INFO source=main.go:1278 msg="Server is ready to receive web requests."
prometheus-1       | time=2025-11-07T15:47:35.825Z level=INFO source=manager.go:190 msg="Starting rule manager..." component="rule manager"
prometheus-1       | time=2025-11-07T16:43:37.190Z level=WARN source=main.go:1080 msg="Received an OS signal, exiting gracefully..." signal=terminated
prometheus-1       | time=2025-11-07T16:43:37.236Z level=INFO source=main.go:1105 msg="Stopping scrape discovery manager..."
prometheus-1       | time=2025-11-07T16:43:37.240Z level=INFO source=main.go:1119 msg="Stopping notify discovery manager..."
prometheus-1       | time=2025-11-07T16:43:37.264Z level=INFO source=main.go:1115 msg="Notify discovery manager stopped"
prometheus-1       | time=2025-11-07T16:43:37.271Z level=INFO source=manager.go:204 msg="Stopping rule manager..." component="rule manager"
prometheus-1       | time=2025-11-07T16:43:37.271Z level=INFO source=manager.go:220 msg="Rule manager stopped" component="rule manager"
prometheus-1       | time=2025-11-07T16:43:37.240Z level=INFO source=main.go:1101 msg="Scrape discovery manager stopped"
prometheus-1       | time=2025-11-07T16:43:37.294Z level=INFO source=main.go:1156 msg="Stopping scrape manager..."
prometheus-1       | time=2025-11-07T16:43:37.329Z level=INFO source=main.go:1148 msg="Scrape manager stopped"
prometheus-1       | time=2025-11-07T16:43:38.172Z level=INFO source=manager.go:559 msg="Stopping notification manager..." component=notifier
prometheus-1       | time=2025-11-07T16:43:38.207Z level=INFO source=manager.go:301 msg="Draining any remaining notifications..." component=notifier
prometheus-1       | time=2025-11-07T16:43:38.222Z level=INFO source=manager.go:307 msg="Remaining notifications drained" component=notifier
prometheus-1       | time=2025-11-07T16:43:38.230Z level=INFO source=manager.go:234 msg="Notification manager stopped" component=notifier
prometheus-1       | time=2025-11-07T16:43:38.230Z level=INFO source=main.go:1426 msg="Notifier manager stopped"
prometheus-1       | time=2025-11-07T16:43:38.231Z level=INFO source=main.go:1440 msg="See you next time!"
prometheus-1       | time=2026-02-25T10:39:26.253Z level=INFO source=main.go:1549 msg="updated GOGC" old=100 new=75
prometheus-1       | time=2026-02-25T10:39:26.254Z level=INFO source=main.go:680 msg="Leaving GOMAXPROCS=12: CPU quota undefined" component=automaxprocs
prometheus-1       | time=2026-02-25T10:39:26.254Z level=INFO source=memlimit.go:198 msg="GOMEMLIMIT is updated" component=automemlimit package=github.com/KimMachineGun/automemlimit/memlimit GOMEMLIMIT=7395976396 previous=9223372036854775807
prometheus-1       | time=2026-02-25T10:39:26.255Z level=INFO source=main.go:722 msg="No time or size retention was set so using the default time retention" duration=15d
prometheus-1       | time=2026-02-25T10:39:26.255Z level=INFO source=main.go:773 msg="Starting Prometheus Server" mode=server version="(version=3.7.1, branch=HEAD, revision=0aeb4fddc93b64e4e95104d5e8ea8b55ad36fb61)"
prometheus-1       | time=2026-02-25T10:39:26.255Z level=INFO source=main.go:778 msg="operational information" build_context="(go=go1.25.3, platform=linux/arm64, user=root@54bf11233185, date=20251017-06:35:17, tags=netgo,builtinassets)" host_details="(Linux 6.10.14-linuxkit #1 SMP Sat May 17 08:28:57 UTC 2025 aarch64 826fd5223daa (none))" fd_limits="(soft=1048576, hard=1048576)" vm_limits="(soft=unlimited, hard=unlimited)"
prometheus-1       | time=2026-02-25T10:39:26.263Z level=INFO source=web.go:660 msg="Start listening for connections" component=web address=0.0.0.0:9090
prometheus-1       | time=2026-02-25T10:39:26.267Z level=INFO source=main.go:1293 msg="Starting TSDB ..."
prometheus-1       | time=2026-02-25T10:39:26.269Z level=INFO source=tls_config.go:346 msg="Listening on" component=web address=[::]:9090
prometheus-1       | time=2026-02-25T10:39:26.269Z level=INFO source=tls_config.go:349 msg="TLS is disabled." component=web http2=false address=[::]:9090
prometheus-1       | time=2026-02-25T10:39:26.277Z level=INFO source=head.go:669 msg="Replaying on-disk memory mappable chunks if any" component=tsdb
prometheus-1       | time=2026-02-25T10:39:26.280Z level=INFO source=head.go:755 msg="On-disk memory mappable chunks replay completed" component=tsdb duration=693.834Âµs
prometheus-1       | time=2026-02-25T10:39:26.280Z level=INFO source=head.go:763 msg="Replaying WAL, this may take a while" component=tsdb
prometheus-1       | time=2026-02-25T10:39:26.284Z level=INFO source=head.go:836 msg="WAL segment loaded" component=tsdb segment=0 maxSegment=1 duration=4.560333ms
prometheus-1       | time=2026-02-25T10:39:26.285Z level=INFO source=head.go:836 msg="WAL segment loaded" component=tsdb segment=1 maxSegment=1 duration=93.583Âµs
prometheus-1       | time=2026-02-25T10:39:26.285Z level=INFO source=head.go:873 msg="WAL replay completed" component=tsdb checkpoint_replay_duration=33.417Âµs wal_replay_duration=4.717667ms wbl_replay_duration=41ns chunk_snapshot_load_duration=0s mmap_chunk_replay_duration=693.834Âµs total_replay_duration=5.483958ms
prometheus-1       | time=2026-02-25T10:39:26.288Z level=INFO source=main.go:1314 msg="filesystem information" fs_type=EXT4_SUPER_MAGIC
prometheus-1       | time=2026-02-25T10:39:26.288Z level=INFO source=main.go:1317 msg="TSDB started"
prometheus-1       | time=2026-02-25T10:39:26.288Z level=INFO source=main.go:1502 msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
prometheus-1       | time=2026-02-25T10:39:26.291Z level=INFO source=main.go:1542 msg="Completed loading of configuration file" db_storage=1.5Âµs remote_storage=1.125Âµs web_handler=458ns query_engine=4.75Âµs scrape=2.027042ms scrape_sd=24.542Âµs notify=959ns notify_sd=542ns rules=1.083Âµs tracing=5.334Âµs filename=/etc/prometheus/prometheus.yml totalDuration=3.755083ms
prometheus-1       | time=2026-02-25T10:39:26.291Z level=INFO source=main.go:1278 msg="Server is ready to receive web requests."
prometheus-1       | time=2026-02-25T10:39:26.291Z level=INFO source=manager.go:190 msg="Starting rule manager..." component="rule manager"
prometheus-1       | time=2026-02-25T10:39:38.523Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1762530473520 maxt=1762531200000 ulid=01KJA6420VZ3YJS4B1RQ7FJ3PT
prometheus-1       | time=2026-02-25T10:39:38.544Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1762530473520 maxt=1762531200000 ulid=01KJA6420VZ3YJS4B1RQ7FJ3PT duration=20.992125ms ooo=false
prometheus-1       | time=2026-02-25T10:39:38.544Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus-1       | time=2026-02-25T10:39:38.546Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.305375ms
prometheus-1       | time=2026-02-25T10:39:38.546Z level=INFO source=compact.go:565 msg="write block started" component=tsdb mint=1762531208512 maxt=1762538400000 ulid=01KJA6421JRJ4AZ5SNB07MG529
prometheus-1       | time=2026-02-25T10:39:38.559Z level=INFO source=compact.go:599 msg="write block completed" component=tsdb mint=1762531208512 maxt=1762538400000 ulid=01KJA6421JRJ4AZ5SNB07MG529 duration=13.420584ms ooo=false
prometheus-1       | time=2026-02-25T10:39:38.559Z level=INFO source=head.go:1420 msg="Head GC started" component=tsdb caller=truncateMemory
prometheus-1       | time=2026-02-25T10:39:38.561Z level=INFO source=head.go:1424 msg="Head GC completed" component=tsdb caller=truncateMemory duration=1.168459ms
zookeeper-1        | [2025-11-07 15:48:34,589] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,589] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,589] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,589] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,589] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,590] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,590] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,591] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)
zookeeper-1        | [2025-11-07 15:48:34,608] INFO minSessionTimeout set to 4000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,608] INFO maxSessionTimeout set to 40000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,609] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper-1        | [2025-11-07 15:48:34,609] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper-1        | [2025-11-07 15:48:34,612] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2025-11-07 15:48:34,612] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2025-11-07 15:48:34,612] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2025-11-07 15:48:34,612] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2025-11-07 15:48:34,612] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2025-11-07 15:48:34,612] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2025-11-07 15:48:34,630] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,631] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,631] INFO Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:34,869] INFO Logging initialized @24724ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
zookeeper-1        | [2025-11-07 15:48:38,327] WARN o.e.j.s.ServletContextHandler@2584b82d{/,null,STOPPED} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2025-11-07 15:48:38,328] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2025-11-07 15:48:38,866] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS (org.eclipse.jetty.server.Server)
zookeeper-1        | [2025-11-07 15:48:39,292] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
zookeeper-1        | [2025-11-07 15:48:39,292] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
zookeeper-1        | [2025-11-07 15:48:39,298] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
zookeeper-1        | [2025-11-07 15:48:39,398] WARN ServletContext@o.e.j.s.ServletContextHandler@2584b82d{/,null,STARTING} has uncovered http methods for path: /* (org.eclipse.jetty.security.SecurityHandler)
zookeeper-1        | [2025-11-07 15:48:39,472] INFO Started o.e.j.s.ServletContextHandler@2584b82d{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2025-11-07 15:48:39,905] INFO Started ServerConnector@247310d0{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
zookeeper-1        | [2025-11-07 15:48:39,907] INFO Started @29761ms (org.eclipse.jetty.server.Server)
zookeeper-1        | [2025-11-07 15:48:39,907] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
zookeeper-1        | [2025-11-07 15:48:39,940] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper-1        | [2025-11-07 15:48:39,942] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper-1        | [2025-11-07 15:48:39,943] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 24 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper-1        | [2025-11-07 15:48:39,945] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper-1        | [2025-11-07 15:48:40,077] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
zookeeper-1        | [2025-11-07 15:48:40,077] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
zookeeper-1        | [2025-11-07 15:48:40,078] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2025-11-07 15:48:40,078] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2025-11-07 15:48:40,310] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)
zookeeper-1        | [2025-11-07 15:48:40,310] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2025-11-07 15:48:40,319] INFO Snapshot loaded in 241 ms, highest zxid is 0x0, digest is 1371985504 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2025-11-07 15:48:40,319] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2025-11-07 15:48:40,320] INFO Snapshot taken in 0 ms (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2025-11-07 15:48:40,403] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)
zookeeper-1        | [2025-11-07 15:48:40,411] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)
zookeeper-1        | [2025-11-07 15:48:40,849] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)
zookeeper-1        | [2025-11-07 15:48:40,850] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)
zookeeper-1        | [2025-11-07 15:48:41,465] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
zookeeper-1        | ===> User
zookeeper-1        | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
zookeeper-1        | ===> Configuring ...
zookeeper-1        | ===> Running preflight checks ... 
zookeeper-1        | ===> Check if /var/lib/zookeeper/data is writable ...
zookeeper-1        | ===> Check if /var/lib/zookeeper/log is writable ...
zookeeper-1        | ===> Launching ... 
zookeeper-1        | ===> Launching zookeeper ... 
zookeeper-1        | [2026-02-25 10:39:33,061] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,086] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,086] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,086] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,086] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,093] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1        | [2026-02-25 10:39:33,093] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1        | [2026-02-25 10:39:33,093] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
zookeeper-1        | [2026-02-25 10:39:33,093] WARN Either no config or no quorum defined in config, running in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
zookeeper-1        | [2026-02-25 10:39:33,100] INFO Log4j 1.2 jmx support found and enabled. (org.apache.zookeeper.jmx.ManagedUtil)
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
zookeeper-1        | [2026-02-25 10:39:33,125] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,127] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,127] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,127] INFO observerMasterPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,127] INFO metricsProvider.className is org.apache.zookeeper.metrics.impl.DefaultMetricsProvider (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
zookeeper-1        | [2026-02-25 10:39:33,127] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
zookeeper-1        | [2026-02-25 10:39:33,156] INFO ServerMetrics initialized with provider org.apache.zookeeper.metrics.impl.DefaultMetricsProvider@52102734 (org.apache.zookeeper.server.ServerMetrics)
zookeeper-1        | [2026-02-25 10:39:33,161] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO   ______                  _                                           (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO  |___  /                 | |                                          (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __    (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |     (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_| (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO                                               | |                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO                                               |_|                      (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,179] INFO  (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:host.name=db3d5cbacf85 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:java.version=11.0.13 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.server.ZooKeeperServer)
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  write to custom object with { processEnv: myObject }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:39:31.963Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-json-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-raft-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/netty-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/connect-transforms-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-shell-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-clients-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/trogdor-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.68.Final.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.68.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.19.3.jar:/usr/bin/../share/java/kafka/kafka-streams-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.68.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/connect-mirror-7.0.1-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,181] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:os.memory.free=492MB (org.apache.zookeeper.server.ZooKeeperServer)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO zookeeper.enableEagerACLCheck = false (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO zookeeper.digest.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO zookeeper.closeSessionTxn.enabled = true (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO zookeeper.flushDelay=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO zookeeper.maxWriteQueuePollTime=0 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO zookeeper.maxBatchSize=1000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,182] INFO zookeeper.intBufferStartingSizeBytes = 1024 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,184] INFO Weighed connection throttling is disabled (org.apache.zookeeper.server.BlueThrottle)
zookeeper-1        | [2026-02-25 10:39:33,186] INFO minSessionTimeout set to 4000 (org.apache.zookeeper.server.ZooKeeperServer)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
zookeeper-1        | [2026-02-25 10:39:33,186] INFO maxSessionTimeout set to 40000 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,189] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper-1        | [2026-02-25 10:39:33,189] INFO Response cache size is initialized with value 400. (org.apache.zookeeper.server.ResponseCache)
zookeeper-1        | [2026-02-25 10:39:33,190] INFO zookeeper.pathStats.slotCapacity = 60 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 10:39:33,190] INFO zookeeper.pathStats.slotDuration = 15 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 10:39:33,190] INFO zookeeper.pathStats.maxDepth = 6 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 10:39:33,190] INFO zookeeper.pathStats.initialDelay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 10:39:33,190] INFO zookeeper.pathStats.delay = 5 (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 10:39:33,190] INFO zookeeper.pathStats.enabled = false (org.apache.zookeeper.server.util.RequestPathMetricsCollector)
zookeeper-1        | [2026-02-25 10:39:33,193] INFO The max bytes for all large requests are set to 104857600 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,194] INFO The large request threshold is set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,195] INFO Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 clientPortListenBacklog -1 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,229] INFO Logging initialized @1578ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
zookeeper-1        | [2026-02-25 10:39:33,454] WARN o.e.j.s.ServletContextHandler@7d3e8655{/,null,STOPPED} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
zookeeper-1        | [2026-02-25 10:39:33,454] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2026-02-25 10:39:33,504] INFO jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS (org.eclipse.jetty.server.Server)
zookeeper-1        | [2026-02-25 10:39:33,560] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
zookeeper-1        | [2026-02-25 10:39:33,561] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
zookeeper-1        | [2026-02-25 10:39:33,563] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
zookeeper-1        | [2026-02-25 10:39:33,566] WARN ServletContext@o.e.j.s.ServletContextHandler@7d3e8655{/,null,STARTING} has uncovered http methods for path: /* (org.eclipse.jetty.security.SecurityHandler)
zookeeper-1        | [2026-02-25 10:39:33,579] INFO Started o.e.j.s.ServletContextHandler@7d3e8655{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
zookeeper-1        | [2026-02-25 10:39:33,608] INFO Started ServerConnector@76494737{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
zookeeper-1        | [2026-02-25 10:39:33,608] INFO Started @1957ms (org.eclipse.jetty.server.Server)
zookeeper-1        | [2026-02-25 10:39:33,609] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
zookeeper-1        | [2026-02-25 10:39:33,616] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper-1        | [2026-02-25 10:39:33,617] WARN maxCnxns is not configured, using default value 0. (org.apache.zookeeper.server.ServerCnxnFactory)
zookeeper-1        | [2026-02-25 10:39:33,618] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 24 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper-1        | [2026-02-25 10:39:33,620] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
zookeeper-1        | [2026-02-25 10:39:33,642] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
zookeeper-1        | [2026-02-25 10:39:33,642] INFO Using org.apache.zookeeper.server.watch.WatchManager as watch manager (org.apache.zookeeper.server.watch.WatchManagerFactory)
zookeeper-1        | [2026-02-25 10:39:33,644] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2026-02-25 10:39:33,644] INFO zookeeper.commitLogCount=500 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2026-02-25 10:39:33,652] INFO zookeeper.snapshot.compression.method = CHECKED (org.apache.zookeeper.server.persistence.SnapStream)
zookeeper-1        | [2026-02-25 10:39:33,653] INFO Reading snapshot /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileSnap)
zookeeper-1        | [2026-02-25 10:39:33,658] INFO The digest value is empty in snapshot (org.apache.zookeeper.server.DataTree)
zookeeper-1        | [2026-02-25 10:39:33,711] INFO 141 txns loaded in 48 ms (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2026-02-25 10:39:33,711] INFO Snapshot loaded in 67 ms, highest zxid is 0x8d, digest is 297590216985 (org.apache.zookeeper.server.ZKDatabase)
zookeeper-1        | [2026-02-25 10:39:33,712] INFO Snapshotting: 0x8d to /var/lib/zookeeper/data/version-2/snapshot.8d (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
zookeeper-1        | [2026-02-25 10:39:33,719] INFO Snapshot taken in 7 ms (org.apache.zookeeper.server.ZooKeeperServer)
zookeeper-1        | [2026-02-25 10:39:33,728] INFO zookeeper.request_throttler.shutdownTimeout = 10000 (org.apache.zookeeper.server.RequestThrottler)
zookeeper-1        | [2026-02-25 10:39:33,728] INFO PrepRequestProcessor (sid:0) started, reconfigEnabled=false (org.apache.zookeeper.server.PrepRequestProcessor)
zookeeper-1        | [2026-02-25 10:39:33,764] INFO Using checkIntervalMs=60000 maxPerMinute=10000 maxNeverUsedIntervalMs=0 (org.apache.zookeeper.server.ContainerManager)
zookeeper-1        | [2026-02-25 10:39:33,766] INFO ZooKeeper audit is disabled. (org.apache.zookeeper.audit.ZKAuditProvider)
zookeeper-1        | [2026-02-25 10:39:33,807] INFO Creating new log file: log.8e (org.apache.zookeeper.server.persistence.FileTxnLog)
zookeeper-1        | [2026-02-25 10:39:51,906] INFO Expiring session 0x100005b512f0001, timeout of 18000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): CREATE TABLE IF NOT EXISTS "DeadLetterEvents" ("id" VARCHAR(255) , "eventType" VARCHAR(255), "payload" JSON, "errorMessage" TEXT, "createdAt" TIMESTAMP WITH TIME ZONE, "updatedAt" TIMESTAMP WITH TIME ZONE NOT NULL, PRIMARY KEY ("id"));
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.122Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.123Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":0,"retryTime":277}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.402Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.403Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":1,"retryTime":552}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.956Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.956Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":2,"retryTime":1256}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:34.213Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:34.214Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":3,"retryTime":2892}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:37.120Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:37.122Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":4,"retryTime":6600}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:43.750Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:43.752Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12032}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:39:43.753Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:43.770Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:43.771Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":325}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:44.107Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:44.108Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":624}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:44.738Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:44.739Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1342}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:46.087Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:46.087Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2826}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:48.925Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:48.926Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5386}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:39:53 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:39:53.544Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:54.323Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:54.324Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11688}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 11688,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | npm notice
order-service-1    | npm notice New major version of npm available! 10.8.2 -> 11.10.1
order-service-1    | npm notice Changelog: https://github.com/npm/cli/releases/tag/v11.10.1
order-service-1    | npm notice To update run: npm install -g npm@11.10.1
order-service-1    | npm notice
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ¤– agentic secret storage: https://dotenvx.com/as2
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:39:55.066Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
grafana-1          | logger=settings t=2025-11-07T15:47:38.861840377Z level=info msg="Path Home" path=/usr/share/grafana
grafana-1          | logger=settings t=2025-11-07T15:47:38.861842294Z level=info msg="Path Data" path=/var/lib/grafana
grafana-1          | logger=settings t=2025-11-07T15:47:38.861843836Z level=info msg="Path Logs" path=/var/log/grafana
grafana-1          | logger=settings t=2025-11-07T15:47:38.861845127Z level=info msg="Path Plugins" path=/var/lib/grafana/plugins
grafana-1          | logger=settings t=2025-11-07T15:47:38.861846586Z level=info msg="Path Provisioning" path=/etc/grafana/provisioning
grafana-1          | logger=settings t=2025-11-07T15:47:38.861848377Z level=info msg="App mode production"
grafana-1          | logger=featuremgmt t=2025-11-07T15:47:38.862636169Z level=info msg=FeatureToggles logsExploreTableVisualisation=true recordedQueriesMulti=true lokiLabelNamesQueryApi=true prometheusAzureOverrideAudience=true tlsMemcached=true ssoSettingsLDAP=true improvedExternalSessionHandling=true formatString=true grafanaAssistantInProfilesDrilldown=true azureMonitorEnableUserAuth=true alertingImportYAMLUI=true logsContextDatasourceUi=true logRowsPopoverMenu=true awsAsyncQueryCaching=true publicDashboardsScene=true preinstallAutoUpdate=true panelMonitoring=true influxdbBackendMigration=true useSessionStorageForRedirection=true alertRuleRestore=true newPDFRendering=true alertingSaveStateCompressed=true alertingRuleVersionHistoryRestore=true annotationPermissionUpdate=true alertingQueryAndExpressionsStepMode=true alertingRulePermanentlyDelete=true dataplaneFrontendFallback=true dashboardDsAdHocFiltering=true alertingUIOptimizeReducer=true skipTokenRotationIfRecent=true groupToNestedTableTransformation=true improvedExternalSessionHandlingSAML=true dashboardSceneForViewers=true addFieldFromCalculationStatFunctions=true alertingBulkActionsInUI=true transformationsRedesign=true azureMonitorPrometheusExemplars=true newFiltersUI=true onPremToCloudMigrations=true unifiedStorageHistoryPruner=true correlations=true grafanaconThemes=true pinNavItems=true alertingNotificationsStepMode=true dashboardScene=true alertingMigrationUI=true logsPanelControls=true dashgpt=true newDashboardSharingComponent=true logsInfiniteScrolling=true dashboardSceneSolo=true alertingRuleRecoverDeleted=true cloudWatchCrossAccountQuerying=true awsDatasourcesTempCredentials=true cloudWatchNewLabelParsing=true adhocFiltersInTooltips=true kubernetesDashboards=true lokiQuerySplitting=true unifiedRequestLog=true cloudWatchRoundUpEndTime=true promQLScope=true
grafana-1          | logger=sqlstore t=2025-11-07T15:47:38.862704461Z level=info msg="Connecting to DB" dbtype=sqlite3
grafana-1          | logger=sqlstore t=2025-11-07T15:47:38.862720419Z level=info msg="Creating SQLite database file" path=/var/lib/grafana/grafana.db
grafana-1          | logger=migrator t=2025-11-07T15:47:39.041247627Z level=info msg="Locking database"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.041276461Z level=info msg="Starting DB migrations"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.041814544Z level=info msg="Executing migration" id="create migration_log table"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.051653794Z level=info msg="Migration successfully executed" id="create migration_log table" duration=9.837ms
grafana-1          | logger=migrator t=2025-11-07T15:47:39.101267127Z level=info msg="Executing migration" id="create user table"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.102269044Z level=info msg="Migration successfully executed" id="create user table" duration=1.002291ms
grafana-1          | logger=migrator t=2025-11-07T15:47:39.170564836Z level=info msg="Executing migration" id="add unique index user.login"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.171523961Z level=info msg="Migration successfully executed" id="add unique index user.login" duration=960.667Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:39.297652128Z level=info msg="Executing migration" id="add unique index user.email"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.298563878Z level=info msg="Migration successfully executed" id="add unique index user.email" duration=913.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:39.367799294Z level=info msg="Executing migration" id="drop index UQE_user_login - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.368683419Z level=info msg="Migration successfully executed" id="drop index UQE_user_login - v1" duration=885.541Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:39.554719294Z level=info msg="Executing migration" id="drop index UQE_user_email - v1"
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
grafana-1          | logger=migrator t=2025-11-07T15:47:39.559703169Z level=info msg="Migration successfully executed" id="drop index UQE_user_email - v1" duration=4.982584ms
grafana-1          | logger=migrator t=2025-11-07T15:47:39.689951461Z level=info msg="Executing migration" id="Rename table user to user_v1 - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.701177628Z level=info msg="Migration successfully executed" id="Rename table user to user_v1 - v1" duration=11.222791ms
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:39:31.925Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
grafana-1          | logger=migrator t=2025-11-07T15:47:39.810154711Z level=info msg="Executing migration" id="create user table v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.814138294Z level=info msg="Migration successfully executed" id="create user table v2" duration=3.983334ms
grafana-1          | logger=migrator t=2025-11-07T15:47:39.896592294Z level=info msg="Executing migration" id="create index UQE_user_login - v2"
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
grafana-1          | logger=migrator t=2025-11-07T15:47:39.897473128Z level=info msg="Migration successfully executed" id="create index UQE_user_login - v2" duration=881.833Âµs
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2025-11-07T15:47:39.926620961Z level=info msg="Executing migration" id="create index UQE_user_email - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:39.931364169Z level=info msg="Migration successfully executed" id="create index UQE_user_email - v2" duration=4.71925ms
grafana-1          | logger=migrator t=2025-11-07T15:47:40.053530795Z level=info msg="Executing migration" id="copy data_source v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:40.054138086Z level=info msg="Migration successfully executed" id="copy data_source v1 to v2" duration=611.5Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:40.15371867Z level=info msg="Executing migration" id="Drop old table user_v1"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:47:40.154748461Z level=info msg="Migration successfully executed" id="Drop old table user_v1" duration=1.031084ms
grafana-1          | logger=migrator t=2025-11-07T15:47:40.262691586Z level=info msg="Executing migration" id="Add column help_flags1 to user table"
grafana-1          | logger=migrator t=2025-11-07T15:47:40.263804211Z level=info msg="Migration successfully executed" id="Add column help_flags1 to user table" duration=1.114125ms
grafana-1          | logger=migrator t=2025-11-07T15:47:40.444700586Z level=info msg="Executing migration" id="Update user table charset"
grafana-1          | logger=migrator t=2025-11-07T15:47:40.444736628Z level=info msg="Migration successfully executed" id="Update user table charset" duration=38Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:40.541019378Z level=info msg="Executing migration" id="Add last_seen_at column to user"
grafana-1          | logger=migrator t=2025-11-07T15:47:40.541930045Z level=info msg="Migration successfully executed" id="Add last_seen_at column to user" duration=911.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:40.60078292Z level=info msg="Executing migration" id="Add missing user data"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:47:40.616985128Z level=info msg="Migration successfully executed" id="Add missing user data" duration=16.194667ms
grafana-1          | logger=migrator t=2025-11-07T15:47:40.752198837Z level=info msg="Executing migration" id="Add is_disabled column to user"
grafana-1          | logger=migrator t=2025-11-07T15:47:40.754017878Z level=info msg="Migration successfully executed" id="Add is_disabled column to user" duration=1.822708ms
grafana-1          | logger=migrator t=2025-11-07T15:47:40.915784962Z level=info msg="Executing migration" id="Add index user.login/user.email"
grafana-1          | logger=migrator t=2025-11-07T15:47:40.916819337Z level=info msg="Migration successfully executed" id="Add index user.login/user.email" duration=1.035208ms
grafana-1          | logger=migrator t=2025-11-07T15:47:41.142956087Z level=info msg="Executing migration" id="Add is_service_account column to user"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:47:41.145307587Z level=info msg="Migration successfully executed" id="Add is_service_account column to user" duration=2.351542ms
grafana-1          | logger=migrator t=2025-11-07T15:47:41.206172795Z level=info msg="Executing migration" id="Update is_service_account column to nullable"
grafana-1          | logger=migrator t=2025-11-07T15:47:41.223612628Z level=info msg="Migration successfully executed" id="Update is_service_account column to nullable" duration=17.44ms
grafana-1          | logger=migrator t=2025-11-07T15:47:41.384254629Z level=info msg="Executing migration" id="Add uid column to user"
grafana-1          | logger=migrator t=2025-11-07T15:47:41.38570742Z level=info msg="Migration successfully executed" id="Add uid column to user" duration=1.456416ms
grafana-1          | logger=migrator t=2025-11-07T15:47:41.523010045Z level=info msg="Executing migration" id="Update uid column values for users"
grafana-1          | logger=migrator t=2025-11-07T15:47:41.523261462Z level=info msg="Migration successfully executed" id="Update uid column values for users" duration=252.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:41.568094754Z level=info msg="Executing migration" id="Make sure users uid are set"
grafana-1          | logger=migrator t=2025-11-07T15:47:41.568529587Z level=info msg="Migration successfully executed" id="Make sure users uid are set" duration=435.875Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:41.677233587Z level=info msg="Executing migration" id="Add unique index user_uid"
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2025-11-07T15:47:41.67839317Z level=info msg="Migration successfully executed" id="Add unique index user_uid" duration=1.161041ms
grafana-1          | logger=migrator t=2025-11-07T15:47:41.969695004Z level=info msg="Executing migration" id="Add is_provisioned column to user"
grafana-1          | logger=migrator t=2025-11-07T15:47:41.973733879Z level=info msg="Migration successfully executed" id="Add is_provisioned column to user" duration=4.036083ms
grafana-1          | logger=migrator t=2025-11-07T15:47:42.125841921Z level=info msg="Executing migration" id="update login field with orgid to allow for multiple service accounts with same name across orgs"
grafana-1          | logger=migrator t=2025-11-07T15:47:42.126328796Z level=info msg="Migration successfully executed" id="update login field with orgid to allow for multiple service accounts with same name across orgs" duration=491.709Âµs
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2025-11-07T15:47:42.287182296Z level=info msg="Executing migration" id="update service accounts login field orgid to appear only once"
grafana-1          | logger=migrator t=2025-11-07T15:47:42.299892129Z level=info msg="Migration successfully executed" id="update service accounts login field orgid to appear only once" duration=12.70875ms
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:47:42.528179004Z level=info msg="Executing migration" id="update login and email fields to lowercase"
grafana-1          | logger=migrator t=2025-11-07T15:47:42.531045462Z level=info msg="Migration successfully executed" id="update login and email fields to lowercase" duration=2.866334ms
grafana-1          | logger=migrator t=2025-11-07T15:47:42.714463254Z level=info msg="Executing migration" id="update login and email fields to lowercase2"
grafana-1          | logger=migrator t=2025-11-07T15:47:42.717812296Z level=info msg="Migration successfully executed" id="update login and email fields to lowercase2" duration=3.3465ms
grafana-1          | logger=migrator t=2025-11-07T15:47:42.837883004Z level=info msg="Executing migration" id="Add index on user.is_service_account and user.last_seen_at"
grafana-1          | logger=migrator t=2025-11-07T15:47:42.838898963Z level=info msg="Migration successfully executed" id="Add index on user.is_service_account and user.last_seen_at" duration=1.018542ms
grafana-1          | logger=migrator t=2025-11-07T15:47:42.972909254Z level=info msg="Executing migration" id="create temp user table v1-7"
grafana-1          | logger=migrator t=2025-11-07T15:47:42.974092004Z level=info msg="Migration successfully executed" id="create temp user table v1-7" duration=1.182958ms
grafana-1          | logger=migrator t=2025-11-07T15:47:43.180771504Z level=info msg="Executing migration" id="create index IDX_temp_user_email - v1-7"
grafana-1          | logger=migrator t=2025-11-07T15:47:43.181401254Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_email - v1-7" duration=634.083Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:43.295768963Z level=info msg="Executing migration" id="create index IDX_temp_user_org_id - v1-7"
grafana-1          | logger=migrator t=2025-11-07T15:47:43.301993254Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_org_id - v1-7" duration=6.216042ms
grafana-1          | logger=migrator t=2025-11-07T15:47:43.591543963Z level=info msg="Executing migration" id="create index IDX_temp_user_code - v1-7"
grafana-1          | logger=migrator t=2025-11-07T15:47:43.592649588Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_code - v1-7" duration=1.106416ms
grafana-1          | logger=migrator t=2025-11-07T15:47:43.668471463Z level=info msg="Executing migration" id="create index IDX_temp_user_status - v1-7"
grafana-1          | logger=migrator t=2025-11-07T15:47:43.669124588Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_status - v1-7" duration=655.292Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:43.848138046Z level=info msg="Executing migration" id="Update temp_user table charset"
grafana-1          | logger=migrator t=2025-11-07T15:47:43.848182963Z level=info msg="Migration successfully executed" id="Update temp_user table charset" duration=45.416Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:43.962648921Z level=info msg="Executing migration" id="drop index IDX_temp_user_email - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:43.963209546Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_email - v1" duration=562.125Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:44.060729963Z level=info msg="Executing migration" id="drop index IDX_temp_user_org_id - v1"
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.096Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.096Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":0,"retryTime":341}
grafana-1          | logger=migrator t=2025-11-07T15:47:44.06275988Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_org_id - v1" duration=2.032416ms
grafana-1          | logger=migrator t=2025-11-07T15:47:44.11825363Z level=info msg="Executing migration" id="drop index IDX_temp_user_code - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:44.137255838Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_code - v1" duration=19.003541ms
grafana-1          | logger=migrator t=2025-11-07T15:47:44.27284738Z level=info msg="Executing migration" id="drop index IDX_temp_user_status - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:44.273695588Z level=info msg="Migration successfully executed" id="drop index IDX_temp_user_status - v1" duration=850.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:44.527548297Z level=info msg="Executing migration" id="Rename table temp_user to temp_user_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:44.53088588Z level=info msg="Migration successfully executed" id="Rename table temp_user to temp_user_tmp_qwerty - v1" duration=3.336542ms
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.440Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:32.440Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":1,"retryTime":652}
grafana-1          | logger=migrator t=2025-11-07T15:47:44.645005463Z level=info msg="Executing migration" id="create temp_user v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:44.64616413Z level=info msg="Migration successfully executed" id="create temp_user v2" duration=1.253084ms
grafana-1          | logger=migrator t=2025-11-07T15:47:44.794130422Z level=info msg="Executing migration" id="create index IDX_temp_user_email - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:44.798698255Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_email - v2" duration=4.56875ms
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:33.094Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:33.095Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":2,"retryTime":1532}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:34.629Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:34.630Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":3,"retryTime":3076}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:37.711Z","logger":"kafkajs","message":"[Connection] Connection error: connect ECONNREFUSED 172.18.0.7:9092","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: connect ECONNREFUSED 172.18.0.7:9092\n    at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1555:16)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:37.712Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: connect ECONNREFUSED 172.18.0.7:9092","retryCount":4,"retryTime":6224}
kafka-1            | ===> User
kafka-1            | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:43.955Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:43.956Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11902}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
grafana-1          | logger=migrator t=2025-11-07T15:47:44.991852422Z level=info msg="Executing migration" id="create index IDX_temp_user_org_id - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:44.993217964Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_org_id - v2" duration=1.365375ms
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
grafana-1          | logger=migrator t=2025-11-07T15:47:45.292466672Z level=info msg="Executing migration" id="create index IDX_temp_user_code - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:45.293635839Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_code - v2" duration=1.247042ms
grafana-1          | logger=migrator t=2025-11-07T15:47:45.393338714Z level=info msg="Executing migration" id="create index IDX_temp_user_status - v2"
kafka-1            | ===> Configuring ...
kafka-1            | ===> Running preflight checks ... 
kafka-1            | ===> Check if /var/lib/kafka/data is writable ...
kafka-1            | ===> Check if Zookeeper is healthy ...
grafana-1          | logger=migrator t=2025-11-07T15:47:45.393986214Z level=info msg="Migration successfully executed" id="create index IDX_temp_user_status - v2" duration=648.458Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:45.564932006Z level=info msg="Executing migration" id="copy temp_user v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:45.565887881Z level=info msg="Migration successfully executed" id="copy temp_user v1 to v2" duration=956.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:45.723041297Z level=info msg="Executing migration" id="drop temp_user_tmp_qwerty"
kafka-1            | SLF4J: Class path contains multiple SLF4J bindings.
kafka-1            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
kafka-1            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
grafana-1          | logger=migrator t=2025-11-07T15:47:45.723844881Z level=info msg="Migration successfully executed" id="drop temp_user_tmp_qwerty" duration=805.458Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:45.959537714Z level=info msg="Executing migration" id="Set created for temp users that will otherwise prematurely expire"
grafana-1          | logger=migrator t=2025-11-07T15:47:45.960149839Z level=info msg="Migration successfully executed" id="Set created for temp users that will otherwise prematurely expire" duration=613.125Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:46.279457423Z level=info msg="Executing migration" id="create star table"
grafana-1          | logger=migrator t=2025-11-07T15:47:46.280880589Z level=info msg="Migration successfully executed" id="create star table" duration=1.429833ms
kafka-1            | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
kafka-1            | SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=363a57706629
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.13
grafana-1          | logger=migrator t=2025-11-07T15:47:46.539948881Z level=info msg="Executing migration" id="add unique index star.user_id_dashboard_id"
grafana-1          | logger=migrator t=2025-11-07T15:47:46.540671048Z level=info msg="Migration successfully executed" id="add unique index star.user_id_dashboard_id" duration=723.208Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:46.773465548Z level=info msg="Executing migration" id="Add column dashboard_uid in star"
grafana-1          | logger=migrator t=2025-11-07T15:47:46.827264589Z level=info msg="Migration successfully executed" id="Add column dashboard_uid in star" duration=53.795459ms
grafana-1          | logger=migrator t=2025-11-07T15:47:47.010122881Z level=info msg="Executing migration" id="Add column org_id in star"
grafana-1          | logger=migrator t=2025-11-07T15:47:47.011233298Z level=info msg="Migration successfully executed" id="Add column org_id in star" duration=1.111333ms
grafana-1          | logger=migrator t=2025-11-07T15:47:47.189250923Z level=info msg="Executing migration" id="Add column updated in star"
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 11902,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 11902,
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
grafana-1          | logger=migrator t=2025-11-07T15:47:47.193972881Z level=info msg="Migration successfully executed" id="Add column updated in star" duration=4.72275ms
grafana-1          | logger=migrator t=2025-11-07T15:47:47.38935659Z level=info msg="Executing migration" id="add index in star table on dashboard_uid, org_id and user_id columns"
grafana-1          | logger=migrator t=2025-11-07T15:47:47.392538923Z level=info msg="Migration successfully executed" id="add index in star table on dashboard_uid, org_id and user_id columns" duration=3.201208ms
grafana-1          | logger=migrator t=2025-11-07T15:47:47.574780965Z level=info msg="Executing migration" id="create org table v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:47.575798298Z level=info msg="Migration successfully executed" id="create org table v1" duration=1.017917ms
grafana-1          | logger=migrator t=2025-11-07T15:47:47.841068423Z level=info msg="Executing migration" id="create index UQE_org_name - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:47.844092548Z level=info msg="Migration successfully executed" id="create index UQE_org_name - v1" duration=3.556584ms
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/usr/share/java/cp-base-new/lz4-java-1.7.1.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/cp-base-new/zstd-jni-1.5.0-2.jar:/usr/share/java/cp-base-new/jolokia-core-1.6.2.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/kafka-metadata-7.0.1-ccs.jar:/usr/share/java/cp-base-new/json-simple-1.1.1.jar:/usr/share/java/cp-base-new/kafka-raft-7.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-databind-2.12.3.jar:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar:/usr/share/java/cp-base-new/kafka_2.13-7.0.1-ccs.jar:/usr/share/java/cp-base-new/kafka-server-common-7.0.1-ccs.jar:/usr/share/java/cp-base-new/metrics-core-4.1.12.1.jar:/usr/share/java/cp-base-new/snakeyaml-1.27.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/kafka-clients-7.0.1-ccs.jar:/usr/share/java/cp-base-new/confluent-log4j-1.2.17-cp2.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.3.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/jmx_prometheus_javaagent-0.14.0.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.12.3.jar:/usr/share/java/cp-base-new/kafka-storage-api-7.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/cp-base-new/snappy-java-1.1.8.1.jar:/usr/share/java/cp-base-new/gson-2.8.6.jar:/usr/share/java/cp-base-new/jackson-annotations-2.12.3.jar:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar:/usr/share/java/cp-base-new/disk-usage-agent-7.0.1.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.30.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.6.3.jar:/usr/share/java/cp-base-new/jackson-core-2.12.3.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.5.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/kafka-storage-7.0.1-ccs.jar:/usr/share/java/cp-base-new/common-utils-7.0.1.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/jolokia-jvm-1.6.2-agent.jar:/usr/share/java/cp-base-new/zookeeper-3.6.3.jar:/usr/share/java/cp-base-new/utility-belt-7.0.1.jar:/usr/share/java/cp-base-new/scala-library-2.13.5.jar
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
grafana-1          | logger=migrator t=2025-11-07T15:47:47.970312382Z level=info msg="Executing migration" id="create org_user table v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:47.971667673Z level=info msg="Migration successfully executed" id="create org_user table v1" duration=1.353875ms
grafana-1          | logger=migrator t=2025-11-07T15:47:48.12364734Z level=info msg="Executing migration" id="create index IDX_org_user_org_id - v1"
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=6.10.14-linuxkit
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/home/appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/home/appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=117MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=1960MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=124MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@a7e666
kafka-1            | [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
kafka-1            | [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 1048575 Bytes
kafka-1            | [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=false
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
grafana-1          | logger=migrator t=2025-11-07T15:47:48.124698007Z level=info msg="Migration successfully executed" id="create index IDX_org_user_org_id - v1" duration=1.053834ms
grafana-1          | logger=migrator t=2025-11-07T15:47:48.28533709Z level=info msg="Executing migration" id="create index UQE_org_user_org_id_user_id - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:48.296217632Z level=info msg="Migration successfully executed" id="create index UQE_org_user_org_id_user_id - v1" duration=10.877209ms
grafana-1          | logger=migrator t=2025-11-07T15:47:48.614371132Z level=info msg="Executing migration" id="create index IDX_org_user_user_id - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:48.615483799Z level=info msg="Migration successfully executed" id="create index IDX_org_user_user_id - v1" duration=1.113167ms
grafana-1          | logger=migrator t=2025-11-07T15:47:48.816583382Z level=info msg="Executing migration" id="Update org table charset"
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
grafana-1          | logger=migrator t=2025-11-07T15:47:48.816632424Z level=info msg="Migration successfully executed" id="Update org table charset" duration=51Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:48.97313834Z level=info msg="Executing migration" id="Update org_user table charset"
grafana-1          | logger=migrator t=2025-11-07T15:47:48.979450715Z level=info msg="Migration successfully executed" id="Update org_user table charset" duration=6.314875ms
grafana-1          | logger=migrator t=2025-11-07T15:47:49.283166674Z level=info msg="Executing migration" id="Migrate all Read Only Viewers to Viewers"
grafana-1          | logger=migrator t=2025-11-07T15:47:49.286608841Z level=info msg="Migration successfully executed" id="Migrate all Read Only Viewers to Viewers" duration=3.442375ms
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
grafana-1          | logger=migrator t=2025-11-07T15:47:49.460427549Z level=info msg="Executing migration" id="create dashboard table"
grafana-1          | logger=migrator t=2025-11-07T15:47:49.462176966Z level=info msg="Migration successfully executed" id="create dashboard table" duration=1.761375ms
grafana-1          | logger=migrator t=2025-11-07T15:47:49.769296383Z level=info msg="Executing migration" id="add index dashboard.account_id"
grafana-1          | logger=migrator t=2025-11-07T15:47:49.779749758Z level=info msg="Migration successfully executed" id="add index dashboard.account_id" duration=10.4505ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.022804549Z level=info msg="Executing migration" id="add unique index dashboard_account_id_slug"
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
grafana-1          | logger=migrator t=2025-11-07T15:47:50.025784841Z level=info msg="Migration successfully executed" id="add unique index dashboard_account_id_slug" duration=2.978667ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.245080508Z level=info msg="Executing migration" id="create dashboard_tag table"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.246175341Z level=info msg="Migration successfully executed" id="create dashboard_tag table" duration=1.100208ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.431315299Z level=info msg="Executing migration" id="add unique index dashboard_tag.dasboard_id_term"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.432548133Z level=info msg="Migration successfully executed" id="add unique index dashboard_tag.dasboard_id_term" duration=1.238ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.56416755Z level=info msg="Executing migration" id="drop index UQE_dashboard_tag_dashboard_id_term - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.565190925Z level=info msg="Migration successfully executed" id="drop index UQE_dashboard_tag_dashboard_id_term - v1" duration=1.025459ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.5807703Z level=info msg="Executing migration" id="Rename table dashboard to dashboard_v1 - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.600318383Z level=info msg="Migration successfully executed" id="Rename table dashboard to dashboard_v1 - v1" duration=19.545792ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.75022705Z level=info msg="Executing migration" id="create dashboard v2"
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:55.147Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:55.147Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":348}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:55.507Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:55.509Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":580}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:56.098Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:56.098Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1310}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:57.420Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:39:57.422Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2886}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:00.320Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:00.321Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5854}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.183Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.184Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12248}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:40:06.185Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.192Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.193Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":253}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.452Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.453Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":474}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.936Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:06.936Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":928}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:07.876Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:07.876Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1644}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:40:08 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:40:08.548Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:09.527Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:09.528Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3506}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:13.047Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:13.048Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7524}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 7524,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ¤– agentic secret storage: https://dotenvx.com/as2
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:40:13.669Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:13.748Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:13.749Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":277}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.031Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.032Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":450}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.488Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.488Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":958}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:15.463Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:15.466Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1850}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:17.326Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
grafana-1          | logger=migrator t=2025-11-07T15:47:50.750886966Z level=info msg="Migration successfully executed" id="create dashboard v2" duration=660.959Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:50.7636543Z level=info msg="Executing migration" id="create index IDX_dashboard_org_id - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.769343216Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_org_id - v2" duration=5.699292ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.794594508Z level=info msg="Executing migration" id="create index UQE_dashboard_org_id_slug - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.796584716Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_org_id_slug - v2" duration=1.990584ms
grafana-1          | logger=migrator t=2025-11-07T15:47:50.807014883Z level=info msg="Executing migration" id="copy dashboard v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.8076403Z level=info msg="Migration successfully executed" id="copy dashboard v1 to v2" duration=626.667Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:50.984541716Z level=info msg="Executing migration" id="drop table dashboard_v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:50.99104755Z level=info msg="Migration successfully executed" id="drop table dashboard_v1" duration=6.000375ms
grafana-1          | logger=migrator t=2025-11-07T15:47:51.1479598Z level=info msg="Executing migration" id="alter dashboard.data to mediumtext v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:51.147993092Z level=info msg="Migration successfully executed" id="alter dashboard.data to mediumtext v1" duration=35.958Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:51.319066925Z level=info msg="Executing migration" id="Add column updated_by in dashboard - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:51.324604508Z level=info msg="Migration successfully executed" id="Add column updated_by in dashboard - v2" duration=5.537791ms
grafana-1          | logger=migrator t=2025-11-07T15:47:51.525751092Z level=info msg="Executing migration" id="Add column created_by in dashboard - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:51.546504592Z level=info msg="Migration successfully executed" id="Add column created_by in dashboard - v2" duration=20.735916ms
grafana-1          | logger=migrator t=2025-11-07T15:47:51.631920425Z level=info msg="Executing migration" id="Add column gnetId in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:51.634364467Z level=info msg="Migration successfully executed" id="Add column gnetId in dashboard" duration=2.449583ms
grafana-1          | logger=migrator t=2025-11-07T15:47:51.929238509Z level=info msg="Executing migration" id="Add index for gnetId in dashboard"
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
grafana-1          | logger=migrator t=2025-11-07T15:47:51.930322175Z level=info msg="Migration successfully executed" id="Add index for gnetId in dashboard" duration=1.085083ms
grafana-1          | logger=migrator t=2025-11-07T15:47:52.102226592Z level=info msg="Executing migration" id="Add column plugin_id in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:52.1089828Z level=info msg="Migration successfully executed" id="Add column plugin_id in dashboard" duration=6.760625ms
product-service-1  |     [cause]: undefined
grafana-1          | logger=migrator t=2025-11-07T15:47:52.253995509Z level=info msg="Executing migration" id="Add index for plugin_id in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:52.25711805Z level=info msg="Migration successfully executed" id="Add index for plugin_id in dashboard" duration=3.12125ms
grafana-1          | logger=migrator t=2025-11-07T15:47:52.513845759Z level=info msg="Executing migration" id="Add index for dashboard_id in dashboard_tag"
grafana-1          | logger=migrator t=2025-11-07T15:47:52.514660759Z level=info msg="Migration successfully executed" id="Add index for dashboard_id in dashboard_tag" duration=816.292Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:52.806791801Z level=info msg="Executing migration" id="Update dashboard table charset"
grafana-1          | logger=migrator t=2025-11-07T15:47:52.806850967Z level=info msg="Migration successfully executed" id="Update dashboard table charset" duration=62.083Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:53.002569134Z level=info msg="Executing migration" id="Update dashboard_tag table charset"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.002638759Z level=info msg="Migration successfully executed" id="Update dashboard_tag table charset" duration=72Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:53.147004551Z level=info msg="Executing migration" id="Add column folder_id in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.149760926Z level=info msg="Migration successfully executed" id="Add column folder_id in dashboard" duration=2.757583ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.312693843Z level=info msg="Executing migration" id="Add column isFolder in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.315121968Z level=info msg="Migration successfully executed" id="Add column isFolder in dashboard" duration=2.427625ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.465403968Z level=info msg="Executing migration" id="Add column has_acl in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.468959676Z level=info msg="Migration successfully executed" id="Add column has_acl in dashboard" duration=3.554625ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.579926009Z level=info msg="Executing migration" id="Add column uid in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.582541718Z level=info msg="Migration successfully executed" id="Add column uid in dashboard" duration=2.61575ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.866705134Z level=info msg="Executing migration" id="Update uid column values in dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.867912384Z level=info msg="Migration successfully executed" id="Update uid column values in dashboard" duration=1.207958ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.877238551Z level=info msg="Executing migration" id="Add unique index dashboard_org_id_uid"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.878235176Z level=info msg="Migration successfully executed" id="Add unique index dashboard_org_id_uid" duration=998.041Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:53.898844759Z level=info msg="Executing migration" id="Remove unique index org_id_slug"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.900606134Z level=info msg="Migration successfully executed" id="Remove unique index org_id_slug" duration=1.764667ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.913040759Z level=info msg="Executing migration" id="Update dashboard title length"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.913235468Z level=info msg="Migration successfully executed" id="Update dashboard title length" duration=197.5Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:53.920636593Z level=info msg="Executing migration" id="Add unique index for dashboard_org_id_title_folder_id"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.928107384Z level=info msg="Migration successfully executed" id="Add unique index for dashboard_org_id_title_folder_id" duration=7.468083ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.932449509Z level=info msg="Executing migration" id="create dashboard_provisioning"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.933479718Z level=info msg="Migration successfully executed" id="create dashboard_provisioning" duration=1.031876ms
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:17.327Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3934}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:21.276Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:21.279Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9220}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:40:21.283Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:21.301Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:21.301Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":328}
grafana-1          | logger=migrator t=2025-11-07T15:47:53.93805726Z level=info msg="Executing migration" id="Rename table dashboard_provisioning to dashboard_provisioning_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:53.958999635Z level=info msg="Migration successfully executed" id="Rename table dashboard_provisioning to dashboard_provisioning_tmp_qwerty - v1" duration=19.147666ms
grafana-1          | logger=migrator t=2025-11-07T15:47:53.985899218Z level=info msg="Executing migration" id="create dashboard_provisioning v2"
product-service-1  |   }
product-service-1  | }
product-service-1  | npm notice
product-service-1  | npm notice New major version of npm available! 10.8.2 -> 11.10.1
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:21.643Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:21.644Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":628}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:22.282Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:22.283Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1324}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:40:23 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:40:23.550Z"}
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
grafana-1          | logger=migrator t=2025-11-07T15:47:53.991835801Z level=info msg="Migration successfully executed" id="create dashboard_provisioning v2" duration=5.931583ms
grafana-1          | logger=migrator t=2025-11-07T15:47:54.06603126Z level=info msg="Executing migration" id="create index IDX_dashboard_provisioning_dashboard_id - v2"
product-service-1  | npm notice Changelog: https://github.com/npm/cli/releases/tag/v11.10.1
product-service-1  | npm notice To update run: npm install -g npm@11.10.1
product-service-1  | npm notice
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:23.618Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:47:54.069061343Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_provisioning_dashboard_id - v2" duration=3.014833ms
grafana-1          | logger=migrator t=2025-11-07T15:47:54.184325885Z level=info msg="Executing migration" id="create index IDX_dashboard_provisioning_dashboard_id_name - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.189129093Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_provisioning_dashboard_id_name - v2" duration=4.797209ms
grafana-1          | logger=migrator t=2025-11-07T15:47:54.268043343Z level=info msg="Executing migration" id="copy dashboard_provisioning v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.268803093Z level=info msg="Migration successfully executed" id="copy dashboard_provisioning v1 to v2" duration=759.5Âµs
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:23.619Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3000}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:26.633Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:47:54.37372176Z level=info msg="Executing migration" id="drop dashboard_provisioning_tmp_qwerty"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.37444451Z level=info msg="Migration successfully executed" id="drop dashboard_provisioning_tmp_qwerty" duration=720.166Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:54.480138176Z level=info msg="Executing migration" id="Add check_sum column"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.496162385Z level=info msg="Migration successfully executed" id="Add check_sum column" duration=16.021083ms
grafana-1          | logger=migrator t=2025-11-07T15:47:54.550964843Z level=info msg="Executing migration" id="Add index for dashboard_title"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.551663968Z level=info msg="Migration successfully executed" id="Add index for dashboard_title" duration=699.792Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:54.577879926Z level=info msg="Executing migration" id="delete tags for deleted dashboards"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.579031426Z level=info msg="Migration successfully executed" id="delete tags for deleted dashboards" duration=1.152208ms
grafana-1          | logger=migrator t=2025-11-07T15:47:54.659792468Z level=info msg="Executing migration" id="delete stars for deleted dashboards"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.667447218Z level=info msg="Migration successfully executed" id="delete stars for deleted dashboards" duration=7.65225ms
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:26.635Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6500}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:33.147Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:33.148Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":15462}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
grafana-1          | logger=migrator t=2025-11-07T15:47:54.983070343Z level=info msg="Executing migration" id="Add index for dashboard_is_folder"
grafana-1          | logger=migrator t=2025-11-07T15:47:54.989695302Z level=info msg="Migration successfully executed" id="Add index for dashboard_is_folder" duration=6.621375ms
grafana-1          | logger=migrator t=2025-11-07T15:47:55.143783843Z level=info msg="Executing migration" id="Add isPublic for dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:55.272052343Z level=info msg="Migration successfully executed" id="Add isPublic for dashboard" duration=128.26725ms
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 15462,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  override existing env vars with { override: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:39:44.681Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
grafana-1          | logger=migrator t=2025-11-07T15:47:55.579586427Z level=info msg="Executing migration" id="Add deleted for dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:55.60946126Z level=info msg="Migration successfully executed" id="Add deleted for dashboard" duration=29.873041ms
grafana-1          | logger=migrator t=2025-11-07T15:47:55.797743802Z level=info msg="Executing migration" id="Add index for deleted"
grafana-1          | logger=migrator t=2025-11-07T15:47:55.798834635Z level=info msg="Migration successfully executed" id="Add index for deleted" duration=1.099917ms
grafana-1          | logger=migrator t=2025-11-07T15:47:56.153156761Z level=info msg="Executing migration" id="Add column dashboard_uid in dashboard_tag"
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
order-service-1    | }
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:47:56.159098094Z level=info msg="Migration successfully executed" id="Add column dashboard_uid in dashboard_tag" duration=5.937792ms
grafana-1          | logger=migrator t=2025-11-07T15:47:56.356035094Z level=info msg="Executing migration" id="Add column org_id in dashboard_tag"
grafana-1          | logger=migrator t=2025-11-07T15:47:56.362254386Z level=info msg="Migration successfully executed" id="Add column org_id in dashboard_tag" duration=6.216459ms
grafana-1          | logger=migrator t=2025-11-07T15:47:56.681373552Z level=info msg="Executing migration" id="Add missing dashboard_uid and org_id to dashboard_tag"
grafana-1          | logger=migrator t=2025-11-07T15:47:56.688196511Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid and org_id to dashboard_tag" duration=6.838458ms
grafana-1          | logger=migrator t=2025-11-07T15:47:56.889916886Z level=info msg="Executing migration" id="Add apiVersion for dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:47:56.894767844Z level=info msg="Migration successfully executed" id="Add apiVersion for dashboard" duration=4.850416ms
grafana-1          | logger=migrator t=2025-11-07T15:47:56.998425094Z level=info msg="Executing migration" id="Add index for dashboard_uid on dashboard_tag table"
grafana-1          | logger=migrator t=2025-11-07T15:47:57.003425053Z level=info msg="Migration successfully executed" id="Add index for dashboard_uid on dashboard_tag table" duration=4.981916ms
grafana-1          | logger=migrator t=2025-11-07T15:47:57.200254136Z level=info msg="Executing migration" id="Add missing dashboard_uid and org_id to star"
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
grafana-1          | logger=migrator t=2025-11-07T15:47:57.200732886Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid and org_id to star" duration=471.709Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:57.340047344Z level=info msg="Executing migration" id="create data_source table"
grafana-1          | logger=migrator t=2025-11-07T15:47:57.341441886Z level=info msg="Migration successfully executed" id="create data_source table" duration=1.399959ms
grafana-1          | logger=migrator t=2025-11-07T15:47:57.535899928Z level=info msg="Executing migration" id="add index data_source.account_id"
grafana-1          | logger=migrator t=2025-11-07T15:47:57.536613678Z level=info msg="Migration successfully executed" id="add index data_source.account_id" duration=715.041Âµs
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
grafana-1          | logger=migrator t=2025-11-07T15:47:57.731507303Z level=info msg="Executing migration" id="add unique index data_source.account_id_name"
grafana-1          | logger=migrator t=2025-11-07T15:47:57.732621178Z level=info msg="Migration successfully executed" id="add unique index data_source.account_id_name" duration=1.115583ms
grafana-1          | logger=migrator t=2025-11-07T15:47:57.99474772Z level=info msg="Executing migration" id="drop index IDX_data_source_account_id - v1"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | 
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:40:33.854Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.3:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
kafka-1            | java.net.ConnectException: Connection refused
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:47:57.99570472Z level=info msg="Migration successfully executed" id="drop index IDX_data_source_account_id - v1" duration=958.292Âµs
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:47:58.195035262Z level=info msg="Executing migration" id="drop index UQE_data_source_account_id_name - v1"
grafana-1          | logger=migrator t=2025-11-07T15:47:58.19607122Z level=info msg="Migration successfully executed" id="drop index UQE_data_source_account_id_name - v1" duration=1.037167ms
grafana-1          | logger=migrator t=2025-11-07T15:47:58.37839497Z level=info msg="Executing migration" id="Rename table data_source to data_source_v1 - v1"
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:44.769Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:44.769Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":279}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:45.060Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:45.061Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":586}
grafana-1          | logger=migrator t=2025-11-07T15:47:58.38559522Z level=info msg="Migration successfully executed" id="Rename table data_source to data_source_v1 - v1" duration=7.137625ms
grafana-1          | logger=migrator t=2025-11-07T15:47:58.581959512Z level=info msg="Executing migration" id="create data_source table v2"
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:45.658Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:45.659Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":990}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:46.664Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:46.665Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2074}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:48.750Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:48.751Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4324}
grafana-1          | logger=migrator t=2025-11-07T15:47:58.582693678Z level=info msg="Migration successfully executed" id="create data_source table v2" duration=735.458Âµs
grafana-1          | logger=migrator t=2025-11-07T15:47:59.005649137Z level=info msg="Executing migration" id="create index IDX_data_source_org_id - v2"
grafana-1          | logger=migrator t=2025-11-07T15:47:59.006655345Z level=info msg="Migration successfully executed" id="create index IDX_data_source_org_id - v2" duration=1.00675ms
grafana-1          | logger=migrator t=2025-11-07T15:47:59.497426137Z level=info msg="Executing migration" id="create index UQE_data_source_org_id_name - v2"
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:53.084Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:53.085Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7890}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
grafana-1          | logger=migrator t=2025-11-07T15:47:59.499011387Z level=info msg="Migration successfully executed" id="create index UQE_data_source_org_id_name - v2" duration=1.58525ms
grafana-1          | logger=migrator t=2025-11-07T15:47:59.626251929Z level=info msg="Executing migration" id="Drop old table data_source_v1 #2"
grafana-1          | logger=migrator t=2025-11-07T15:47:59.627044554Z level=info msg="Migration successfully executed" id="Drop old table data_source_v1 #2" duration=794.292Âµs
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
grafana-1          | logger=migrator t=2025-11-07T15:47:59.722393179Z level=info msg="Executing migration" id="Add column with_credentials"
grafana-1          | logger=migrator t=2025-11-07T15:47:59.726245721Z level=info msg="Migration successfully executed" id="Add column with_credentials" duration=3.853458ms
grafana-1          | logger=migrator t=2025-11-07T15:47:59.939482346Z level=info msg="Executing migration" id="Add secure json data column"
grafana-1          | logger=migrator t=2025-11-07T15:47:59.945318846Z level=info msg="Migration successfully executed" id="Add secure json data column" duration=5.8345ms
grafana-1          | logger=migrator t=2025-11-07T15:48:00.280733721Z level=info msg="Executing migration" id="Update data_source table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:00.281337513Z level=info msg="Migration successfully executed" id="Update data_source table charset" duration=626.334Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:00.466591138Z level=info msg="Executing migration" id="Update initial version to 1"
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:48:00.467469138Z level=info msg="Migration successfully executed" id="Update initial version to 1" duration=878.916Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:00.581275013Z level=info msg="Executing migration" id="Add read_only data column"
grafana-1          | logger=migrator t=2025-11-07T15:48:00.586720846Z level=info msg="Migration successfully executed" id="Add read_only data column" duration=5.442166ms
grafana-1          | logger=migrator t=2025-11-07T15:48:00.775926179Z level=info msg="Executing migration" id="Migrate logging ds to loki ds"
grafana-1          | logger=migrator t=2025-11-07T15:48:00.776239388Z level=info msg="Migration successfully executed" id="Migrate logging ds to loki ds" duration=315.792Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:00.869383179Z level=info msg="Executing migration" id="Update json_data with nulls"
grafana-1          | logger=migrator t=2025-11-07T15:48:00.869809763Z level=info msg="Migration successfully executed" id="Update json_data with nulls" duration=427.333Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:01.07421393Z level=info msg="Executing migration" id="Add uid column"
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
grafana-1          | logger=migrator t=2025-11-07T15:48:01.077191096Z level=info msg="Migration successfully executed" id="Add uid column" duration=2.977375ms
grafana-1          | logger=migrator t=2025-11-07T15:48:01.242186263Z level=info msg="Executing migration" id="Update uid value"
grafana-1          | logger=migrator t=2025-11-07T15:48:01.242514971Z level=info msg="Migration successfully executed" id="Update uid value" duration=330.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:01.489073096Z level=info msg="Executing migration" id="Add unique index datasource_org_id_uid"
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7890,
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
grafana-1          | logger=migrator t=2025-11-07T15:48:01.49019093Z level=info msg="Migration successfully executed" id="Add unique index datasource_org_id_uid" duration=1.118417ms
grafana-1          | logger=migrator t=2025-11-07T15:48:01.817342388Z level=info msg="Executing migration" id="add unique index datasource_org_id_is_default"
grafana-1          | logger=migrator t=2025-11-07T15:48:01.818447597Z level=info msg="Migration successfully executed" id="add unique index datasource_org_id_is_default" duration=1.106084ms
grafana-1          | logger=migrator t=2025-11-07T15:48:02.051274472Z level=info msg="Executing migration" id="Add is_prunable column"
grafana-1          | logger=migrator t=2025-11-07T15:48:02.057411013Z level=info msg="Migration successfully executed" id="Add is_prunable column" duration=6.134709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:02.355189597Z level=info msg="Executing migration" id="Add api_version column"
grafana-1          | logger=migrator t=2025-11-07T15:48:02.393505389Z level=info msg="Migration successfully executed" id="Add api_version column" duration=38.298458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:02.714242889Z level=info msg="Executing migration" id="Update secure_json_data column to MediumText"
grafana-1          | logger=migrator t=2025-11-07T15:48:02.71435218Z level=info msg="Migration successfully executed" id="Update secure_json_data column to MediumText" duration=110.958Âµs
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7890,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:48:02.955999389Z level=info msg="Executing migration" id="create api_key table"
grafana-1          | logger=migrator t=2025-11-07T15:48:02.956987305Z level=info msg="Migration successfully executed" id="create api_key table" duration=989.708Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:03.268340041Z level=info msg="Executing migration" id="add index api_key.account_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:03.269491583Z level=info msg="Migration successfully executed" id="add index api_key.account_id" duration=1.155417ms
grafana-1          | logger=migrator t=2025-11-07T15:48:03.479359708Z level=info msg="Executing migration" id="add index api_key.key"
grafana-1          | logger=migrator t=2025-11-07T15:48:03.480548625Z level=info msg="Migration successfully executed" id="add index api_key.key" duration=1.190208ms
grafana-1          | logger=migrator t=2025-11-07T15:48:03.803426417Z level=info msg="Executing migration" id="add index api_key.account_id_name"
grafana-1          | logger=migrator t=2025-11-07T15:48:03.804683958Z level=info msg="Migration successfully executed" id="add index api_key.account_id_name" duration=1.256833ms
grafana-1          | logger=migrator t=2025-11-07T15:48:04.045813833Z level=info msg="Executing migration" id="drop index IDX_api_key_account_id - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:04.046411958Z level=info msg="Migration successfully executed" id="drop index IDX_api_key_account_id - v1" duration=599.208Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:04.518954334Z level=info msg="Executing migration" id="drop index UQE_api_key_key - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:04.519875084Z level=info msg="Migration successfully executed" id="drop index UQE_api_key_key - v1" duration=922.708Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:04.971524834Z level=info msg="Executing migration" id="drop index UQE_api_key_account_id_name - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:04.981294917Z level=info msg="Migration successfully executed" id="drop index UQE_api_key_account_id_name - v1" duration=9.75825ms
grafana-1          | logger=migrator t=2025-11-07T15:48:05.171116792Z level=info msg="Executing migration" id="Rename table api_key to api_key_v1 - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:05.175225167Z level=info msg="Migration successfully executed" id="Rename table api_key to api_key_v1 - v1" duration=4.106625ms
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:48:05.368928376Z level=info msg="Executing migration" id="create api_key table v2"
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:33.937Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:33.937Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":284}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:34.239Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:34.240Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":612}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:34.863Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:34.864Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1386}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:36.297Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:36.302Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2254}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:38.567Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:38.568Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5060}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:43.642Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:43.644Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8706}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:40:43.647Z"}
order-service-1    | Server is running on port 3001
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.3:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
grafana-1          | logger=migrator t=2025-11-07T15:48:05.370119917Z level=info msg="Migration successfully executed" id="create api_key table v2" duration=1.191417ms
grafana-1          | logger=migrator t=2025-11-07T15:48:05.459650334Z level=info msg="Executing migration" id="create index IDX_api_key_org_id - v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:05.460847959Z level=info msg="Migration successfully executed" id="create index IDX_api_key_org_id - v2" duration=1.197958ms
grafana-1          | logger=migrator t=2025-11-07T15:48:05.626079834Z level=info msg="Executing migration" id="create index UQE_api_key_key - v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:05.627217751Z level=info msg="Migration successfully executed" id="create index UQE_api_key_key - v2" duration=1.139708ms
grafana-1          | logger=migrator t=2025-11-07T15:48:05.819575251Z level=info msg="Executing migration" id="create index UQE_api_key_org_id_name - v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:05.820301584Z level=info msg="Migration successfully executed" id="create index UQE_api_key_org_id_name - v2" duration=727.583Âµs
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.18.0.7:40036, server: zookeeper/172.18.0.3:2181
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.18.0.3:2181, session id = 0x100005b512f0000, negotiated timeout = 40000
kafka-1            | [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100005b512f0000
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:43.663Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:43.663Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":258}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:43.937Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:43.938Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":448}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:44.396Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:44.397Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":894}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:45.304Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:45.305Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2112}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:47.431Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:47.432Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4312}
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x100005b512f0000 closed
kafka-1            | ===> Launching ... 
kafka-1            | ===> Launching kafka ... 
kafka-1            | [2025-11-07 15:49:25,083] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
kafka-1            | [2025-11-07 15:49:30,238] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
kafka-1            | [2025-11-07 15:49:31,239] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:51.754Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:51.755Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6906}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:39:53.845Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    |   retryTime: 6906,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
kafka-1            | [2025-11-07 15:49:31,335] INFO starting (kafka.server.KafkaServer)
kafka-1            | [2025-11-07 15:49:31,338] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
grafana-1          | logger=migrator t=2025-11-07T15:48:05.988777876Z level=info msg="Executing migration" id="copy api_key v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:05.989423126Z level=info msg="Migration successfully executed" id="copy api_key v1 to v2" duration=645.833Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:06.166093043Z level=info msg="Executing migration" id="Drop old table api_key_v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:06.166847793Z level=info msg="Migration successfully executed" id="Drop old table api_key_v1" duration=756.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:06.396320168Z level=info msg="Executing migration" id="Update api_key table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:06.396362543Z level=info msg="Migration successfully executed" id="Update api_key table charset" duration=44.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:06.639972668Z level=info msg="Executing migration" id="Add expires to api_key table"
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
kafka-1            | [2025-11-07 15:49:31,448] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2025-11-07 15:49:31,470] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:host.name=363a57706629 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:java.version=11.0.13 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-json-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-raft-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/netty-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/connect-transforms-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-shell-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-clients-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/trogdor-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.68.Final.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.68.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.19.3.jar:/usr/bin/../share/java/kafka/kafka-streams-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.68.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/connect-mirror-7.0.1-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
grafana-1          | logger=migrator t=2025-11-07T15:48:06.657690626Z level=info msg="Migration successfully executed" id="Add expires to api_key table" duration=17.718791ms
grafana-1          | logger=migrator t=2025-11-07T15:48:06.913587335Z level=info msg="Executing migration" id="Add service account foreign key"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ” prevent committing .env to code: https://dotenvx.com/precommit
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:40:52.426Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
grafana-1          | logger=migrator t=2025-11-07T15:48:06.917609418Z level=info msg="Migration successfully executed" id="Add service account foreign key" duration=4.023292ms
grafana-1          | logger=migrator t=2025-11-07T15:48:07.137633418Z level=info msg="Executing migration" id="set service account foreign key to nil if 0"
grafana-1          | logger=migrator t=2025-11-07T15:48:07.138038085Z level=info msg="Migration successfully executed" id="set service account foreign key to nil if 0" duration=405.458Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:07.309116627Z level=info msg="Executing migration" id="Add last_used_at to api_key table"
grafana-1          | logger=migrator t=2025-11-07T15:48:07.31238596Z level=info msg="Migration successfully executed" id="Add last_used_at to api_key table" duration=3.269125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:07.56133671Z level=info msg="Executing migration" id="Add is_revoked column to api_key table"
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
grafana-1          | logger=migrator t=2025-11-07T15:48:07.564499293Z level=info msg="Migration successfully executed" id="Add is_revoked column to api_key table" duration=3.174ms
grafana-1          | logger=migrator t=2025-11-07T15:48:07.956646502Z level=info msg="Executing migration" id="create dashboard_snapshot table v4"
grafana-1          | logger=migrator t=2025-11-07T15:48:07.957701127Z level=info msg="Migration successfully executed" id="create dashboard_snapshot table v4" duration=1.056041ms
grafana-1          | logger=migrator t=2025-11-07T15:48:08.180925377Z level=info msg="Executing migration" id="drop table dashboard_snapshot_v4 #1"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.181393877Z level=info msg="Migration successfully executed" id="drop table dashboard_snapshot_v4 #1" duration=469.666Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:08.366072794Z level=info msg="Executing migration" id="create dashboard_snapshot table v5 #2"
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2025-11-07T15:48:08.37440746Z level=info msg="Migration successfully executed" id="create dashboard_snapshot table v5 #2" duration=8.334167ms
grafana-1          | logger=migrator t=2025-11-07T15:48:08.654809627Z level=info msg="Executing migration" id="create index UQE_dashboard_snapshot_key - v5"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.655804627Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_snapshot_key - v5" duration=992.875Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:08.807126877Z level=info msg="Executing migration" id="create index UQE_dashboard_snapshot_delete_key - v5"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.807779836Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_snapshot_delete_key - v5" duration=653.875Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:08.822412919Z level=info msg="Executing migration" id="create index IDX_dashboard_snapshot_user_id - v5"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.823556419Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_snapshot_user_id - v5" duration=1.1445ms
grafana-1          | logger=migrator t=2025-11-07T15:48:08.831991627Z level=info msg="Executing migration" id="alter dashboard_snapshot to mediumtext v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.832025836Z level=info msg="Migration successfully executed" id="alter dashboard_snapshot to mediumtext v2" duration=35.875Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:08.835724336Z level=info msg="Executing migration" id="Update dashboard_snapshot table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.836749419Z level=info msg="Migration successfully executed" id="Update dashboard_snapshot table charset" duration=1.025458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:08.842920336Z level=info msg="Executing migration" id="Add column external_delete_url to dashboard_snapshots table"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.845885294Z level=info msg="Migration successfully executed" id="Add column external_delete_url to dashboard_snapshots table" duration=2.965083ms
grafana-1          | logger=migrator t=2025-11-07T15:48:08.852318502Z level=info msg="Executing migration" id="Add encrypted dashboard json column"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.855621544Z level=info msg="Migration successfully executed" id="Add encrypted dashboard json column" duration=3.30125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:08.861981502Z level=info msg="Executing migration" id="Change dashboard_encrypted column to MEDIUMBLOB"
grafana-1          | logger=migrator t=2025-11-07T15:48:08.863752711Z level=info msg="Migration successfully executed" id="Change dashboard_encrypted column to MEDIUMBLOB" duration=1.772958ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.005306336Z level=info msg="Executing migration" id="create quota table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.029727877Z level=info msg="Migration successfully executed" id="create quota table v1" duration=26.253959ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.067295336Z level=info msg="Executing migration" id="create index UQE_quota_org_id_user_id_target - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.074441086Z level=info msg="Migration successfully executed" id="create index UQE_quota_org_id_user_id_target - v1" duration=7.17025ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.180115377Z level=info msg="Executing migration" id="Update quota table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.181657461Z level=info msg="Migration successfully executed" id="Update quota table charset" duration=1.55125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.233924002Z level=info msg="Executing migration" id="create plugin_setting table"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.241734211Z level=info msg="Migration successfully executed" id="create plugin_setting table" duration=7.748625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.556202378Z level=info msg="Executing migration" id="create index UQE_plugin_setting_org_id_plugin_id - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.562377336Z level=info msg="Migration successfully executed" id="create index UQE_plugin_setting_org_id_plugin_id - v1" duration=4.418042ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.720213669Z level=info msg="Executing migration" id="Add column plugin_version to plugin_settings"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.744485794Z level=info msg="Migration successfully executed" id="Add column plugin_version to plugin_settings" duration=24.267333ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.855299544Z level=info msg="Executing migration" id="Update plugin_setting table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.856674836Z level=info msg="Migration successfully executed" id="Update plugin_setting table charset" duration=1.377208ms
grafana-1          | logger=migrator t=2025-11-07T15:48:09.969699878Z level=info msg="Executing migration" id="update NULL org_id to 1"
grafana-1          | logger=migrator t=2025-11-07T15:48:09.970057753Z level=info msg="Migration successfully executed" id="update NULL org_id to 1" duration=358.791Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:10.081894086Z level=info msg="Executing migration" id="make org_id NOT NULL and DEFAULT VALUE 1"
grafana-1          | logger=migrator t=2025-11-07T15:48:10.116844961Z level=info msg="Migration successfully executed" id="make org_id NOT NULL and DEFAULT VALUE 1" duration=34.953792ms
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:53.941Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:53.941Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":349}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:54.306Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:54.307Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":728}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:55.042Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:55.043Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1208}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:56.260Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:56.261Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2286}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:58.557Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:39:58.557Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3796}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:02.367Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:02.369Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7702}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7702,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7702,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:48:10.525828128Z level=info msg="Executing migration" id="create session table"
grafana-1          | logger=migrator t=2025-11-07T15:48:10.527013253Z level=info msg="Migration successfully executed" id="create session table" duration=1.183042ms
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:40:03.249Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=migrator t=2025-11-07T15:48:10.69687117Z level=info msg="Executing migration" id="Drop old table playlist table"
grafana-1          | logger=migrator t=2025-11-07T15:48:10.704865712Z level=info msg="Migration successfully executed" id="Drop old table playlist table" duration=7.994292ms
grafana-1          | logger=migrator t=2025-11-07T15:48:10.990298337Z level=info msg="Executing migration" id="Drop old table playlist_item table"
grafana-1          | logger=migrator t=2025-11-07T15:48:10.990432212Z level=info msg="Migration successfully executed" id="Drop old table playlist_item table" duration=688.334Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:11.099381128Z level=info msg="Executing migration" id="create playlist table v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:11.100541378Z level=info msg="Migration successfully executed" id="create playlist table v2" duration=1.160833ms
grafana-1          | logger=migrator t=2025-11-07T15:48:11.383898045Z level=info msg="Executing migration" id="create playlist item table v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:11.384576795Z level=info msg="Migration successfully executed" id="create playlist item table v2" duration=679.833Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:11.524464962Z level=info msg="Executing migration" id="Update playlist table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:11.524543379Z level=info msg="Migration successfully executed" id="Update playlist table charset" duration=79.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:11.643283795Z level=info msg="Executing migration" id="Update playlist_item table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:11.643335212Z level=info msg="Migration successfully executed" id="Update playlist_item table charset" duration=54.041Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:11.816459462Z level=info msg="Executing migration" id="Add playlist column created_at"
grafana-1          | logger=migrator t=2025-11-07T15:48:11.820396212Z level=info msg="Migration successfully executed" id="Add playlist column created_at" duration=3.936417ms
grafana-1          | logger=migrator t=2025-11-07T15:48:11.943042004Z level=info msg="Executing migration" id="Add playlist column updated_at"
grafana-1          | logger=migrator t=2025-11-07T15:48:11.950550212Z level=info msg="Migration successfully executed" id="Add playlist column updated_at" duration=7.508375ms
grafana-1          | logger=migrator t=2025-11-07T15:48:12.254805296Z level=info msg="Executing migration" id="drop preferences table v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:12.255071171Z level=info msg="Migration successfully executed" id="drop preferences table v2" duration=268.083Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:12.434592837Z level=info msg="Executing migration" id="drop preferences table v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:12.434822921Z level=info msg="Migration successfully executed" id="drop preferences table v3" duration=232.292Âµs
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2025-11-07T15:48:12.537861296Z level=info msg="Executing migration" id="create preferences table v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:12.543088379Z level=info msg="Migration successfully executed" id="create preferences table v3" duration=5.227875ms
grafana-1          | logger=migrator t=2025-11-07T15:48:12.847009963Z level=info msg="Executing migration" id="Update preferences table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:12.847061629Z level=info msg="Migration successfully executed" id="Update preferences table charset" duration=54.209Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:13.015454421Z level=info msg="Executing migration" id="Add column team_id in preferences"
grafana-1          | logger=migrator t=2025-11-07T15:48:13.019345254Z level=info msg="Migration successfully executed" id="Add column team_id in preferences" duration=3.890542ms
grafana-1          | logger=migrator t=2025-11-07T15:48:13.300885796Z level=info msg="Executing migration" id="Update team_id column values in preferences"
grafana-1          | logger=migrator t=2025-11-07T15:48:13.301320171Z level=info msg="Migration successfully executed" id="Update team_id column values in preferences" duration=435.208Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:13.556431546Z level=info msg="Executing migration" id="Add column week_start in preferences"
grafana-1          | logger=migrator t=2025-11-07T15:48:13.560513338Z level=info msg="Migration successfully executed" id="Add column week_start in preferences" duration=4.08025ms
grafana-1          | logger=migrator t=2025-11-07T15:48:13.786248671Z level=info msg="Executing migration" id="Add column preferences.json_data"
grafana-1          | logger=migrator t=2025-11-07T15:48:13.788206463Z level=info msg="Migration successfully executed" id="Add column preferences.json_data" duration=1.958333ms
grafana-1          | logger=migrator t=2025-11-07T15:48:14.022006213Z level=info msg="Executing migration" id="alter preferences.json_data to mediumtext v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:14.022045546Z level=info msg="Migration successfully executed" id="alter preferences.json_data to mediumtext v1" duration=41.833Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:14.417364088Z level=info msg="Executing migration" id="Add preferences index org_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:14.42231938Z level=info msg="Migration successfully executed" id="Add preferences index org_id" duration=4.959167ms
grafana-1          | logger=migrator t=2025-11-07T15:48:14.697700047Z level=info msg="Executing migration" id="Add preferences index user_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:14.70061313Z level=info msg="Migration successfully executed" id="Add preferences index user_id" duration=2.928709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:14.974977214Z level=info msg="Executing migration" id="Add home_dashboard_uid column to preferences table"
grafana-1          | logger=migrator t=2025-11-07T15:48:14.98050938Z level=info msg="Migration successfully executed" id="Add home_dashboard_uid column to preferences table" duration=5.533292ms
grafana-1          | logger=migrator t=2025-11-07T15:48:15.268143172Z level=info msg="Executing migration" id="Add missing dashboard_uid to preferences table"
grafana-1          | logger=migrator t=2025-11-07T15:48:15.269131047Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid to preferences table" duration=990.083Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:15.521152672Z level=info msg="Executing migration" id="create alert table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:15.525741089Z level=info msg="Migration successfully executed" id="create alert table v1" duration=4.587917ms
grafana-1          | logger=migrator t=2025-11-07T15:48:15.784378256Z level=info msg="Executing migration" id="add index alert org_id & id "
grafana-1          | logger=migrator t=2025-11-07T15:48:15.785104964Z level=info msg="Migration successfully executed" id="add index alert org_id & id " duration=728.917Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:16.079981631Z level=info msg="Executing migration" id="add index alert state"
grafana-1          | logger=migrator t=2025-11-07T15:48:16.081013756Z level=info msg="Migration successfully executed" id="add index alert state" duration=1.051041ms
grafana-1          | logger=migrator t=2025-11-07T15:48:16.318031423Z level=info msg="Executing migration" id="add index alert dashboard_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:16.319212923Z level=info msg="Migration successfully executed" id="add index alert dashboard_id" duration=1.180834ms
grafana-1          | logger=migrator t=2025-11-07T15:48:16.579442214Z level=info msg="Executing migration" id="Create alert_rule_tag table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:16.580379131Z level=info msg="Migration successfully executed" id="Create alert_rule_tag table v1" duration=938.084Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:16.805500631Z level=info msg="Executing migration" id="Add unique index alert_rule_tag.alert_id_tag_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:16.816725464Z level=info msg="Migration successfully executed" id="Add unique index alert_rule_tag.alert_id_tag_id" duration=11.2195ms
grafana-1          | logger=migrator t=2025-11-07T15:48:16.922493464Z level=info msg="Executing migration" id="drop index UQE_alert_rule_tag_alert_id_tag_id - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:16.923831173Z level=info msg="Migration successfully executed" id="drop index UQE_alert_rule_tag_alert_id_tag_id - v1" duration=1.338625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:17.045715631Z level=info msg="Executing migration" id="Rename table alert_rule_tag to alert_rule_tag_v1 - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:17.062799548Z level=info msg="Migration successfully executed" id="Rename table alert_rule_tag to alert_rule_tag_v1 - v1" duration=17.081208ms
grafana-1          | logger=migrator t=2025-11-07T15:48:17.250085381Z level=info msg="Executing migration" id="Create alert_rule_tag table v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:17.253159423Z level=info msg="Migration successfully executed" id="Create alert_rule_tag table v2" duration=3.073458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:17.380716923Z level=info msg="Executing migration" id="create index UQE_alert_rule_tag_alert_id_tag_id - Add unique index alert_rule_tag.alert_id_tag_id V2"
grafana-1          | logger=migrator t=2025-11-07T15:48:17.384130673Z level=info msg="Migration successfully executed" id="create index UQE_alert_rule_tag_alert_id_tag_id - Add unique index alert_rule_tag.alert_id_tag_id V2" duration=3.410458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:17.65928709Z level=info msg="Executing migration" id="copy alert_rule_tag v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:17.659890381Z level=info msg="Migration successfully executed" id="copy alert_rule_tag v1 to v2" duration=605Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:17.803248048Z level=info msg="Executing migration" id="drop table alert_rule_tag_v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:17.80419484Z level=info msg="Migration successfully executed" id="drop table alert_rule_tag_v1" duration=948.666Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:18.246396673Z level=info msg="Executing migration" id="create alert_notification table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:18.247449798Z level=info msg="Migration successfully executed" id="create alert_notification table v1" duration=1.054583ms
grafana-1          | logger=migrator t=2025-11-07T15:48:18.687858507Z level=info msg="Executing migration" id="Add column is_default"
grafana-1          | logger=migrator t=2025-11-07T15:48:18.69263684Z level=info msg="Migration successfully executed" id="Add column is_default" duration=4.778416ms
grafana-1          | logger=migrator t=2025-11-07T15:48:18.880057674Z level=info msg="Executing migration" id="Add column frequency"
grafana-1          | logger=migrator t=2025-11-07T15:48:18.89286934Z level=info msg="Migration successfully executed" id="Add column frequency" duration=12.812833ms
grafana-1          | logger=migrator t=2025-11-07T15:48:19.055587882Z level=info msg="Executing migration" id="Add column send_reminder"
grafana-1          | logger=migrator t=2025-11-07T15:48:19.061758799Z level=info msg="Migration successfully executed" id="Add column send_reminder" duration=6.16625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:19.078046507Z level=info msg="Executing migration" id="Add column disable_resolve_message"
grafana-1          | logger=migrator t=2025-11-07T15:48:19.085655799Z level=info msg="Migration successfully executed" id="Add column disable_resolve_message" duration=7.603167ms
grafana-1          | logger=migrator t=2025-11-07T15:48:19.115056466Z level=info msg="Executing migration" id="add index alert_notification org_id & name"
grafana-1          | logger=migrator t=2025-11-07T15:48:19.307869757Z level=info msg="Migration successfully executed" id="add index alert_notification org_id & name" duration=192.805917ms
grafana-1          | logger=migrator t=2025-11-07T15:48:19.551393424Z level=info msg="Executing migration" id="Update alert table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:19.551444299Z level=info msg="Migration successfully executed" id="Update alert table charset" duration=56.708Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:19.866879924Z level=info msg="Executing migration" id="Update alert_notification table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:19.866944008Z level=info msg="Migration successfully executed" id="Update alert_notification table charset" duration=65.959Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:20.221557216Z level=info msg="Executing migration" id="create notification_journal table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:20.223183508Z level=info msg="Migration successfully executed" id="create notification_journal table v1" duration=1.625667ms
grafana-1          | logger=migrator t=2025-11-07T15:48:20.565393841Z level=info msg="Executing migration" id="add index notification_journal org_id & alert_id & notifier_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:20.569439633Z level=info msg="Migration successfully executed" id="add index notification_journal org_id & alert_id & notifier_id" duration=4.046875ms
grafana-1          | logger=migrator t=2025-11-07T15:48:21.02926405Z level=info msg="Executing migration" id="drop alert_notification_journal"
grafana-1          | logger=migrator t=2025-11-07T15:48:21.03022705Z level=info msg="Migration successfully executed" id="drop alert_notification_journal" duration=963.125Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:21.5182443Z level=info msg="Executing migration" id="create alert_notification_state table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:21.526926008Z level=info msg="Migration successfully executed" id="create alert_notification_state table v1" duration=8.678291ms
grafana-1          | logger=migrator t=2025-11-07T15:48:21.847289592Z level=info msg="Executing migration" id="add index alert_notification_state org_id & alert_id & notifier_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:21.848804883Z level=info msg="Migration successfully executed" id="add index alert_notification_state org_id & alert_id & notifier_id" duration=1.5155ms
grafana-1          | logger=migrator t=2025-11-07T15:48:22.252988092Z level=info msg="Executing migration" id="Add for to alert table"
grafana-1          | logger=migrator t=2025-11-07T15:48:22.258009759Z level=info msg="Migration successfully executed" id="Add for to alert table" duration=5.0215ms
grafana-1          | logger=migrator t=2025-11-07T15:48:22.568862092Z level=info msg="Executing migration" id="Add column uid in alert_notification"
grafana-1          | logger=migrator t=2025-11-07T15:48:22.571170467Z level=info msg="Migration successfully executed" id="Add column uid in alert_notification" duration=2.306458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:22.681267551Z level=info msg="Executing migration" id="Update uid column values in alert_notification"
grafana-1          | logger=migrator t=2025-11-07T15:48:22.687461176Z level=info msg="Migration successfully executed" id="Update uid column values in alert_notification" duration=6.208458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:22.702838676Z level=info msg="Executing migration" id="Add unique index alert_notification_org_id_uid"
grafana-1          | logger=migrator t=2025-11-07T15:48:22.707185301Z level=info msg="Migration successfully executed" id="Add unique index alert_notification_org_id_uid" duration=4.311666ms
grafana-1          | logger=migrator t=2025-11-07T15:48:22.719154551Z level=info msg="Executing migration" id="Remove unique index org_id_name"
grafana-1          | logger=migrator t=2025-11-07T15:48:22.725488259Z level=info msg="Migration successfully executed" id="Remove unique index org_id_name" duration=6.332625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:22.747146384Z level=info msg="Executing migration" id="Add column secure_settings in alert_notification"
grafana-1          | logger=migrator t=2025-11-07T15:48:22.771045676Z level=info msg="Migration successfully executed" id="Add column secure_settings in alert_notification" duration=24.059167ms
grafana-1          | logger=migrator t=2025-11-07T15:48:23.222996676Z level=info msg="Executing migration" id="alter alert.settings to mediumtext"
grafana-1          | logger=migrator t=2025-11-07T15:48:23.223035967Z level=info msg="Migration successfully executed" id="alter alert.settings to mediumtext" duration=40.833Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:23.538114593Z level=info msg="Executing migration" id="Add non-unique index alert_notification_state_alert_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:23.539368968Z level=info msg="Migration successfully executed" id="Add non-unique index alert_notification_state_alert_id" duration=1.25475ms
grafana-1          | logger=migrator t=2025-11-07T15:48:23.764256968Z level=info msg="Executing migration" id="Add non-unique index alert_rule_tag_alert_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:23.765471093Z level=info msg="Migration successfully executed" id="Add non-unique index alert_rule_tag_alert_id" duration=1.209375ms
grafana-1          | logger=migrator t=2025-11-07T15:48:24.01699451Z level=info msg="Executing migration" id="Drop old annotation table v4"
grafana-1          | logger=migrator t=2025-11-07T15:48:24.01712901Z level=info msg="Migration successfully executed" id="Drop old annotation table v4" duration=137.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:24.479825176Z level=info msg="Executing migration" id="create annotation table v5"
grafana-1          | logger=migrator t=2025-11-07T15:48:24.481485801Z level=info msg="Migration successfully executed" id="create annotation table v5" duration=1.660208ms
grafana-1          | logger=migrator t=2025-11-07T15:48:24.558449635Z level=info msg="Executing migration" id="add index annotation 0 v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:24.560584843Z level=info msg="Migration successfully executed" id="add index annotation 0 v3" duration=2.144458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:25.031528677Z level=info msg="Executing migration" id="add index annotation 1 v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:25.032917427Z level=info msg="Migration successfully executed" id="add index annotation 1 v3" duration=1.391042ms
grafana-1          | logger=migrator t=2025-11-07T15:48:25.155638635Z level=info msg="Executing migration" id="add index annotation 2 v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:25.158129385Z level=info msg="Migration successfully executed" id="add index annotation 2 v3" duration=2.490958ms
grafana-1          | logger=migrator t=2025-11-07T15:48:25.21796801Z level=info msg="Executing migration" id="add index annotation 3 v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:25.221771135Z level=info msg="Migration successfully executed" id="add index annotation 3 v3" duration=3.802083ms
grafana-1          | logger=migrator t=2025-11-07T15:48:25.323106177Z level=info msg="Executing migration" id="add index annotation 4 v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:25.323972177Z level=info msg="Migration successfully executed" id="add index annotation 4 v3" duration=867.084Âµs
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:48:25.513833594Z level=info msg="Executing migration" id="Update annotation table charset"
grafana-1          | logger=migrator t=2025-11-07T15:48:25.513888927Z level=info msg="Migration successfully executed" id="Update annotation table charset" duration=58.416Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:25.638817427Z level=info msg="Executing migration" id="Add column region_id to annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:25.644123719Z level=info msg="Migration successfully executed" id="Add column region_id to annotation table" duration=5.304625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:26.008609177Z level=info msg="Executing migration" id="Drop category_id index"
grafana-1          | logger=migrator t=2025-11-07T15:48:26.009296885Z level=info msg="Migration successfully executed" id="Drop category_id index" duration=688.875Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:26.195226302Z level=info msg="Executing migration" id="Add column tags to annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:26.200614136Z level=info msg="Migration successfully executed" id="Add column tags to annotation table" duration=5.367792ms
grafana-1          | logger=migrator t=2025-11-07T15:48:26.397145136Z level=info msg="Executing migration" id="Create annotation_tag table v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:26.398210761Z level=info msg="Migration successfully executed" id="Create annotation_tag table v2" duration=1.066833ms
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:48:26.562491469Z level=info msg="Executing migration" id="Add unique index annotation_tag.annotation_id_tag_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:26.563678594Z level=info msg="Migration successfully executed" id="Add unique index annotation_tag.annotation_id_tag_id" duration=1.188208ms
grafana-1          | logger=migrator t=2025-11-07T15:48:26.770686428Z level=info msg="Executing migration" id="drop index UQE_annotation_tag_annotation_id_tag_id - v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:26.773092303Z level=info msg="Migration successfully executed" id="drop index UQE_annotation_tag_annotation_id_tag_id - v2" duration=2.405958ms
grafana-1          | logger=migrator t=2025-11-07T15:48:27.106138928Z level=info msg="Executing migration" id="Rename table annotation_tag to annotation_tag_v2 - v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:27.169739428Z level=info msg="Migration successfully executed" id="Rename table annotation_tag to annotation_tag_v2 - v2" duration=63.57925ms
grafana-1          | logger=migrator t=2025-11-07T15:48:27.60934397Z level=info msg="Executing migration" id="Create annotation_tag table v3"
grafana-1          | logger=migrator t=2025-11-07T15:48:27.610284011Z level=info msg="Migration successfully executed" id="Create annotation_tag table v3" duration=941.333Âµs
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
grafana-1          | logger=migrator t=2025-11-07T15:48:28.064817595Z level=info msg="Executing migration" id="create index UQE_annotation_tag_annotation_id_tag_id - Add unique index annotation_tag.annotation_id_tag_id V3"
grafana-1          | logger=migrator t=2025-11-07T15:48:28.065645511Z level=info msg="Migration successfully executed" id="create index UQE_annotation_tag_annotation_id_tag_id - Add unique index annotation_tag.annotation_id_tag_id V3" duration=828.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:28.398840178Z level=info msg="Executing migration" id="copy annotation_tag v2 to v3"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2025-11-07T15:48:28.399289637Z level=info msg="Migration successfully executed" id="copy annotation_tag v2 to v3" duration=451.208Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:28.816855637Z level=info msg="Executing migration" id="drop table annotation_tag_v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:28.869635845Z level=info msg="Migration successfully executed" id="drop table annotation_tag_v2" duration=52.780625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:29.021411887Z level=info msg="Executing migration" id="Update alert annotations and set TEXT to empty"
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:03.332Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:03.332Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":264}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:03.615Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:03.616Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":612}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:04.236Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:04.236Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1274}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:05.524Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:05.526Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2720}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:08.258Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:08.259Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5508}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:13.775Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:13.776Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10060}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
grafana-1          | logger=migrator t=2025-11-07T15:48:29.025312554Z level=info msg="Migration successfully executed" id="Update alert annotations and set TEXT to empty" duration=3.901416ms
grafana-1          | logger=migrator t=2025-11-07T15:48:29.318732345Z level=info msg="Executing migration" id="Add created time to annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:29.330553845Z level=info msg="Migration successfully executed" id="Add created time to annotation table" duration=11.819542ms
grafana-1          | logger=migrator t=2025-11-07T15:48:29.711465221Z level=info msg="Executing migration" id="Add updated time to annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:29.720841596Z level=info msg="Migration successfully executed" id="Add updated time to annotation table" duration=9.376542ms
grafana-1          | logger=migrator t=2025-11-07T15:48:30.010152137Z level=info msg="Executing migration" id="Add index for created in annotation table"
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
grafana-1          | logger=migrator t=2025-11-07T15:48:30.011553096Z level=info msg="Migration successfully executed" id="Add index for created in annotation table" duration=1.402125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:30.238772637Z level=info msg="Executing migration" id="Add index for updated in annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:30.239991971Z level=info msg="Migration successfully executed" id="Add index for updated in annotation table" duration=1.220458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:30.436812429Z level=info msg="Executing migration" id="Convert existing annotations from seconds to milliseconds"
grafana-1          | logger=migrator t=2025-11-07T15:48:30.437469554Z level=info msg="Migration successfully executed" id="Convert existing annotations from seconds to milliseconds" duration=658.75Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:30.603289138Z level=info msg="Executing migration" id="Add epoch_end column"
grafana-1          | logger=migrator t=2025-11-07T15:48:30.614358138Z level=info msg="Migration successfully executed" id="Add epoch_end column" duration=11.064959ms
product-service-1  |   retryTime: 10060,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:52.529Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:52.529Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":356}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:52.895Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:52.897Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":852}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:53.761Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:53.762Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1460}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:55.236Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:48:30.953434138Z level=info msg="Executing migration" id="Add index for epoch_end"
grafana-1          | logger=migrator t=2025-11-07T15:48:30.954570304Z level=info msg="Migration successfully executed" id="Add index for epoch_end" duration=1.140709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:31.426248638Z level=info msg="Executing migration" id="Make epoch_end the same as epoch"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:55.238Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2506}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:57.754Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:40:57.755Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4244}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.011Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.013Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9646}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:41:02.015Z"}
grafana-1          | logger=migrator t=2025-11-07T15:48:31.426489846Z level=info msg="Migration successfully executed" id="Make epoch_end the same as epoch" duration=242.458Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:31.706707388Z level=info msg="Executing migration" id="Move region to single row"
grafana-1          | logger=migrator t=2025-11-07T15:48:31.70737818Z level=info msg="Migration successfully executed" id="Move region to single row" duration=672Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:31.993494222Z level=info msg="Executing migration" id="Remove index org_id_epoch from annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:31.994637763Z level=info msg="Migration successfully executed" id="Remove index org_id_epoch from annotation table" duration=1.144083ms
grafana-1          | logger=migrator t=2025-11-07T15:48:32.227775555Z level=info msg="Executing migration" id="Remove index org_id_dashboard_id_panel_id_epoch from annotation table"
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.031Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.031Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":319}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.360Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.361Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":690}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:03.057Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:03.057Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1498}
grafana-1          | logger=migrator t=2025-11-07T15:48:32.228853555Z level=info msg="Migration successfully executed" id="Remove index org_id_dashboard_id_panel_id_epoch from annotation table" duration=1.078709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:32.541697014Z level=info msg="Executing migration" id="Add index for org_id_dashboard_id_epoch_end_epoch on annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:32.54285943Z level=info msg="Migration successfully executed" id="Add index for org_id_dashboard_id_epoch_end_epoch on annotation table" duration=1.162791ms
grafana-1          | logger=migrator t=2025-11-07T15:48:32.624344472Z level=info msg="Executing migration" id="Add index for org_id_epoch_end_epoch on annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:32.62501718Z level=info msg="Migration successfully executed" id="Add index for org_id_epoch_end_epoch on annotation table" duration=671.583Âµs
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:04.569Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:04.570Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2876}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.467Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.468Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4910}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:41:08 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:41:08.543Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:12.390Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:12.391Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11686}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
grafana-1          | logger=migrator t=2025-11-07T15:48:33.060343097Z level=info msg="Executing migration" id="Remove index org_id_epoch_epoch_end from annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:33.061974514Z level=info msg="Migration successfully executed" id="Remove index org_id_epoch_epoch_end from annotation table" duration=1.629875ms
grafana-1          | logger=migrator t=2025-11-07T15:48:33.217816375Z level=info msg="Executing migration" id="Add index for alert_id on annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:33.225533166Z level=info msg="Migration successfully executed" id="Add index for alert_id on annotation table" duration=7.717958ms
grafana-1          | logger=migrator t=2025-11-07T15:48:33.413682625Z level=info msg="Executing migration" id="Increase tags column to length 4096"
grafana-1          | logger=migrator t=2025-11-07T15:48:33.413719291Z level=info msg="Migration successfully executed" id="Increase tags column to length 4096" duration=51.291Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:33.587505083Z level=info msg="Executing migration" id="Increase prev_state column to length 40 not null"
grafana-1          | logger=migrator t=2025-11-07T15:48:33.587525875Z level=info msg="Migration successfully executed" id="Increase prev_state column to length 40 not null" duration=22.417Âµs
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
grafana-1          | logger=migrator t=2025-11-07T15:48:33.766883125Z level=info msg="Executing migration" id="Increase new_state column to length 40 not null"
grafana-1          | logger=migrator t=2025-11-07T15:48:33.7669085Z level=info msg="Migration successfully executed" id="Increase new_state column to length 40 not null" duration=27.375Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:34.036352958Z level=info msg="Executing migration" id="Add dashboard_uid column to annotation table"
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
grafana-1          | logger=migrator t=2025-11-07T15:48:34.041490417Z level=info msg="Migration successfully executed" id="Add dashboard_uid column to annotation table" duration=5.136833ms
grafana-1          | logger=migrator t=2025-11-07T15:48:34.257342542Z level=info msg="Executing migration" id="Add missing dashboard_uid to annotation table"
grafana-1          | logger=migrator t=2025-11-07T15:48:34.257893458Z level=info msg="Migration successfully executed" id="Add missing dashboard_uid to annotation table" duration=552.75Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:34.492021708Z level=info msg="Executing migration" id="create test_data table"
grafana-1          | logger=migrator t=2025-11-07T15:48:34.493109875Z level=info msg="Migration successfully executed" id="create test_data table" duration=1.06675ms
grafana-1          | logger=migrator t=2025-11-07T15:48:34.729480375Z level=info msg="Executing migration" id="create dashboard_version table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:34.730569709Z level=info msg="Migration successfully executed" id="create dashboard_version table v1" duration=1.090583ms
grafana-1          | logger=migrator t=2025-11-07T15:48:34.94205775Z level=info msg="Executing migration" id="add index dashboard_version.dashboard_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:34.942719Z level=info msg="Migration successfully executed" id="add index dashboard_version.dashboard_id" duration=662.167Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:35.220677751Z level=info msg="Executing migration" id="add unique index dashboard_version.dashboard_id and dashboard_version.version"
grafana-1          | logger=migrator t=2025-11-07T15:48:35.224575626Z level=info msg="Migration successfully executed" id="add unique index dashboard_version.dashboard_id and dashboard_version.version" duration=3.897875ms
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 11686,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:48:35.552876167Z level=info msg="Executing migration" id="Set dashboard version to 1 where 0"
grafana-1          | logger=migrator t=2025-11-07T15:48:35.553122167Z level=info msg="Migration successfully executed" id="Set dashboard version to 1 where 0" duration=246.959Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:35.671231751Z level=info msg="Executing migration" id="save existing dashboard data in dashboard_version table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:35.672000376Z level=info msg="Migration successfully executed" id="save existing dashboard data in dashboard_version table v1" duration=769.791Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:35.852326251Z level=info msg="Executing migration" id="alter dashboard_version.data to mediumtext v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:35.852379042Z level=info msg="Migration successfully executed" id="alter dashboard_version.data to mediumtext v1" duration=55.209Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:36.125956001Z level=info msg="Executing migration" id="Add apiVersion for dashboard_version"
grafana-1          | logger=migrator t=2025-11-07T15:48:36.140753834Z level=info msg="Migration successfully executed" id="Add apiVersion for dashboard_version" duration=14.795958ms
grafana-1          | logger=migrator t=2025-11-07T15:48:36.261679043Z level=info msg="Executing migration" id="create team table"
grafana-1          | logger=migrator t=2025-11-07T15:48:36.262285418Z level=info msg="Migration successfully executed" id="create team table" duration=607.209Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:36.593072418Z level=info msg="Executing migration" id="add index team.org_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:36.593761335Z level=info msg="Migration successfully executed" id="add index team.org_id" duration=690.167Âµs
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10060,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
grafana-1          | logger=migrator t=2025-11-07T15:48:36.838880626Z level=info msg="Executing migration" id="add unique index team_org_id_name"
grafana-1          | logger=migrator t=2025-11-07T15:48:36.843727918Z level=info msg="Migration successfully executed" id="add unique index team_org_id_name" duration=4.833709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:37.155944335Z level=info msg="Executing migration" id="Add column uid in team"
grafana-1          | logger=migrator t=2025-11-07T15:48:37.162335335Z level=info msg="Migration successfully executed" id="Add column uid in team" duration=6.38925ms
grafana-1          | logger=migrator t=2025-11-07T15:48:37.53261196Z level=info msg="Executing migration" id="Update uid column values in team"
grafana-1          | logger=migrator t=2025-11-07T15:48:37.532923043Z level=info msg="Migration successfully executed" id="Update uid column values in team" duration=313.083Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:37.669203002Z level=info msg="Executing migration" id="Add unique index team_org_id_uid"
grafana-1          | logger=migrator t=2025-11-07T15:48:37.670308668Z level=info msg="Migration successfully executed" id="Add unique index team_org_id_uid" duration=1.107292ms
grafana-1          | logger=migrator t=2025-11-07T15:48:38.020849169Z level=info msg="Executing migration" id="Add column external_uid in team"
grafana-1          | logger=migrator t=2025-11-07T15:48:38.028547294Z level=info msg="Migration successfully executed" id="Add column external_uid in team" duration=7.68ms
grafana-1          | logger=migrator t=2025-11-07T15:48:38.143385377Z level=info msg="Executing migration" id="Add column is_provisioned in team"
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  enable debug logging with { debug: true }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:41:13.034Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
grafana-1          | logger=migrator t=2025-11-07T15:48:38.151968919Z level=info msg="Migration successfully executed" id="Add column is_provisioned in team" duration=8.582ms
grafana-1          | logger=migrator t=2025-11-07T15:48:38.529443419Z level=info msg="Executing migration" id="create team member table"
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:40:14.326Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:48:38.530104627Z level=info msg="Migration successfully executed" id="create team member table" duration=661.75Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:39.038054836Z level=info msg="Executing migration" id="add index team_member.org_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:39.038806377Z level=info msg="Migration successfully executed" id="add index team_member.org_id" duration=752.458Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:39.299985711Z level=info msg="Executing migration" id="add unique index team_member_org_id_team_id_user_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:39.300660086Z level=info msg="Migration successfully executed" id="add unique index team_member_org_id_team_id_user_id" duration=675.333Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:39.605519753Z level=info msg="Executing migration" id="add index team_member.team_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:39.609202044Z level=info msg="Migration successfully executed" id="add index team_member.team_id" duration=3.682ms
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:13.113Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:48:39.947293169Z level=info msg="Executing migration" id="Add column email to team table"
grafana-1          | logger=migrator t=2025-11-07T15:48:39.955114044Z level=info msg="Migration successfully executed" id="Add column email to team table" duration=7.820125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:40.180265795Z level=info msg="Executing migration" id="Add column external to team_member table"
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2025-11-07T15:48:40.235604253Z level=info msg="Migration successfully executed" id="Add column external to team_member table" duration=55.335458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:40.43914117Z level=info msg="Executing migration" id="Add column permission to team_member table"
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:13.114Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":330}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:13.460Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:13.462Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":746}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:14.224Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:14.226Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1356}
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:15.595Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:15.598Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2878}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.415Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.416Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":259}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.684Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:14.685Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":548}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:15.249Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:15.250Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":878}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:18.488Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:18.489Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6754}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:25.259Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:48:40.444702336Z level=info msg="Migration successfully executed" id="Add column permission to team_member table" duration=5.561166ms
grafana-1          | logger=migrator t=2025-11-07T15:48:40.615593795Z level=info msg="Executing migration" id="add unique index team_member_user_id_org_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:40.61734492Z level=info msg="Migration successfully executed" id="add unique index team_member_user_id_org_id" duration=1.756125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:40.990809003Z level=info msg="Executing migration" id="create dashboard acl table"
grafana-1          | logger=migrator t=2025-11-07T15:48:40.992009253Z level=info msg="Migration successfully executed" id="create dashboard acl table" duration=1.200959ms
grafana-1          | logger=migrator t=2025-11-07T15:48:41.26526192Z level=info msg="Executing migration" id="add index dashboard_acl_dashboard_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:41.26645492Z level=info msg="Migration successfully executed" id="add index dashboard_acl_dashboard_id" duration=1.211375ms
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:25.261Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":15522}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:41:25.264Z"}
order-service-1    | Server is running on port 3001
grafana-1          | logger=migrator t=2025-11-07T15:48:41.438303337Z level=info msg="Executing migration" id="add unique index dashboard_acl_dashboard_id_user_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:41.455785212Z level=info msg="Migration successfully executed" id="add unique index dashboard_acl_dashboard_id_user_id" duration=17.482583ms
grafana-1          | logger=migrator t=2025-11-07T15:48:41.492807754Z level=info msg="Executing migration" id="add unique index dashboard_acl_dashboard_id_team_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:41.545074212Z level=info msg="Migration successfully executed" id="add unique index dashboard_acl_dashboard_id_team_id" duration=52.270042ms
grafana-1          | logger=migrator t=2025-11-07T15:48:41.704243504Z level=info msg="Executing migration" id="add index dashboard_acl_user_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:41.705136337Z level=info msg="Migration successfully executed" id="add index dashboard_acl_user_id" duration=893.583Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:42.024999337Z level=info msg="Executing migration" id="add index dashboard_acl_team_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:42.027000587Z level=info msg="Migration successfully executed" id="add index dashboard_acl_team_id" duration=2.004ms
grafana-1          | logger=migrator t=2025-11-07T15:48:42.065808504Z level=info msg="Executing migration" id="add index dashboard_acl_org_id_role"
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:16.136Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:16.137Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1958}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:18.110Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:18.111Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3928}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:22.055Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:22.056Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6614}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6614,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:25.279Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:25.279Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":347}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:25.635Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:25.636Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":638}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:26.282Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:26.284Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1438}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:27.735Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:27.736Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2542}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:30.293Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:30.295Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5016}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:35.328Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:35.329Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11668}
grafana-1          | logger=migrator t=2025-11-07T15:48:42.066515045Z level=info msg="Migration successfully executed" id="add index dashboard_acl_org_id_role" duration=707.042Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:42.181722545Z level=info msg="Executing migration" id="add index dashboard_permission"
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
grafana-1          | logger=migrator t=2025-11-07T15:48:42.187132546Z level=info msg="Migration successfully executed" id="add index dashboard_permission" duration=6.70525ms
grafana-1          | logger=migrator t=2025-11-07T15:48:42.245003629Z level=info msg="Executing migration" id="save default acl rules in dashboard_acl table"
grafana-1          | logger=migrator t=2025-11-07T15:48:42.249942962Z level=info msg="Migration successfully executed" id="save default acl rules in dashboard_acl table" duration=4.939375ms
grafana-1          | logger=migrator t=2025-11-07T15:48:42.511467171Z level=info msg="Executing migration" id="delete acl rules for deleted dashboards and folders"
grafana-1          | logger=migrator t=2025-11-07T15:48:42.512530879Z level=info msg="Migration successfully executed" id="delete acl rules for deleted dashboards and folders" duration=1.062083ms
grafana-1          | logger=migrator t=2025-11-07T15:48:42.570210546Z level=info msg="Executing migration" id="create tag table"
grafana-1          | logger=migrator t=2025-11-07T15:48:42.570957129Z level=info msg="Migration successfully executed" id="create tag table" duration=748.084Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:42.689580546Z level=info msg="Executing migration" id="add index tag.key_value"
grafana-1          | logger=migrator t=2025-11-07T15:48:42.714638962Z level=info msg="Migration successfully executed" id="add index tag.key_value" duration=4.781084ms
grafana-1          | logger=migrator t=2025-11-07T15:48:42.949753296Z level=info msg="Executing migration" id="create login attempt table"
grafana-1          | logger=migrator t=2025-11-07T15:48:42.950837504Z level=info msg="Migration successfully executed" id="create login attempt table" duration=1.0855ms
grafana-1          | logger=migrator t=2025-11-07T15:48:43.225429504Z level=info msg="Executing migration" id="add index login_attempt.username"
grafana-1          | logger=migrator t=2025-11-07T15:48:43.226966254Z level=info msg="Migration successfully executed" id="add index login_attempt.username" duration=1.523417ms
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:48:43.373745838Z level=info msg="Executing migration" id="drop index IDX_login_attempt_username - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:43.378294421Z level=info msg="Migration successfully executed" id="drop index IDX_login_attempt_username - v1" duration=4.549625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:43.670826755Z level=info msg="Executing migration" id="Rename table login_attempt to login_attempt_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:43.712940921Z level=info msg="Migration successfully executed" id="Rename table login_attempt to login_attempt_tmp_qwerty - v1" duration=42.113709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:44.006580338Z level=info msg="Executing migration" id="create login_attempt v2"
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
grafana-1          | logger=migrator t=2025-11-07T15:48:44.007721505Z level=info msg="Migration successfully executed" id="create login_attempt v2" duration=1.14175ms
grafana-1          | logger=migrator t=2025-11-07T15:48:44.137319671Z level=info msg="Executing migration" id="create index IDX_login_attempt_username - v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:44.138529421Z level=info msg="Migration successfully executed" id="create index IDX_login_attempt_username - v2" duration=1.211084ms
grafana-1          | logger=migrator t=2025-11-07T15:48:44.260516255Z level=info msg="Executing migration" id="copy login_attempt v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:48:44.261050005Z level=info msg="Migration successfully executed" id="copy login_attempt v1 to v2" duration=537.458Âµs
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
grafana-1          | logger=migrator t=2025-11-07T15:48:44.340777088Z level=info msg="Executing migration" id="drop login_attempt_tmp_qwerty"
grafana-1          | logger=migrator t=2025-11-07T15:48:44.35972738Z level=info msg="Migration successfully executed" id="drop login_attempt_tmp_qwerty" duration=18.942ms
grafana-1          | logger=migrator t=2025-11-07T15:48:44.535521672Z level=info msg="Executing migration" id="increase login_attempt.ip_address column length for IPv6 support"
order-service-1    |   retryTime: 11668,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:48:44.535581838Z level=info msg="Migration successfully executed" id="increase login_attempt.ip_address column length for IPv6 support" duration=62.916Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:44.672927213Z level=info msg="Executing migration" id="alter table login_attempt alter column created type to bigint"
grafana-1          | logger=migrator t=2025-11-07T15:48:44.673022755Z level=info msg="Migration successfully executed" id="alter table login_attempt alter column created type to bigint" duration=111.583Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:44.88074888Z level=info msg="Executing migration" id="create user auth table"
grafana-1          | logger=migrator t=2025-11-07T15:48:44.881879797Z level=info msg="Migration successfully executed" id="create user auth table" duration=1.130958ms
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
grafana-1          | logger=migrator t=2025-11-07T15:48:45.049461505Z level=info msg="Executing migration" id="create index IDX_user_auth_auth_module_auth_id - v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:45.050764297Z level=info msg="Migration successfully executed" id="create index IDX_user_auth_auth_module_auth_id - v1" duration=1.303834ms
grafana-1          | logger=migrator t=2025-11-07T15:48:45.244548714Z level=info msg="Executing migration" id="alter user_auth.auth_id to length 190"
product-service-1  | Startup error: KafkaJSNonRetriableError
grafana-1          | logger=migrator t=2025-11-07T15:48:45.244580089Z level=info msg="Migration successfully executed" id="alter user_auth.auth_id to length 190" duration=37.542Âµs
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6614,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:48:45.532620172Z level=info msg="Executing migration" id="Add OAuth access token to user_auth"
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ” encrypt with Dotenvx: https://dotenvx.com
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:41:36.233Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:36.321Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:36.321Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":280}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:36.616Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:36.618Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":546}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:37.183Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:37.183Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1148}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:38.344Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:38.346Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1954}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:40.319Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:40.323Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3846}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:44.182Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:44.183Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6308}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:41:44.185Z"}
order-service-1    | Server is running on port 3001
grafana-1          | logger=migrator t=2025-11-07T15:48:45.538591214Z level=info msg="Migration successfully executed" id="Add OAuth access token to user_auth" duration=5.970708ms
grafana-1          | logger=migrator t=2025-11-07T15:48:45.658141672Z level=info msg="Executing migration" id="Add OAuth refresh token to user_auth"
grafana-1          | logger=migrator t=2025-11-07T15:48:45.679366547Z level=info msg="Migration successfully executed" id="Add OAuth refresh token to user_auth" duration=21.223709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:46.097651297Z level=info msg="Executing migration" id="Add OAuth token type to user_auth"
grafana-1          | logger=migrator t=2025-11-07T15:48:46.103736672Z level=info msg="Migration successfully executed" id="Add OAuth token type to user_auth" duration=6.084458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:46.308555256Z level=info msg="Executing migration" id="Add OAuth expiry to user_auth"
grafana-1          | logger=migrator t=2025-11-07T15:48:46.320602964Z level=info msg="Migration successfully executed" id="Add OAuth expiry to user_auth" duration=12.032583ms
grafana-1          | logger=migrator t=2025-11-07T15:48:46.680835339Z level=info msg="Executing migration" id="Add index to user_id column in user_auth"
grafana-1          | logger=migrator t=2025-11-07T15:48:46.681536381Z level=info msg="Migration successfully executed" id="Add index to user_id column in user_auth" duration=701.958Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:46.811697506Z level=info msg="Executing migration" id="Add OAuth ID token to user_auth"
grafana-1          | logger=migrator t=2025-11-07T15:48:46.829533214Z level=info msg="Migration successfully executed" id="Add OAuth ID token to user_auth" duration=17.835125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:47.079943756Z level=info msg="Executing migration" id="Add user_unique_id to user_auth"
grafana-1          | logger=migrator t=2025-11-07T15:48:47.088748381Z level=info msg="Migration successfully executed" id="Add user_unique_id to user_auth" duration=8.802417ms
grafana-1          | logger=migrator t=2025-11-07T15:48:47.577548756Z level=info msg="Executing migration" id="create server_lock table"
grafana-1          | logger=migrator t=2025-11-07T15:48:47.578589548Z level=info msg="Migration successfully executed" id="create server_lock table" duration=1.040542ms
grafana-1          | logger=migrator t=2025-11-07T15:48:48.063289465Z level=info msg="Executing migration" id="add index server_lock.operation_uid"
grafana-1          | logger=migrator t=2025-11-07T15:48:48.064071923Z level=info msg="Migration successfully executed" id="add index server_lock.operation_uid" duration=783.458Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:48.534836882Z level=info msg="Executing migration" id="create user auth token table"
grafana-1          | logger=migrator t=2025-11-07T15:48:48.536827007Z level=info msg="Migration successfully executed" id="create user auth token table" duration=1.990875ms
grafana-1          | logger=migrator t=2025-11-07T15:48:48.873862799Z level=info msg="Executing migration" id="add unique index user_auth_token.auth_token"
grafana-1          | logger=migrator t=2025-11-07T15:48:48.874759215Z level=info msg="Migration successfully executed" id="add unique index user_auth_token.auth_token" duration=897.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:49.177649382Z level=info msg="Executing migration" id="add unique index user_auth_token.prev_auth_token"
grafana-1          | logger=migrator t=2025-11-07T15:48:49.301023632Z level=info msg="Migration successfully executed" id="add unique index user_auth_token.prev_auth_token" duration=123.791875ms
grafana-1          | logger=migrator t=2025-11-07T15:48:49.544728799Z level=info msg="Executing migration" id="add index user_auth_token.user_id"
grafana-1          | logger=migrator t=2025-11-07T15:48:49.546101841Z level=info msg="Migration successfully executed" id="add index user_auth_token.user_id" duration=1.375125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:50.319021924Z level=info msg="Executing migration" id="Add revoked_at to the user auth token"
grafana-1          | logger=migrator t=2025-11-07T15:48:50.341872091Z level=info msg="Migration successfully executed" id="Add revoked_at to the user auth token" duration=22.852375ms
grafana-1          | logger=migrator t=2025-11-07T15:48:50.496820841Z level=info msg="Executing migration" id="add index user_auth_token.revoked_at"
grafana-1          | logger=migrator t=2025-11-07T15:48:50.498227633Z level=info msg="Migration successfully executed" id="add index user_auth_token.revoked_at" duration=1.406917ms
grafana-1          | logger=migrator t=2025-11-07T15:48:50.614953716Z level=info msg="Executing migration" id="add external_session_id to user_auth_token"
grafana-1          | logger=migrator t=2025-11-07T15:48:50.644638341Z level=info msg="Migration successfully executed" id="add external_session_id to user_auth_token" duration=29.687041ms
grafana-1          | logger=migrator t=2025-11-07T15:48:51.20367305Z level=info msg="Executing migration" id="create cache_data table"
grafana-1          | logger=migrator t=2025-11-07T15:48:51.204935508Z level=info msg="Migration successfully executed" id="create cache_data table" duration=1.261792ms
grafana-1          | logger=migrator t=2025-11-07T15:48:51.4818513Z level=info msg="Executing migration" id="add unique index cache_data.cache_key"
grafana-1          | logger=migrator t=2025-11-07T15:48:51.483140467Z level=info msg="Migration successfully executed" id="add unique index cache_data.cache_key" duration=1.275292ms
grafana-1          | logger=migrator t=2025-11-07T15:48:52.071201509Z level=info msg="Executing migration" id="create short_url table v1"
grafana-1          | logger=migrator t=2025-11-07T15:48:52.075791425Z level=info msg="Migration successfully executed" id="create short_url table v1" duration=4.586459ms
grafana-1          | logger=migrator t=2025-11-07T15:48:52.537881925Z level=info msg="Executing migration" id="add index short_url.org_id-uid"
grafana-1          | logger=migrator t=2025-11-07T15:48:52.543008467Z level=info msg="Migration successfully executed" id="add index short_url.org_id-uid" duration=5.124667ms
grafana-1          | logger=migrator t=2025-11-07T15:48:52.895650842Z level=info msg="Executing migration" id="alter table short_url alter column created_by type to bigint"
grafana-1          | logger=migrator t=2025-11-07T15:48:52.895732842Z level=info msg="Migration successfully executed" id="alter table short_url alter column created_by type to bigint" duration=83.708Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:53.003179717Z level=info msg="Executing migration" id="delete alert_definition table"
grafana-1          | logger=migrator t=2025-11-07T15:48:53.003369009Z level=info msg="Migration successfully executed" id="delete alert_definition table" duration=221.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:53.122004842Z level=info msg="Executing migration" id="recreate alert_definition table"
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
grafana-1          | logger=migrator t=2025-11-07T15:48:53.123407592Z level=info msg="Migration successfully executed" id="recreate alert_definition table" duration=1.4025ms
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”‘ add access controls to secrets: https://dotenvx.com/ops
grafana-1          | logger=migrator t=2025-11-07T15:48:53.357566676Z level=info msg="Executing migration" id="add index in alert_definition on org_id and title columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:53.358773884Z level=info msg="Migration successfully executed" id="add index in alert_definition on org_id and title columns" duration=1.208625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:53.726856926Z level=info msg="Executing migration" id="add index in alert_definition on org_id and uid columns"
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:40:23.002Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2025-11-07T15:48:53.729418926Z level=info msg="Migration successfully executed" id="add index in alert_definition on org_id and uid columns" duration=2.562291ms
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:48:53.865437301Z level=info msg="Executing migration" id="alter alert_definition table data column to mediumtext in mysql"
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:48:53.865545551Z level=info msg="Migration successfully executed" id="alter alert_definition table data column to mediumtext in mysql" duration=112.375Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:54.050127718Z level=info msg="Executing migration" id="drop index in alert_definition on org_id and title columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:54.052424051Z level=info msg="Migration successfully executed" id="drop index in alert_definition on org_id and title columns" duration=2.296666ms
grafana-1          | logger=migrator t=2025-11-07T15:48:54.194809718Z level=info msg="Executing migration" id="drop index in alert_definition on org_id and uid columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:54.196843135Z level=info msg="Migration successfully executed" id="drop index in alert_definition on org_id and uid columns" duration=2.035458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:54.456827468Z level=info msg="Executing migration" id="add unique index in alert_definition on org_id and title columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:54.46057376Z level=info msg="Migration successfully executed" id="add unique index in alert_definition on org_id and title columns" duration=3.744875ms
grafana-1          | logger=migrator t=2025-11-07T15:48:54.609065593Z level=info msg="Executing migration" id="add unique index in alert_definition on org_id and uid columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:54.60984726Z level=info msg="Migration successfully executed" id="add unique index in alert_definition on org_id and uid columns" duration=782.708Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:54.767855135Z level=info msg="Executing migration" id="Add column paused in alert_definition"
grafana-1          | logger=migrator t=2025-11-07T15:48:54.774627802Z level=info msg="Migration successfully executed" id="Add column paused in alert_definition" duration=6.772792ms
grafana-1          | logger=migrator t=2025-11-07T15:48:55.069725427Z level=info msg="Executing migration" id="drop alert_definition table"
grafana-1          | logger=migrator t=2025-11-07T15:48:55.070987718Z level=info msg="Migration successfully executed" id="drop alert_definition table" duration=1.263083ms
grafana-1          | logger=migrator t=2025-11-07T15:48:55.365499135Z level=info msg="Executing migration" id="delete alert_definition_version table"
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2025-11-07T15:48:55.365597635Z level=info msg="Migration successfully executed" id="delete alert_definition_version table" duration=99.792Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:55.599548052Z level=info msg="Executing migration" id="recreate alert_definition_version table"
grafana-1          | logger=migrator t=2025-11-07T15:48:55.603661427Z level=info msg="Migration successfully executed" id="recreate alert_definition_version table" duration=4.113375ms
grafana-1          | logger=migrator t=2025-11-07T15:48:55.74389426Z level=info msg="Executing migration" id="add index in alert_definition_version table on alert_definition_id and version columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:55.745317844Z level=info msg="Migration successfully executed" id="add index in alert_definition_version table on alert_definition_id and version columns" duration=1.425708ms
grafana-1          | logger=migrator t=2025-11-07T15:48:56.076899885Z level=info msg="Executing migration" id="add index in alert_definition_version table on alert_definition_uid and version columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:56.090192719Z level=info msg="Migration successfully executed" id="add index in alert_definition_version table on alert_definition_uid and version columns" duration=13.292166ms
grafana-1          | logger=migrator t=2025-11-07T15:48:56.233812844Z level=info msg="Executing migration" id="alter alert_definition_version table data column to mediumtext in mysql"
grafana-1          | logger=migrator t=2025-11-07T15:48:56.233951844Z level=info msg="Migration successfully executed" id="alter alert_definition_version table data column to mediumtext in mysql" duration=113.584Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:56.296352594Z level=info msg="Executing migration" id="drop alert_definition_version table"
grafana-1          | logger=migrator t=2025-11-07T15:48:56.298179344Z level=info msg="Migration successfully executed" id="drop alert_definition_version table" duration=1.816792ms
grafana-1          | logger=migrator t=2025-11-07T15:48:56.347727344Z level=info msg="Executing migration" id="create alert_instance table"
grafana-1          | logger=migrator t=2025-11-07T15:48:56.349282969Z level=info msg="Migration successfully executed" id="create alert_instance table" duration=1.556583ms
grafana-1          | logger=migrator t=2025-11-07T15:48:56.428020886Z level=info msg="Executing migration" id="add index in alert_instance table on def_org_id, def_uid and current_state columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:56.430689052Z level=info msg="Migration successfully executed" id="add index in alert_instance table on def_org_id, def_uid and current_state columns" duration=2.673041ms
grafana-1          | logger=migrator t=2025-11-07T15:48:56.581874011Z level=info msg="Executing migration" id="add index in alert_instance table on def_org_id, current_state columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:56.587062511Z level=info msg="Migration successfully executed" id="add index in alert_instance table on def_org_id, current_state columns" duration=5.189167ms
grafana-1          | logger=migrator t=2025-11-07T15:48:56.905708969Z level=info msg="Executing migration" id="add column current_state_end to alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:56.914384053Z level=info msg="Migration successfully executed" id="add column current_state_end to alert_instance" duration=8.674125ms
grafana-1          | logger=migrator t=2025-11-07T15:48:57.071937136Z level=info msg="Executing migration" id="remove index def_org_id, def_uid, current_state on alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:57.072751178Z level=info msg="Migration successfully executed" id="remove index def_org_id, def_uid, current_state on alert_instance" duration=815.167Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:57.262570178Z level=info msg="Executing migration" id="remove index def_org_id, current_state on alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:57.264874511Z level=info msg="Migration successfully executed" id="remove index def_org_id, current_state on alert_instance" duration=2.306208ms
grafana-1          | logger=migrator t=2025-11-07T15:48:57.463740886Z level=info msg="Executing migration" id="rename def_org_id to rule_org_id in alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:57.538077261Z level=info msg="Migration successfully executed" id="rename def_org_id to rule_org_id in alert_instance" duration=74.33625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:57.791769261Z level=info msg="Executing migration" id="rename def_uid to rule_uid in alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:58.179611845Z level=info msg="Migration successfully executed" id="rename def_uid to rule_uid in alert_instance" duration=387.839625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:58.251605761Z level=info msg="Executing migration" id="add index rule_org_id, rule_uid, current_state on alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:58.252357053Z level=info msg="Migration successfully executed" id="add index rule_org_id, rule_uid, current_state on alert_instance" duration=752.167Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:58.410184803Z level=info msg="Executing migration" id="add index rule_org_id, current_state on alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:58.411388762Z level=info msg="Migration successfully executed" id="add index rule_org_id, current_state on alert_instance" duration=1.205709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:58.534874803Z level=info msg="Executing migration" id="add current_reason column related to current_state"
grafana-1          | logger=migrator t=2025-11-07T15:48:58.548223678Z level=info msg="Migration successfully executed" id="add current_reason column related to current_state" duration=13.347458ms
grafana-1          | logger=migrator t=2025-11-07T15:48:58.702872637Z level=info msg="Executing migration" id="add result_fingerprint column to alert_instance"
grafana-1          | logger=migrator t=2025-11-07T15:48:58.710257137Z level=info msg="Migration successfully executed" id="add result_fingerprint column to alert_instance" duration=7.383625ms
grafana-1          | logger=migrator t=2025-11-07T15:48:58.896942095Z level=info msg="Executing migration" id="create alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:48:58.898266012Z level=info msg="Migration successfully executed" id="create alert_rule table" duration=1.325833ms
grafana-1          | logger=migrator t=2025-11-07T15:48:59.078920595Z level=info msg="Executing migration" id="add index in alert_rule on org_id and title columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.080388929Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id and title columns" duration=1.469083ms
grafana-1          | logger=migrator t=2025-11-07T15:48:59.20649722Z level=info msg="Executing migration" id="add index in alert_rule on org_id and uid columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.209347429Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id and uid columns" duration=2.851042ms
grafana-1          | logger=migrator t=2025-11-07T15:48:59.289088804Z level=info msg="Executing migration" id="add index in alert_rule on org_id, namespace_uid, group_uid columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.289843595Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id, namespace_uid, group_uid columns" duration=755.291Âµs
grafana-1          | logger=migrator t=2025-11-07T15:48:59.34428472Z level=info msg="Executing migration" id="alter alert_rule table data column to mediumtext in mysql"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.344305887Z level=info msg="Migration successfully executed" id="alter alert_rule table data column to mediumtext in mysql" duration=2.952416ms
grafana-1          | logger=migrator t=2025-11-07T15:48:59.450840137Z level=info msg="Executing migration" id="add column for to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.462804054Z level=info msg="Migration successfully executed" id="add column for to alert_rule" duration=11.9485ms
grafana-1          | logger=migrator t=2025-11-07T15:48:59.589870804Z level=info msg="Executing migration" id="add column annotations to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.593788137Z level=info msg="Migration successfully executed" id="add column annotations to alert_rule" duration=3.915333ms
grafana-1          | logger=migrator t=2025-11-07T15:48:59.797651971Z level=info msg="Executing migration" id="add column labels to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.805653304Z level=info msg="Migration successfully executed" id="add column labels to alert_rule" duration=8.000709ms
grafana-1          | logger=migrator t=2025-11-07T15:48:59.960609471Z level=info msg="Executing migration" id="remove unique index from alert_rule on org_id, title columns"
grafana-1          | logger=migrator t=2025-11-07T15:48:59.969640054Z level=info msg="Migration successfully executed" id="remove unique index from alert_rule on org_id, title columns" duration=8.565709ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.015218637Z level=info msg="Executing migration" id="add index in alert_rule on org_id, namespase_uid and title columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.016848054Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id, namespase_uid and title columns" duration=1.630625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.059500679Z level=info msg="Executing migration" id="add dashboard_uid column to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.072367304Z level=info msg="Migration successfully executed" id="add dashboard_uid column to alert_rule" duration=12.808042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.136470596Z level=info msg="Executing migration" id="add panel_id column to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.153887179Z level=info msg="Migration successfully executed" id="add panel_id column to alert_rule" duration=17.408459ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.185520637Z level=info msg="Executing migration" id="add index in alert_rule on org_id, dashboard_uid and panel_id columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.187561012Z level=info msg="Migration successfully executed" id="add index in alert_rule on org_id, dashboard_uid and panel_id columns" duration=2.039833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.221368137Z level=info msg="Executing migration" id="add rule_group_idx column to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.330164137Z level=info msg="Migration successfully executed" id="add rule_group_idx column to alert_rule" duration=108.780792ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.363682888Z level=info msg="Executing migration" id="add is_paused column to alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.386134513Z level=info msg="Migration successfully executed" id="add is_paused column to alert_rule table" duration=22.384416ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.411798054Z level=info msg="Executing migration" id="fix is_paused column for alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.412535179Z level=info msg="Migration successfully executed" id="fix is_paused column for alert_rule table" duration=741.5Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:00.431222096Z level=info msg="Executing migration" id="alter table alert_rule alter column rule_group_idx type to bigint"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.431307804Z level=info msg="Migration successfully executed" id="alter table alert_rule alter column rule_group_idx type to bigint" duration=88.959Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:00.455368846Z level=info msg="Executing migration" id="create alert_rule_version table"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.473857679Z level=info msg="Migration successfully executed" id="create alert_rule_version table" duration=18.488792ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.546757346Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_org_id, rule_uid and version columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.560801429Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_org_id, rule_uid and version columns" duration=14.025584ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.582455763Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_org_id, rule_namespace_uid and rule_group columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.598681388Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_org_id, rule_namespace_uid and rule_group columns" duration=16.218833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:00.673913138Z level=info msg="Executing migration" id="alter alert_rule_version table data column to mediumtext in mysql"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.673980638Z level=info msg="Migration successfully executed" id="alter alert_rule_version table data column to mediumtext in mysql" duration=70.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:00.932838888Z level=info msg="Executing migration" id="add column for to alert_rule_version"
grafana-1          | logger=migrator t=2025-11-07T15:49:00.952195096Z level=info msg="Migration successfully executed" id="add column for to alert_rule_version" duration=19.35275ms
grafana-1          | logger=migrator t=2025-11-07T15:49:01.038903846Z level=info msg="Executing migration" id="add column annotations to alert_rule_version"
grafana-1          | logger=migrator t=2025-11-07T15:49:01.073334055Z level=info msg="Migration successfully executed" id="add column annotations to alert_rule_version" duration=34.428376ms
grafana-1          | logger=migrator t=2025-11-07T15:49:01.179661596Z level=info msg="Executing migration" id="add column labels to alert_rule_version"
grafana-1          | logger=migrator t=2025-11-07T15:49:01.192815846Z level=info msg="Migration successfully executed" id="add column labels to alert_rule_version" duration=13.153959ms
grafana-1          | logger=migrator t=2025-11-07T15:49:01.354288388Z level=info msg="Executing migration" id="add rule_group_idx column to alert_rule_version"
grafana-1          | logger=migrator t=2025-11-07T15:49:01.36506718Z level=info msg="Migration successfully executed" id="add rule_group_idx column to alert_rule_version" duration=10.775042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:01.46123343Z level=info msg="Executing migration" id="add is_paused column to alert_rule_versions table"
grafana-1          | logger=migrator t=2025-11-07T15:49:01.482876555Z level=info msg="Migration successfully executed" id="add is_paused column to alert_rule_versions table" duration=21.613083ms
grafana-1          | logger=migrator t=2025-11-07T15:49:01.657259763Z level=info msg="Executing migration" id="fix is_paused column for alert_rule_version table"
grafana-1          | logger=migrator t=2025-11-07T15:49:01.657319846Z level=info msg="Migration successfully executed" id="fix is_paused column for alert_rule_version table" duration=298.792Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:01.801109347Z level=info msg="Executing migration" id="alter table alert_rule_version alter column rule_group_idx type to bigint"
grafana-1          | logger=migrator t=2025-11-07T15:49:01.801266013Z level=info msg="Migration successfully executed" id="alter table alert_rule_version alter column rule_group_idx type to bigint" duration=158.583Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:01.921080638Z level=info msg="Executing migration" id=create_alert_configuration_table
grafana-1          | logger=migrator t=2025-11-07T15:49:01.924966472Z level=info msg="Migration successfully executed" id=create_alert_configuration_table duration=3.884292ms
grafana-1          | logger=migrator t=2025-11-07T15:49:01.984712138Z level=info msg="Executing migration" id="Add column default in alert_configuration"
grafana-1          | logger=migrator t=2025-11-07T15:49:02.057496555Z level=info msg="Migration successfully executed" id="Add column default in alert_configuration" duration=72.780458ms
grafana-1          | logger=migrator t=2025-11-07T15:49:02.281614263Z level=info msg="Executing migration" id="alert alert_configuration alertmanager_configuration column from TEXT to MEDIUMTEXT if mysql"
grafana-1          | logger=migrator t=2025-11-07T15:49:02.281643847Z level=info msg="Migration successfully executed" id="alert alert_configuration alertmanager_configuration column from TEXT to MEDIUMTEXT if mysql" duration=31.333Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:02.463174014Z level=info msg="Executing migration" id="add column org_id in alert_configuration"
grafana-1          | logger=migrator t=2025-11-07T15:49:02.492184472Z level=info msg="Migration successfully executed" id="add column org_id in alert_configuration" duration=29.006ms
grafana-1          | logger=migrator t=2025-11-07T15:49:02.79995218Z level=info msg="Executing migration" id="add index in alert_configuration table on org_id column"
grafana-1          | logger=migrator t=2025-11-07T15:49:02.805799722Z level=info msg="Migration successfully executed" id="add index in alert_configuration table on org_id column" duration=5.863ms
grafana-1          | logger=migrator t=2025-11-07T15:49:03.129316014Z level=info msg="Executing migration" id="add configuration_hash column to alert_configuration"
grafana-1          | logger=migrator t=2025-11-07T15:49:03.152369972Z level=info msg="Migration successfully executed" id="add configuration_hash column to alert_configuration" duration=23.04275ms
grafana-1          | logger=migrator t=2025-11-07T15:49:03.443850541Z level=info msg="Executing migration" id=create_ngalert_configuration_table
grafana-1          | logger=migrator t=2025-11-07T15:49:03.445644375Z level=info msg="Migration successfully executed" id=create_ngalert_configuration_table duration=1.795167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:03.680742583Z level=info msg="Executing migration" id="add index in ngalert_configuration on org_id column"
grafana-1          | logger=migrator t=2025-11-07T15:49:03.682283041Z level=info msg="Migration successfully executed" id="add index in ngalert_configuration on org_id column" duration=1.541ms
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:23.102Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:23.103Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":325}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:23.446Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:23.447Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":666}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:24.126Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:24.127Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1586}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:25.719Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:25.720Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3490}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:29.223Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:29.224Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":7206}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:44.203Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.ZooKeeper)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:44.203Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":267}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:44.482Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:44.483Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":612}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:45.106Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:os.memory.free=1010MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,471] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,473] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@77b14724 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2025-11-07 15:49:31,489] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
kafka-1            | [2025-11-07 15:49:31,554] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2025-11-07 15:49:31,614] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2025-11-07 15:49:32,020] INFO Opening socket connection to server zookeeper/172.18.0.3:2181. (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2025-11-07 15:49:32,063] INFO SASL config status: Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
grafana-1          | logger=migrator t=2025-11-07T15:49:04.063807Z level=info msg="Executing migration" id="add column send_alerts_to in ngalert_configuration"
grafana-1          | logger=migrator t=2025-11-07T15:49:04.075509042Z level=info msg="Migration successfully executed" id="add column send_alerts_to in ngalert_configuration" duration=11.695292ms
grafana-1          | logger=migrator t=2025-11-07T15:49:04.321526875Z level=info msg="Executing migration" id="create provenance_type table"
kafka-1            | [2025-11-07 15:49:32,088] INFO Socket connection established, initiating session, client: /172.18.0.7:47568, server: zookeeper/172.18.0.3:2181 (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2025-11-07 15:49:32,431] INFO Session establishment complete on server zookeeper/172.18.0.3:2181, session id = 0x100005b512f0001, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2025-11-07 15:49:32,449] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2025-11-07 15:49:35,731] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
grafana-1          | logger=migrator t=2025-11-07T15:49:04.337087292Z level=info msg="Migration successfully executed" id="create provenance_type table" duration=15.560042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:04.514207833Z level=info msg="Executing migration" id="add index to uniquify (record_key, record_type, org_id) columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:04.51551475Z level=info msg="Migration successfully executed" id="add index to uniquify (record_key, record_type, org_id) columns" duration=1.3085ms
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:36.439Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:36.440Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12126}
kafka-1            | [2025-11-07 15:49:36,001] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)
kafka-1            | [2025-11-07 15:49:36,001] INFO Cleared cache (kafka.server.FinalizedFeatureCache)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:45.107Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1086}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:46.200Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:49:04.722266584Z level=info msg="Executing migration" id="create alert_image table"
grafana-1          | logger=migrator t=2025-11-07T15:49:04.723067917Z level=info msg="Migration successfully executed" id="create alert_image table" duration=802.166Âµs
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2025-11-07 15:49:37,640] INFO Cluster ID = qmOUJ3StQVKTJWzG6j8-Fw (kafka.server.KafkaServer)
kafka-1            | [2025-11-07 15:49:37,646] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
kafka-1            | [2025-11-07 15:49:37,981] INFO KafkaConfig values: 
kafka-1            | 	advertised.listeners = PLAINTEXT://kafka:9092
kafka-1            | 	alter.config.policy.class.name = null
kafka-1            | 	alter.log.dirs.replication.quota.window.num = 11
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:46.200Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1834}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:48.048Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:48.049Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3974}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:52.041Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:52.043Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7834}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	alter.log.dirs.replication.quota.window.size.seconds = 1
kafka-1            | 	authorizer.class.name = 
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
grafana-1          | logger=migrator t=2025-11-07T15:49:04.952492959Z level=info msg="Executing migration" id="add unique index on token to alert_image table"
grafana-1          | logger=migrator t=2025-11-07T15:49:04.953972625Z level=info msg="Migration successfully executed" id="add unique index on token to alert_image table" duration=1.480958ms
grafana-1          | logger=migrator t=2025-11-07T15:49:05.097081709Z level=info msg="Executing migration" id="support longer URLs in alert_image table"
grafana-1          | logger=migrator t=2025-11-07T15:49:05.097167709Z level=info msg="Migration successfully executed" id="support longer URLs in alert_image table" duration=87.292Âµs
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
grafana-1          | logger=migrator t=2025-11-07T15:49:05.2178115Z level=info msg="Executing migration" id=create_alert_configuration_history_table
grafana-1          | logger=migrator t=2025-11-07T15:49:05.227151917Z level=info msg="Migration successfully executed" id=create_alert_configuration_history_table duration=9.336459ms
grafana-1          | logger=migrator t=2025-11-07T15:49:05.376681334Z level=info msg="Executing migration" id="drop non-unique orgID index on alert_configuration"
grafana-1          | logger=migrator t=2025-11-07T15:49:05.377538709Z level=info msg="Migration successfully executed" id="drop non-unique orgID index on alert_configuration" duration=858.041Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:05.594668251Z level=info msg="Executing migration" id="drop unique orgID index on alert_configuration if exists"
grafana-1          | logger=migrator t=2025-11-07T15:49:05.595488084Z level=warn msg="Skipping migration: Already executed, but not recorded in migration log" id="drop unique orgID index on alert_configuration if exists"
grafana-1          | logger=migrator t=2025-11-07T15:49:05.899230834Z level=info msg="Executing migration" id="extract alertmanager configuration history to separate table"
grafana-1          | logger=migrator t=2025-11-07T15:49:05.900131209Z level=info msg="Migration successfully executed" id="extract alertmanager configuration history to separate table" duration=901.833Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:06.099853834Z level=info msg="Executing migration" id="add unique index on orgID to alert_configuration"
grafana-1          | logger=migrator t=2025-11-07T15:49:06.102351668Z level=info msg="Migration successfully executed" id="add unique index on orgID to alert_configuration" duration=2.4975ms
grafana-1          | logger=migrator t=2025-11-07T15:49:06.502010293Z level=info msg="Executing migration" id="add last_applied column to alert_configuration_history"
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12126,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | 	auto.create.topics.enable = true
kafka-1            | 	auto.leader.rebalance.enable = true
kafka-1            | 	background.threads = 10
kafka-1            | 	broker.heartbeat.interval.ms = 2000
kafka-1            | 	broker.id = 1
kafka-1            | 	broker.id.generation.enable = true
kafka-1            | 	broker.rack = null
kafka-1            | 	broker.session.timeout.ms = 9000
kafka-1            | 	client.quota.callback.class = null
kafka-1            | 	compression.type = producer
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
kafka-1            | 	connection.failed.authentication.delay.ms = 100
kafka-1            | 	connections.max.idle.ms = 600000
kafka-1            | 	connections.max.reauth.ms = 0
kafka-1            | 	control.plane.listener.name = null
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | 	controlled.shutdown.enable = true
kafka-1            | 	controlled.shutdown.max.retries = 3
kafka-1            | 	controlled.shutdown.retry.backoff.ms = 5000
kafka-1            | 	controller.listener.names = null
kafka-1            | 	controller.quorum.append.linger.ms = 25
kafka-1            | 	controller.quorum.election.backoff.max.ms = 1000
kafka-1            | 	controller.quorum.election.timeout.ms = 1000
kafka-1            | 	controller.quorum.fetch.timeout.ms = 2000
kafka-1            | 	controller.quorum.request.timeout.ms = 2000
kafka-1            | 	controller.quorum.retry.backoff.ms = 20
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 7834,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12126,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	controller.quorum.voters = []
kafka-1            | 	controller.quota.window.num = 11
kafka-1            | 	controller.quota.window.size.seconds = 1
grafana-1          | logger=migrator t=2025-11-07T15:49:06.532162668Z level=info msg="Migration successfully executed" id="add last_applied column to alert_configuration_history" duration=30.118334ms
grafana-1          | logger=migrator t=2025-11-07T15:49:06.866291751Z level=info msg="Executing migration" id="create library_element table v1"
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
grafana-1          | logger=migrator t=2025-11-07T15:49:06.86835146Z level=info msg="Migration successfully executed" id="create library_element table v1" duration=2.064708ms
grafana-1          | logger=migrator t=2025-11-07T15:49:07.144087085Z level=info msg="Executing migration" id="add index library_element org_id-folder_id-name-kind"
grafana-1          | logger=migrator t=2025-11-07T15:49:07.14508521Z level=info msg="Migration successfully executed" id="add index library_element org_id-folder_id-name-kind" duration=998.625Âµs
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
grafana-1          | logger=migrator t=2025-11-07T15:49:07.547631502Z level=info msg="Executing migration" id="create library_element_connection table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:07.551407377Z level=info msg="Migration successfully executed" id="create library_element_connection table v1" duration=3.776833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:07.856587335Z level=info msg="Executing migration" id="add index library_element_connection element_id-kind-connection_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:07.857983002Z level=info msg="Migration successfully executed" id="add index library_element_connection element_id-kind-connection_id" duration=1.396459ms
kafka-1            | 	controller.socket.timeout.ms = 30000
kafka-1            | 	create.topic.policy.class.name = null
kafka-1            | 	default.replication.factor = 1
kafka-1            | 	delegation.token.expiry.check.interval.ms = 3600000
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
kafka-1            | 	delegation.token.expiry.time.ms = 86400000
kafka-1            | 	delegation.token.master.key = null
kafka-1            | 	delegation.token.max.lifetime.ms = 604800000
kafka-1            | 	delegation.token.secret.key = null
kafka-1            | 	delete.records.purgatory.purge.interval.requests = 1
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:41:52.825Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
grafana-1          | logger=migrator t=2025-11-07T15:49:08.061121377Z level=info msg="Executing migration" id="add unique index library_element org_id_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:08.066264585Z level=info msg="Migration successfully executed" id="add unique index library_element org_id_uid" duration=5.140209ms
grafana-1          | logger=migrator t=2025-11-07T15:49:08.180286127Z level=info msg="Executing migration" id="increase max description length to 2048"
kafka-1            | 	delete.topic.enable = true
kafka-1            | 	fetch.max.bytes = 57671680
kafka-1            | 	fetch.purgatory.purge.interval.requests = 1000
kafka-1            | 	group.initial.rebalance.delay.ms = 0
kafka-1            | 	group.max.session.timeout.ms = 1800000
kafka-1            | 	group.max.size = 2147483647
grafana-1          | logger=migrator t=2025-11-07T15:49:08.180429335Z level=info msg="Migration successfully executed" id="increase max description length to 2048" duration=187.5Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:08.42533746Z level=info msg="Executing migration" id="alter library_element model to mediumtext"
grafana-1          | logger=migrator t=2025-11-07T15:49:08.425380252Z level=info msg="Migration successfully executed" id="alter library_element model to mediumtext" duration=45.292Âµs
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
grafana-1          | logger=migrator t=2025-11-07T15:49:08.61970571Z level=info msg="Executing migration" id="add library_element folder uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:08.63015046Z level=info msg="Migration successfully executed" id="add library_element folder uid" duration=10.443416ms
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
kafka-1            | 	group.min.session.timeout.ms = 6000
kafka-1            | 	initial.broker.registration.timeout.ms = 60000
kafka-1            | 	inter.broker.listener.name = null
product-service-1  |     [cause]: undefined
product-service-1  |   }
grafana-1          | logger=migrator t=2025-11-07T15:49:08.672925877Z level=info msg="Executing migration" id="populate library_element folder_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:08.675509335Z level=info msg="Migration successfully executed" id="populate library_element folder_uid" duration=2.585458ms
grafana-1          | logger=migrator t=2025-11-07T15:49:08.739832002Z level=info msg="Executing migration" id="add index library_element org_id-folder_uid-name-kind"
grafana-1          | logger=migrator t=2025-11-07T15:49:08.740622127Z level=info msg="Migration successfully executed" id="add index library_element org_id-folder_uid-name-kind" duration=791.667Âµs
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:49:08.942476377Z level=info msg="Executing migration" id="clone move dashboard alerts to unified alerting"
grafana-1          | logger=migrator t=2025-11-07T15:49:08.949697502Z level=info msg="Migration successfully executed" id="clone move dashboard alerts to unified alerting" duration=7.222458ms
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
kafka-1            | 	inter.broker.protocol.version = 3.0-IV1
kafka-1            | 	kafka.metrics.polling.interval.secs = 10
product-service-1  | }
product-service-1  | 
grafana-1          | logger=migrator t=2025-11-07T15:49:09.048999586Z level=info msg="Executing migration" id="create data_keys table"
grafana-1          | logger=migrator t=2025-11-07T15:49:09.055185127Z level=info msg="Migration successfully executed" id="create data_keys table" duration=6.183125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:09.363609127Z level=info msg="Executing migration" id="create secrets table"
grafana-1          | logger=migrator t=2025-11-07T15:49:09.364318836Z level=info msg="Migration successfully executed" id="create secrets table" duration=710.375Âµs
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” encrypt with Dotenvx: https://dotenvx.com
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:40:37.077Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:49:09.548488294Z level=info msg="Executing migration" id="rename data_keys name column to id"
kafka-1            | 	kafka.metrics.reporters = []
kafka-1            | 	leader.imbalance.check.interval.seconds = 300
grafana-1          | logger=migrator t=2025-11-07T15:49:09.607452961Z level=info msg="Migration successfully executed" id="rename data_keys name column to id" duration=58.960625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:09.789456711Z level=info msg="Executing migration" id="add name column into data_keys"
kafka-1            | 	leader.imbalance.per.broker.percentage = 10
kafka-1            | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
kafka-1            | 	listeners = PLAINTEXT://0.0.0.0:9092
grafana-1          | logger=migrator t=2025-11-07T15:49:09.803495044Z level=info msg="Migration successfully executed" id="add name column into data_keys" duration=14.037209ms
grafana-1          | logger=migrator t=2025-11-07T15:49:10.110429961Z level=info msg="Executing migration" id="copy data_keys id column values into name"
grafana-1          | logger=migrator t=2025-11-07T15:49:10.110799461Z level=info msg="Migration successfully executed" id="copy data_keys id column values into name" duration=376.666Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:10.213427711Z level=info msg="Executing migration" id="rename data_keys name column to label"
grafana-1          | logger=migrator t=2025-11-07T15:49:10.308679003Z level=info msg="Migration successfully executed" id="rename data_keys name column to label" duration=95.2455ms
grafana-1          | logger=migrator t=2025-11-07T15:49:10.855245087Z level=info msg="Executing migration" id="rename data_keys id column back to name"
grafana-1          | logger=migrator t=2025-11-07T15:49:10.931197295Z level=info msg="Migration successfully executed" id="rename data_keys id column back to name" duration=75.958458ms
grafana-1          | logger=migrator t=2025-11-07T15:49:11.214647837Z level=info msg="Executing migration" id="create kv_store table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:11.21536767Z level=info msg="Migration successfully executed" id="create kv_store table v1" duration=733.916Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:11.544819962Z level=info msg="Executing migration" id="add index kv_store.org_id-namespace-key"
grafana-1          | logger=migrator t=2025-11-07T15:49:11.550935337Z level=info msg="Migration successfully executed" id="add index kv_store.org_id-namespace-key" duration=6.116833ms
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
grafana-1          | logger=migrator t=2025-11-07T15:49:11.774854587Z level=info msg="Executing migration" id="update dashboard_uid and panel_id from existing annotations"
grafana-1          | logger=migrator t=2025-11-07T15:49:11.775468004Z level=info msg="Migration successfully executed" id="update dashboard_uid and panel_id from existing annotations" duration=606.583Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:12.037381212Z level=info msg="Executing migration" id="create permission table"
grafana-1          | logger=migrator t=2025-11-07T15:49:12.038653962Z level=info msg="Migration successfully executed" id="create permission table" duration=1.274375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:12.266393087Z level=info msg="Executing migration" id="add unique index permission.role_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:12.270654879Z level=info msg="Migration successfully executed" id="add unique index permission.role_id" duration=4.261625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:12.521950129Z level=info msg="Executing migration" id="add unique index role_id_action_scope"
grafana-1          | logger=migrator t=2025-11-07T15:49:12.527327046Z level=info msg="Migration successfully executed" id="add unique index role_id_action_scope" duration=5.375042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:12.785978962Z level=info msg="Executing migration" id="create role table"
grafana-1          | logger=migrator t=2025-11-07T15:49:12.786824962Z level=info msg="Migration successfully executed" id="create role table" duration=847.5Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:12.899718754Z level=info msg="Executing migration" id="add column display_name"
grafana-1          | logger=migrator t=2025-11-07T15:49:12.918497379Z level=info msg="Migration successfully executed" id="add column display_name" duration=18.758875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:13.136004588Z level=info msg="Executing migration" id="add column group_name"
grafana-1          | logger=migrator t=2025-11-07T15:49:13.155484879Z level=info msg="Migration successfully executed" id="add column group_name" duration=19.481167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:13.465598421Z level=info msg="Executing migration" id="add index role.org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:13.466920379Z level=info msg="Migration successfully executed" id="add index role.org_id" duration=1.323542ms
grafana-1          | logger=migrator t=2025-11-07T15:49:13.814686963Z level=info msg="Executing migration" id="add unique index role_org_id_name"
grafana-1          | logger=migrator t=2025-11-07T15:49:13.818625588Z level=info msg="Migration successfully executed" id="add unique index role_org_id_name" duration=3.936875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:14.068324755Z level=info msg="Executing migration" id="add index role_org_id_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:14.074066046Z level=info msg="Migration successfully executed" id="add index role_org_id_uid" duration=5.740041ms
grafana-1          | logger=migrator t=2025-11-07T15:49:14.33158238Z level=info msg="Executing migration" id="create team role table"
grafana-1          | logger=migrator t=2025-11-07T15:49:14.33626163Z level=info msg="Migration successfully executed" id="create team role table" duration=3.991958ms
grafana-1          | logger=migrator t=2025-11-07T15:49:14.527172713Z level=info msg="Executing migration" id="add index team_role.org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:14.528933963Z level=info msg="Migration successfully executed" id="add index team_role.org_id" duration=1.762625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:17.021281214Z level=info msg="Executing migration" id="add unique index team_role_org_id_team_id_role_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:17.107078381Z level=info msg="Migration successfully executed" id="add unique index team_role_org_id_team_id_role_id" duration=85.795791ms
grafana-1          | logger=migrator t=2025-11-07T15:49:18.633337507Z level=info msg="Executing migration" id="add index team_role.team_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:18.634728965Z level=info msg="Migration successfully executed" id="add index team_role.team_id" duration=1.393084ms
grafana-1          | logger=migrator t=2025-11-07T15:49:18.92008384Z level=info msg="Executing migration" id="create user role table"
grafana-1          | logger=migrator t=2025-11-07T15:49:18.924935382Z level=info msg="Migration successfully executed" id="create user role table" duration=4.851875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:19.109672424Z level=info msg="Executing migration" id="add index user_role.org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:19.159971049Z level=info msg="Migration successfully executed" id="add index user_role.org_id" duration=50.266458ms
grafana-1          | logger=migrator t=2025-11-07T15:49:19.458405466Z level=info msg="Executing migration" id="add unique index user_role_org_id_user_id_role_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:19.460006591Z level=info msg="Migration successfully executed" id="add unique index user_role_org_id_user_id_role_id" duration=1.616375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:19.678683757Z level=info msg="Executing migration" id="add index user_role.user_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:19.680366632Z level=info msg="Migration successfully executed" id="add index user_role.user_id" duration=1.684875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:19.888167507Z level=info msg="Executing migration" id="create builtin role table"
grafana-1          | logger=migrator t=2025-11-07T15:49:19.890190632Z level=info msg="Migration successfully executed" id="create builtin role table" duration=2.023166ms
grafana-1          | logger=migrator t=2025-11-07T15:49:20.153853424Z level=info msg="Executing migration" id="add index builtin_role.role_id"
kafka-1            | 	log.cleaner.backoff.ms = 15000
kafka-1            | 	log.cleaner.dedupe.buffer.size = 134217728
grafana-1          | logger=migrator t=2025-11-07T15:49:20.155206383Z level=info msg="Migration successfully executed" id="add index builtin_role.role_id" duration=1.35475ms
grafana-1          | logger=migrator t=2025-11-07T15:49:20.278067716Z level=info msg="Executing migration" id="add index builtin_role.name"
grafana-1          | logger=migrator t=2025-11-07T15:49:20.284810633Z level=info msg="Migration successfully executed" id="add index builtin_role.name" duration=6.745291ms
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
kafka-1            | 	log.cleaner.delete.retention.ms = 86400000
kafka-1            | 	log.cleaner.enable = true
kafka-1            | 	log.cleaner.io.buffer.load.factor = 0.9
kafka-1            | 	log.cleaner.io.buffer.size = 524288
kafka-1            | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka-1            | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka-1            | 	log.cleaner.min.cleanable.ratio = 0.5
kafka-1            | 	log.cleaner.min.compaction.lag.ms = 0
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
grafana-1          | logger=migrator t=2025-11-07T15:49:20.441978466Z level=info msg="Executing migration" id="Add column org_id to builtin_role table"
grafana-1          | logger=migrator t=2025-11-07T15:49:20.476638258Z level=info msg="Migration successfully executed" id="Add column org_id to builtin_role table" duration=34.657917ms
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | 	log.cleaner.threads = 1
kafka-1            | 	log.cleanup.policy = [delete]
kafka-1            | 	log.dir = /tmp/kafka-logs
kafka-1            | 	log.dirs = /var/lib/kafka/data
kafka-1            | 	log.flush.interval.messages = 9223372036854775807
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	log.flush.interval.ms = null
kafka-1            | 	log.flush.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.flush.scheduler.interval.ms = 9223372036854775807
kafka-1            | 	log.flush.start.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.index.interval.bytes = 4096
kafka-1            | 	log.index.size.max.bytes = 10485760
kafka-1            | 	log.message.downconversion.enable = true
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:49:20.668729008Z level=info msg="Executing migration" id="add index builtin_role.org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:20.675445425Z level=info msg="Migration successfully executed" id="add index builtin_role.org_id" duration=6.717709ms
grafana-1          | logger=migrator t=2025-11-07T15:49:21.11951355Z level=info msg="Executing migration" id="add unique index builtin_role_org_id_role_id_role"
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:52.914Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:52.915Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":271}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:53.197Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:49:21.120411466Z level=info msg="Migration successfully executed" id="add unique index builtin_role_org_id_role_id_role" duration=898.666Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:21.513957717Z level=info msg="Executing migration" id="Remove unique index role_org_id_uid"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:53.199Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":524}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:53.735Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:53.735Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":994}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:54.742Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:54.745Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2340}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:57.100Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:41:57.101Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3796}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:00.910Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	log.message.format.version = 3.0-IV1
kafka-1            | 	log.message.timestamp.difference.max.ms = 9223372036854775807
grafana-1          | logger=migrator t=2025-11-07T15:49:21.515412133Z level=info msg="Migration successfully executed" id="Remove unique index role_org_id_uid" duration=1.455292ms
grafana-1          | logger=migrator t=2025-11-07T15:49:21.932166467Z level=info msg="Executing migration" id="add unique index role.uid"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
kafka-1            | 	log.message.timestamp.type = CreateTime
kafka-1            | 	log.preallocate = false
kafka-1            | 	log.retention.bytes = -1
kafka-1            | 	log.retention.check.interval.ms = 300000
kafka-1            | 	log.retention.hours = 168
grafana-1          | logger=migrator t=2025-11-07T15:49:21.939475717Z level=info msg="Migration successfully executed" id="add unique index role.uid" duration=7.311875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:22.289212967Z level=info msg="Executing migration" id="create seed assignment table"
grafana-1          | logger=migrator t=2025-11-07T15:49:22.289890425Z level=info msg="Migration successfully executed" id="create seed assignment table" duration=678.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:22.506368342Z level=info msg="Executing migration" id="add unique index builtin_role_role_name"
grafana-1          | logger=migrator t=2025-11-07T15:49:22.508269509Z level=info msg="Migration successfully executed" id="add unique index builtin_role_role_name" duration=1.925584ms
kafka-1            | 	log.retention.minutes = null
kafka-1            | 	log.retention.ms = null
kafka-1            | 	log.roll.hours = 168
grafana-1          | logger=migrator t=2025-11-07T15:49:22.769412801Z level=info msg="Executing migration" id="add column hidden to role table"
grafana-1          | logger=migrator t=2025-11-07T15:49:22.789727301Z level=info msg="Migration successfully executed" id="add column hidden to role table" duration=20.313708ms
grafana-1          | logger=migrator t=2025-11-07T15:49:22.980394176Z level=info msg="Executing migration" id="permission kind migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:22.990692551Z level=info msg="Migration successfully executed" id="permission kind migration" duration=10.296875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:23.251368551Z level=info msg="Executing migration" id="permission attribute migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:23.259642384Z level=info msg="Migration successfully executed" id="permission attribute migration" duration=8.273291ms
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:37.176Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:37.177Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":316}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:00.911Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6542}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:42:00.915Z"}
order-service-1    | Server is running on port 3001
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:37.505Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:37.507Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":654}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:38.172Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	log.roll.jitter.hours = 0
kafka-1            | 	log.roll.jitter.ms = null
kafka-1            | 	log.roll.ms = null
kafka-1            | 	log.segment.bytes = 1073741824
grafana-1          | logger=migrator t=2025-11-07T15:49:23.548216343Z level=info msg="Executing migration" id="permission identifier migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:23.562962718Z level=info msg="Migration successfully executed" id="permission identifier migration" duration=14.746167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:23.947623968Z level=info msg="Executing migration" id="add permission identifier index"
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:38.173Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1446}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:39.634Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:49:23.952273093Z level=info msg="Migration successfully executed" id="add permission identifier index" duration=4.650708ms
grafana-1          | logger=migrator t=2025-11-07T15:49:24.403394468Z level=info msg="Executing migration" id="add permission action scope role_id index"
grafana-1          | logger=migrator t=2025-11-07T15:49:24.408265176Z level=info msg="Migration successfully executed" id="add permission action scope role_id index" duration=4.871458ms
grafana-1          | logger=migrator t=2025-11-07T15:49:24.564515426Z level=info msg="Executing migration" id="remove permission role_id action scope index"
grafana-1          | logger=migrator t=2025-11-07T15:49:24.584579135Z level=info msg="Migration successfully executed" id="remove permission role_id action scope index" duration=20.063292ms
grafana-1          | logger=migrator t=2025-11-07T15:49:24.883621135Z level=info msg="Executing migration" id="add group mapping UID column to user_role table"
grafana-1          | logger=migrator t=2025-11-07T15:49:24.903070968Z level=info msg="Migration successfully executed" id="add group mapping UID column to user_role table" duration=19.470417ms
grafana-1          | logger=migrator t=2025-11-07T15:49:25.10278176Z level=info msg="Executing migration" id="add user_role org ID, user ID, role ID, group mapping UID index"
grafana-1          | logger=migrator t=2025-11-07T15:49:25.103690218Z level=info msg="Migration successfully executed" id="add user_role org ID, user ID, role ID, group mapping UID index" duration=909Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:25.214859885Z level=info msg="Executing migration" id="remove user_role org ID, user ID, role ID index"
kafka-1            | 	log.segment.delete.delay.ms = 60000
kafka-1            | 	max.connection.creation.rate = 2147483647
kafka-1            | 	max.connections = 2147483647
kafka-1            | 	max.connections.per.ip = 2147483647
kafka-1            | 	max.connections.per.ip.overrides = 
kafka-1            | 	max.incremental.fetch.session.cache.slots = 1000
kafka-1            | 	message.max.bytes = 1048588
kafka-1            | 	metadata.log.dir = null
kafka-1            | 	metadata.log.max.record.bytes.between.snapshots = 20971520
kafka-1            | 	metadata.log.segment.bytes = 1073741824
grafana-1          | logger=migrator t=2025-11-07T15:49:25.21867826Z level=info msg="Migration successfully executed" id="remove user_role org ID, user ID, role ID index" duration=3.818959ms
grafana-1          | logger=migrator t=2025-11-07T15:49:25.420541343Z level=info msg="Executing migration" id="add permission role_id action index"
kafka-1            | 	metadata.log.segment.min.bytes = 8388608
kafka-1            | 	metadata.log.segment.ms = 604800000
kafka-1            | 	metadata.max.retention.bytes = -1
kafka-1            | 	metadata.max.retention.ms = 604800000
kafka-1            | 	metric.reporters = []
kafka-1            | 	metrics.num.samples = 2
kafka-1            | 	metrics.recording.level = INFO
kafka-1            | 	metrics.sample.window.ms = 30000
kafka-1            | 	min.insync.replicas = 1
kafka-1            | 	node.id = -1
kafka-1            | 	num.io.threads = 8
kafka-1            | 	num.network.threads = 3
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:00.931Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:00.932Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":344}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:01.288Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:01.289Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":662}
kafka-1            | 	num.partitions = 1
kafka-1            | 	num.recovery.threads.per.data.dir = 1
kafka-1            | 	num.replica.alter.log.dirs.threads = null
kafka-1            | 	num.replica.fetchers = 1
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:39.635Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2500}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:42.152Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:42.154Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5570}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:47.738Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:47.739Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9926}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
kafka-1            | 	offset.metadata.max.bytes = 4096
kafka-1            | 	offsets.commit.required.acks = -1
kafka-1            | 	offsets.commit.timeout.ms = 5000
kafka-1            | 	offsets.load.buffer.size = 5242880
grafana-1          | logger=migrator t=2025-11-07T15:49:25.421937218Z level=info msg="Migration successfully executed" id="add permission role_id action index" duration=1.396792ms
grafana-1          | logger=migrator t=2025-11-07T15:49:25.598129969Z level=info msg="Executing migration" id="Remove permission role_id index"
grafana-1          | logger=migrator t=2025-11-07T15:49:25.616451052Z level=info msg="Migration successfully executed" id="Remove permission role_id index" duration=18.309417ms
grafana-1          | logger=migrator t=2025-11-07T15:49:25.88273451Z level=info msg="Executing migration" id="create query_history table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:25.891035635Z level=info msg="Migration successfully executed" id="create query_history table v1" duration=8.300792ms
grafana-1          | logger=migrator t=2025-11-07T15:49:26.14415801Z level=info msg="Executing migration" id="add index query_history.org_id-created_by-datasource_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:26.145039427Z level=info msg="Migration successfully executed" id="add index query_history.org_id-created_by-datasource_uid" duration=876Âµs
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:01.958Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:01.959Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1584}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:03.553Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:03.554Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2784}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:06.346Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:06.346Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5412}
grafana-1          | logger=migrator t=2025-11-07T15:49:26.332415886Z level=info msg="Executing migration" id="alter table query_history alter column created_by type to bigint"
grafana-1          | logger=migrator t=2025-11-07T15:49:26.332469469Z level=info msg="Migration successfully executed" id="alter table query_history alter column created_by type to bigint" duration=56.042Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:26.517037594Z level=info msg="Executing migration" id="create query_history_details table v1"
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
grafana-1          | logger=migrator t=2025-11-07T15:49:26.518130261Z level=info msg="Migration successfully executed" id="create query_history_details table v1" duration=1.09375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:26.702064969Z level=info msg="Executing migration" id="rbac disabled migrator"
grafana-1          | logger=migrator t=2025-11-07T15:49:26.702587802Z level=info msg="Migration successfully executed" id="rbac disabled migrator" duration=528.042Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:27.056775636Z level=info msg="Executing migration" id="teams permissions migration"
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:42:08 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:42:08.553Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:11.771Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:11.772Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12350}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 9926,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 12350,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
grafana-1          | logger=migrator t=2025-11-07T15:49:27.057899094Z level=info msg="Migration successfully executed" id="teams permissions migration" duration=1.125042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:27.340183428Z level=info msg="Executing migration" id="dashboard permissions"
kafka-1            | 	offsets.retention.check.interval.ms = 600000
kafka-1            | 	offsets.retention.minutes = 10080
kafka-1            | 	offsets.topic.compression.codec = 0
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
grafana-1          | logger=migrator t=2025-11-07T15:49:27.341429719Z level=info msg="Migration successfully executed" id="dashboard permissions" duration=1.24925ms
product-service-1  |     helpUrl: undefined,
kafka-1            | 	offsets.topic.num.partitions = 50
kafka-1            | 	offsets.topic.replication.factor = 1
kafka-1            | 	offsets.topic.segment.bytes = 104857600
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
grafana-1          | logger=migrator t=2025-11-07T15:49:27.482313886Z level=info msg="Executing migration" id="dashboard permissions uid scopes"
grafana-1          | logger=migrator t=2025-11-07T15:49:27.483132178Z level=info msg="Migration successfully executed" id="dashboard permissions uid scopes" duration=820Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:27.828923845Z level=info msg="Executing migration" id="drop managed folder create actions"
grafana-1          | logger=migrator t=2025-11-07T15:49:27.82929397Z level=info msg="Migration successfully executed" id="drop managed folder create actions" duration=372.5Âµs
kafka-1            | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka-1            | 	password.encoder.iterations = 4096
kafka-1            | 	password.encoder.key.length = 128
kafka-1            | 	password.encoder.keyfactory.algorithm = null
kafka-1            | 	password.encoder.old.secret = null
kafka-1            | 	password.encoder.secret = null
kafka-1            | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka-1            | 	process.roles = []
kafka-1            | 	producer.purgatory.purge.interval.requests = 1000
kafka-1            | 	queued.max.request.bytes = -1
kafka-1            | 	queued.max.requests = 500
kafka-1            | 	quota.window.num = 11
kafka-1            | 	quota.window.size.seconds = 1
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
kafka-1            | 	remote.log.index.file.cache.total.size.bytes = 1073741824
kafka-1            | 	remote.log.manager.task.interval.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.max.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.ms = 500
kafka-1            | 	remote.log.manager.task.retry.jitter = 0.2
product-service-1  | Startup error: KafkaJSNonRetriableError
kafka-1            | 	remote.log.manager.thread.pool.size = 10
kafka-1            | 	remote.log.metadata.manager.class.name = null
kafka-1            | 	remote.log.metadata.manager.class.path = null
kafka-1            | 	remote.log.metadata.manager.impl.prefix = null
kafka-1            | 	remote.log.metadata.manager.listener.name = null
kafka-1            | 	remote.log.reader.max.pending.tasks = 100
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
kafka-1            | 	remote.log.reader.threads = 10
kafka-1            | 	remote.log.storage.manager.class.name = null
kafka-1            | 	remote.log.storage.manager.class.path = null
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 9926,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | 	remote.log.storage.manager.impl.prefix = null
kafka-1            | 	remote.log.storage.system.enable = false
kafka-1            | 	replica.fetch.backoff.ms = 1000
kafka-1            | 	replica.fetch.max.bytes = 1048576
kafka-1            | 	replica.fetch.min.bytes = 1
kafka-1            | 	replica.fetch.response.max.bytes = 10485760
grafana-1          | logger=migrator t=2025-11-07T15:49:28.066631886Z level=info msg="Executing migration" id="alerting notification permissions"
grafana-1          | logger=migrator t=2025-11-07T15:49:28.070420261Z level=info msg="Migration successfully executed" id="alerting notification permissions" duration=3.786583ms
grafana-1          | logger=migrator t=2025-11-07T15:49:28.130381303Z level=info msg="Executing migration" id="create query_history_star table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:28.131898886Z level=info msg="Migration successfully executed" id="create query_history_star table v1" duration=1.519875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:28.327736053Z level=info msg="Executing migration" id="add index query_history.user_id-query_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:28.329137845Z level=info msg="Migration successfully executed" id="add index query_history.user_id-query_uid" duration=1.403792ms
grafana-1          | logger=migrator t=2025-11-07T15:49:28.576347845Z level=info msg="Executing migration" id="add column org_id in query_history_star"
grafana-1          | logger=migrator t=2025-11-07T15:49:28.593382178Z level=info msg="Migration successfully executed" id="add column org_id in query_history_star" duration=17.032541ms
grafana-1          | logger=migrator t=2025-11-07T15:49:28.832338803Z level=info msg="Executing migration" id="alter table query_history_star_mig column user_id type to bigint"
grafana-1          | logger=migrator t=2025-11-07T15:49:28.83236647Z level=info msg="Migration successfully executed" id="alter table query_history_star_mig column user_id type to bigint" duration=30.125Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:28.984744387Z level=info msg="Executing migration" id="create correlation table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:28.986264012Z level=info msg="Migration successfully executed" id="create correlation table v1" duration=1.520625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:29.26861047Z level=info msg="Executing migration" id="add index correlations.uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:29.270003845Z level=info msg="Migration successfully executed" id="add index correlations.uid" duration=1.394833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:29.489503762Z level=info msg="Executing migration" id="add index correlations.source_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:29.493584095Z level=info msg="Migration successfully executed" id="add index correlations.source_uid" duration=4.080209ms
grafana-1          | logger=migrator t=2025-11-07T15:49:29.865797971Z level=info msg="Executing migration" id="add correlation config column"
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ› ï¸  run anywhere with `dotenvx run -- yourcommand`
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:42:12.452Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
kafka-1            | 	replica.fetch.wait.max.ms = 500
kafka-1            | 	replica.high.watermark.checkpoint.interval.ms = 5000
kafka-1            | 	replica.lag.time.max.ms = 30000
kafka-1            | 	replica.selector.class = null
kafka-1            | 	replica.socket.receive.buffer.bytes = 65536
kafka-1            | 	replica.socket.timeout.ms = 30000
kafka-1            | 	replication.quota.window.num = 11
grafana-1          | logger=migrator t=2025-11-07T15:49:29.877862387Z level=info msg="Migration successfully executed" id="add correlation config column" duration=12.075167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:30.062520554Z level=info msg="Executing migration" id="drop index IDX_correlation_uid - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:30.067865596Z level=info msg="Migration successfully executed" id="drop index IDX_correlation_uid - v1" duration=5.343208ms
grafana-1          | logger=migrator t=2025-11-07T15:49:30.188968971Z level=info msg="Executing migration" id="drop index IDX_correlation_source_uid - v1"
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
grafana-1          | logger=migrator t=2025-11-07T15:49:30.193654471Z level=info msg="Migration successfully executed" id="drop index IDX_correlation_source_uid - v1" duration=4.684209ms
grafana-1          | logger=migrator t=2025-11-07T15:49:30.404587596Z level=info msg="Executing migration" id="Rename table correlation to correlation_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:30.560844679Z level=info msg="Migration successfully executed" id="Rename table correlation to correlation_tmp_qwerty - v1" duration=156.253542ms
kafka-1            | 	replication.quota.window.size.seconds = 1
kafka-1            | 	request.timeout.ms = 30000
kafka-1            | 	reserved.broker.max.id = 1000
kafka-1            | 	sasl.client.callback.handler.class = null
kafka-1            | 	sasl.enabled.mechanisms = [GSSAPI]
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:49:30.613593471Z level=info msg="Executing migration" id="create correlation v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:30.625379679Z level=info msg="Migration successfully executed" id="create correlation v2" duration=11.783625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:31.064757096Z level=info msg="Executing migration" id="create index IDX_correlation_uid - v2"
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:40:48.489Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
grafana-1          | logger=migrator t=2025-11-07T15:49:31.068833096Z level=info msg="Migration successfully executed" id="create index IDX_correlation_uid - v2" duration=4.076083ms
grafana-1          | logger=migrator t=2025-11-07T15:49:31.46796718Z level=info msg="Executing migration" id="create index IDX_correlation_source_uid - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:31.469349596Z level=info msg="Migration successfully executed" id="create index IDX_correlation_source_uid - v2" duration=1.383375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:31.680861763Z level=info msg="Executing migration" id="create index IDX_correlation_org_id - v2"
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=migrator t=2025-11-07T15:49:31.682287513Z level=info msg="Migration successfully executed" id="create index IDX_correlation_org_id - v2" duration=1.42125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:32.063651055Z level=info msg="Executing migration" id="copy correlation v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:32.063909972Z level=info msg="Migration successfully executed" id="copy correlation v1 to v2" duration=260Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:32.524667472Z level=info msg="Executing migration" id="drop correlation_tmp_qwerty"
grafana-1          | logger=migrator t=2025-11-07T15:49:32.525814805Z level=info msg="Migration successfully executed" id="drop correlation_tmp_qwerty" duration=1.147375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:32.751832305Z level=info msg="Executing migration" id="add provisioning column"
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
kafka-1            | 	sasl.jaas.config = null
kafka-1            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka-1            | 	sasl.kerberos.min.time.before.relogin = 60000
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
grafana-1          | logger=migrator t=2025-11-07T15:49:32.760742055Z level=info msg="Migration successfully executed" id="add provisioning column" duration=8.908917ms
grafana-1          | logger=migrator t=2025-11-07T15:49:33.061480555Z level=info msg="Executing migration" id="add type column"
grafana-1          | logger=migrator t=2025-11-07T15:49:33.141606472Z level=info msg="Migration successfully executed" id="add type column" duration=80.121ms
kafka-1            | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka-1            | 	sasl.kerberos.service.name = null
kafka-1            | 	sasl.kerberos.ticket.renew.jitter = 0.05
kafka-1            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
kafka-1            | 	sasl.login.callback.handler.class = null
kafka-1            | 	sasl.login.class = null
kafka-1            | 	sasl.login.refresh.buffer.seconds = 300
kafka-1            | 	sasl.login.refresh.min.period.seconds = 60
kafka-1            | 	sasl.login.refresh.window.factor = 0.8
kafka-1            | 	sasl.login.refresh.window.jitter = 0.05
kafka-1            | 	sasl.mechanism.controller.protocol = GSSAPI
kafka-1            | 	sasl.mechanism.inter.broker.protocol = GSSAPI
kafka-1            | 	sasl.server.callback.handler.class = null
kafka-1            | 	security.inter.broker.protocol = PLAINTEXT
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:12.560Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:12.561Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":263}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:12.840Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:12.841Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":546}
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	security.providers = null
kafka-1            | 	socket.connection.setup.timeout.max.ms = 30000
grafana-1          | logger=migrator t=2025-11-07T15:49:33.434342708Z level=info msg="Executing migration" id="create entity_events table"
grafana-1          | logger=migrator t=2025-11-07T15:49:33.435525583Z level=info msg="Migration successfully executed" id="create entity_events table" duration=1.184125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:33.739127375Z level=info msg="Executing migration" id="create dashboard public config v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:33.74051675Z level=info msg="Migration successfully executed" id="create dashboard public config v1" duration=1.390167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:34.009984292Z level=info msg="Executing migration" id="drop index UQE_dashboard_public_config_uid - v1"
kafka-1            | 	socket.connection.setup.timeout.ms = 10000
kafka-1            | 	socket.receive.buffer.bytes = 102400
kafka-1            | 	socket.request.max.bytes = 104857600
kafka-1            | 	socket.send.buffer.bytes = 102400
kafka-1            | 	ssl.cipher.suites = []
kafka-1            | 	ssl.client.auth = none
kafka-1            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka-1            | 	ssl.endpoint.identification.algorithm = https
kafka-1            | 	ssl.engine.factory.class = null
kafka-1            | 	ssl.key.password = null
kafka-1            | 	ssl.keymanager.algorithm = SunX509
kafka-1            | 	ssl.keystore.certificate.chain = null
kafka-1            | 	ssl.keystore.key = null
kafka-1            | 	ssl.keystore.location = null
kafka-1            | 	ssl.keystore.password = null
kafka-1            | 	ssl.keystore.type = JKS
kafka-1            | 	ssl.principal.mapping.rules = DEFAULT
kafka-1            | 	ssl.protocol = TLSv1.3
kafka-1            | 	ssl.provider = null
kafka-1            | 	ssl.secure.random.implementation = null
kafka-1            | 	ssl.trustmanager.algorithm = PKIX
kafka-1            | 	ssl.truststore.certificates = null
kafka-1            | 	ssl.truststore.location = null
kafka-1            | 	ssl.truststore.password = null
kafka-1            | 	ssl.truststore.type = JKS
kafka-1            | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
grafana-1          | logger=migrator t=2025-11-07T15:49:34.010378583Z level=warn msg="Skipping migration: Already executed, but not recorded in migration log" id="drop index UQE_dashboard_public_config_uid - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:34.353506333Z level=info msg="Executing migration" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:34.354165042Z level=warn msg="Skipping migration: Already executed, but not recorded in migration log" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:34.670643292Z level=info msg="Executing migration" id="Drop old dashboard public config table"
grafana-1          | logger=migrator t=2025-11-07T15:49:34.67175875Z level=info msg="Migration successfully executed" id="Drop old dashboard public config table" duration=1.117ms
grafana-1          | logger=migrator t=2025-11-07T15:49:35.041421167Z level=info msg="Executing migration" id="recreate dashboard public config v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:35.042804834Z level=info msg="Migration successfully executed" id="recreate dashboard public config v1" duration=1.384625ms
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:13.399Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:13.400Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1034}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:14.443Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	transaction.max.timeout.ms = 900000
kafka-1            | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
kafka-1            | 	transaction.state.log.load.buffer.size = 5242880
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:14.444Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1912}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:16.363Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:16.363Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4262}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:20.637Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:20.638Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9880}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:42:20.640Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:20.652Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:20.653Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":352}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:21.018Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:21.018Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":688}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:21.719Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:21.720Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1448}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:23.183Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:23.184Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3394}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:42:23 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:42:23.547Z"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:26.593Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:26.595Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6958}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:33.569Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:33.570Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":15818}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 15818,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | 	transaction.state.log.min.isr = 2
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:42:34.433Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:48.580Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:48.581Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":295}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:48.882Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:48.883Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":644}
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:49.538Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:49.538Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1330}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:50.881Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:50.883Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2184}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:53.077Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:53.078Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4552}
grafana-1          | logger=migrator t=2025-11-07T15:49:35.234755292Z level=info msg="Executing migration" id="create index UQE_dashboard_public_config_uid - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:35.237332209Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_public_config_uid - v1" duration=2.577583ms
kafka-1            | 	transaction.state.log.num.partitions = 50
kafka-1            | 	transaction.state.log.replication.factor = 3
kafka-1            | 	transaction.state.log.segment.bytes = 104857600
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:57.638Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:57.639Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10168}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
grafana-1          | logger=migrator t=2025-11-07T15:49:35.558102334Z level=info msg="Executing migration" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:35.558890667Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v1" duration=789.125Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:35.799358126Z level=info msg="Executing migration" id="drop index UQE_dashboard_public_config_uid - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:35.800834834Z level=info msg="Migration successfully executed" id="drop index UQE_dashboard_public_config_uid - v2" duration=1.476458ms
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10168,
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2025-11-07T15:49:35.983142167Z level=info msg="Executing migration" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:35.984503209Z level=info msg="Migration successfully executed" id="drop index IDX_dashboard_public_config_org_id_dashboard_uid - v2" duration=1.361459ms
grafana-1          | logger=migrator t=2025-11-07T15:49:36.320121376Z level=info msg="Executing migration" id="Drop public config table"
grafana-1          | logger=migrator t=2025-11-07T15:49:36.321165751Z level=info msg="Migration successfully executed" id="Drop public config table" duration=1.045542ms
grafana-1          | logger=migrator t=2025-11-07T15:49:36.670465834Z level=info msg="Executing migration" id="Recreate dashboard public config v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:36.673412459Z level=info msg="Migration successfully executed" id="Recreate dashboard public config v2" duration=2.954166ms
grafana-1          | logger=migrator t=2025-11-07T15:49:36.964914793Z level=info msg="Executing migration" id="create index UQE_dashboard_public_config_uid - v2"
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
grafana-1          | logger=migrator t=2025-11-07T15:49:36.966756585Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_public_config_uid - v2" duration=1.840708ms
grafana-1          | logger=migrator t=2025-11-07T15:49:37.20518021Z level=info msg="Executing migration" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:37.207464876Z level=info msg="Migration successfully executed" id="create index IDX_dashboard_public_config_org_id_dashboard_uid - v2" duration=2.28725ms
kafka-1            | 	transactional.id.expiration.ms = 604800000
kafka-1            | 	unclean.leader.election.enable = false
kafka-1            | 	zookeeper.clientCnxnSocket = null
kafka-1            | 	zookeeper.connect = zookeeper:2181
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
kafka-1            | 	zookeeper.connection.timeout.ms = null
kafka-1            | 	zookeeper.max.in.flight.requests = 10
kafka-1            | 	zookeeper.session.timeout.ms = 18000
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:34.526Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:34.527Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":306}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:34.841Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:34.842Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":664}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:35.519Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:35.520Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1218}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:36.751Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:36.753Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2132}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:38.894Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:38.894Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4356}
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
grafana-1          | logger=migrator t=2025-11-07T15:49:37.486483877Z level=info msg="Executing migration" id="create index UQE_dashboard_public_config_access_token - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:37.495028752Z level=info msg="Migration successfully executed" id="create index UQE_dashboard_public_config_access_token - v2" duration=8.54425ms
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:43.262Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:43.264Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9844}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:42:43.266Z"}
order-service-1    | Server is running on port 3001
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
kafka-1            | 	zookeeper.set.acl = false
kafka-1            | 	zookeeper.ssl.cipher.suites = null
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:43.279Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:43.279Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":244}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:43.539Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:43.541Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":450}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:44.005Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:44.006Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":946}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:44.960Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	zookeeper.ssl.client.enable = false
kafka-1            | 	zookeeper.ssl.crl.enable = false
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:44.961Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2124}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:47.097Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:47.098Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4730}
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10168,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
kafka-1            | 	zookeeper.ssl.enabled.protocols = null
kafka-1            | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka-1            | 	zookeeper.ssl.keystore.location = null
kafka-1            | 	zookeeper.ssl.keystore.password = null
grafana-1          | logger=migrator t=2025-11-07T15:49:37.839070877Z level=info msg="Executing migration" id="Rename table dashboard_public_config to dashboard_public - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:37.92368271Z level=info msg="Migration successfully executed" id="Rename table dashboard_public_config to dashboard_public - v2" duration=84.609166ms
grafana-1          | logger=migrator t=2025-11-07T15:49:38.221910419Z level=info msg="Executing migration" id="add annotations_enabled column"
grafana-1          | logger=migrator t=2025-11-07T15:49:38.288941085Z level=info msg="Migration successfully executed" id="add annotations_enabled column" duration=67.030083ms
grafana-1          | logger=migrator t=2025-11-07T15:49:38.444669127Z level=info msg="Executing migration" id="add time_selection_enabled column"
grafana-1          | logger=migrator t=2025-11-07T15:49:38.46288796Z level=info msg="Migration successfully executed" id="add time_selection_enabled column" duration=18.215959ms
grafana-1          | logger=migrator t=2025-11-07T15:49:38.702408752Z level=info msg="Executing migration" id="delete orphaned public dashboards"
grafana-1          | logger=migrator t=2025-11-07T15:49:38.703090085Z level=info msg="Migration successfully executed" id="delete orphaned public dashboards" duration=682.958Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:38.785620919Z level=info msg="Executing migration" id="add share column"
grafana-1          | logger=migrator t=2025-11-07T15:49:38.906948627Z level=info msg="Migration successfully executed" id="add share column" duration=121.313917ms
grafana-1          | logger=migrator t=2025-11-07T15:49:39.131165586Z level=info msg="Executing migration" id="backfill empty share column fields with default of public"
grafana-1          | logger=migrator t=2025-11-07T15:49:39.131442919Z level=info msg="Migration successfully executed" id="backfill empty share column fields with default of public" duration=278.583Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:39.522183919Z level=info msg="Executing migration" id="create file table"
grafana-1          | logger=migrator t=2025-11-07T15:49:39.523045294Z level=info msg="Migration successfully executed" id="create file table" duration=842.291Âµs
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
grafana-1          | logger=migrator t=2025-11-07T15:49:39.657921211Z level=info msg="Executing migration" id="file table idx: path natural pk"
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
grafana-1          | logger=migrator t=2025-11-07T15:49:39.660544794Z level=info msg="Migration successfully executed" id="file table idx: path natural pk" duration=2.623583ms
grafana-1          | logger=migrator t=2025-11-07T15:49:39.719360544Z level=info msg="Executing migration" id="file table idx: parent_folder_path_hash fast folder retrieval"
grafana-1          | logger=migrator t=2025-11-07T15:49:39.722491544Z level=info msg="Migration successfully executed" id="file table idx: parent_folder_path_hash fast folder retrieval" duration=3.122708ms
grafana-1          | logger=migrator t=2025-11-07T15:49:39.764867711Z level=info msg="Executing migration" id="create file_meta table"
grafana-1          | logger=migrator t=2025-11-07T15:49:39.767867503Z level=info msg="Migration successfully executed" id="create file_meta table" duration=3.0085ms
grafana-1          | logger=migrator t=2025-11-07T15:49:39.812425794Z level=info msg="Executing migration" id="file table idx: path key"
grafana-1          | logger=migrator t=2025-11-07T15:49:39.813274294Z level=info msg="Migration successfully executed" id="file table idx: path key" duration=849.916Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:40.206757919Z level=info msg="Executing migration" id="set path collation in file table"
grafana-1          | logger=migrator t=2025-11-07T15:49:40.206808919Z level=info msg="Migration successfully executed" id="set path collation in file table" duration=57.416Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:40.431680128Z level=info msg="Executing migration" id="migrate contents column to mediumblob for MySQL"
grafana-1          | logger=migrator t=2025-11-07T15:49:40.431733003Z level=info msg="Migration successfully executed" id="migrate contents column to mediumblob for MySQL" duration=55.958Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:40.731236253Z level=info msg="Executing migration" id="managed permissions migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:40.732142045Z level=info msg="Migration successfully executed" id="managed permissions migration" duration=916.208Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:40.997138045Z level=info msg="Executing migration" id="managed folder permissions alert actions migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:40.99741242Z level=info msg="Migration successfully executed" id="managed folder permissions alert actions migration" duration=275.167Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:41.114411753Z level=info msg="Executing migration" id="RBAC action name migrator"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.220041628Z level=info msg="Migration successfully executed" id="RBAC action name migrator" duration=105.626583ms
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:51.839Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:51.841Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10508}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  enable debug logging with { debug: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:40:58.332Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	zookeeper.ssl.keystore.type = null
kafka-1            | 	zookeeper.ssl.ocsp.enable = false
kafka-1            | 	zookeeper.ssl.protocol = TLSv1.2
kafka-1            | 	zookeeper.ssl.truststore.location = null
kafka-1            | 	zookeeper.ssl.truststore.password = null
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
grafana-1          | logger=migrator t=2025-11-07T15:49:41.441724795Z level=info msg="Executing migration" id="Add UID column to playlist"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.49360942Z level=info msg="Migration successfully executed" id="Add UID column to playlist" duration=76.089875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:41.514290628Z level=info msg="Executing migration" id="Update uid column values in playlist"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.514527503Z level=info msg="Migration successfully executed" id="Update uid column values in playlist" duration=237.667Âµs
kafka-1            | 	zookeeper.ssl.truststore.type = null
kafka-1            | 	zookeeper.sync.time.ms = 2000
kafka-1            |  (kafka.server.KafkaConfig)
kafka-1            | [2025-11-07 15:49:38,169] INFO KafkaConfig values: 
kafka-1            | 	advertised.listeners = PLAINTEXT://kafka:9092
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 10508,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:58.428Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:58.429Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":241}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:58.684Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:58.685Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":450}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:59.151Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
kafka-1            | 	alter.config.policy.class.name = null
kafka-1            | 	alter.log.dirs.replication.quota.window.num = 11
kafka-1            | 	alter.log.dirs.replication.quota.window.size.seconds = 1
kafka-1            | 	authorizer.class.name = 
kafka-1            | 	auto.create.topics.enable = true
kafka-1            | 	auto.leader.rebalance.enable = true
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:40:59.152Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":870}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:00.038Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:00.039Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2004}
kafka-1            | 	background.threads = 10
grafana-1          | logger=migrator t=2025-11-07T15:49:41.519006503Z level=info msg="Executing migration" id="Add index for uid in playlist"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.519950128Z level=info msg="Migration successfully executed" id="Add index for uid in playlist" duration=944.542Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:41.522929587Z level=info msg="Executing migration" id="update group index for alert rules"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.52463517Z level=info msg="Migration successfully executed" id="update group index for alert rules" duration=1.705833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:41.527174087Z level=info msg="Executing migration" id="managed folder permissions alert actions repeated migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.52744267Z level=info msg="Migration successfully executed" id="managed folder permissions alert actions repeated migration" duration=264.041Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:41.529363378Z level=info msg="Executing migration" id="admin only folder/dashboard permission"
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.055Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:02.056Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3966}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:06.037Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:06.038Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8570}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:49:41.530828587Z level=info msg="Migration successfully executed" id="admin only folder/dashboard permission" duration=1.462833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:41.53280517Z level=info msg="Executing migration" id="add action column to seed_assignment"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.541967753Z level=info msg="Migration successfully executed" id="add action column to seed_assignment" duration=9.163042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:41.593374503Z level=info msg="Executing migration" id="add scope column to seed_assignment"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.61460692Z level=info msg="Migration successfully executed" id="add scope column to seed_assignment" duration=21.233542ms
grafana-1          | logger=migrator t=2025-11-07T15:49:41.942908087Z level=info msg="Executing migration" id="remove unique index builtin_role_role_name before nullable update"
grafana-1          | logger=migrator t=2025-11-07T15:49:41.94435117Z level=info msg="Migration successfully executed" id="remove unique index builtin_role_role_name before nullable update" duration=1.444958ms
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
grafana-1          | logger=migrator t=2025-11-07T15:49:42.239341629Z level=info msg="Executing migration" id="update seed_assignment role_name column to nullable"
grafana-1          | logger=migrator t=2025-11-07T15:49:42.602912379Z level=info msg="Migration successfully executed" id="update seed_assignment role_name column to nullable" duration=363.557084ms
grafana-1          | logger=migrator t=2025-11-07T15:49:42.935421296Z level=info msg="Executing migration" id="add unique index builtin_role_name back"
grafana-1          | logger=migrator t=2025-11-07T15:49:42.939398254Z level=info msg="Migration successfully executed" id="add unique index builtin_role_name back" duration=3.974917ms
grafana-1          | logger=migrator t=2025-11-07T15:49:43.116939379Z level=info msg="Executing migration" id="add unique index builtin_role_action_scope"
grafana-1          | logger=migrator t=2025-11-07T15:49:43.117887713Z level=info msg="Migration successfully executed" id="add unique index builtin_role_action_scope" duration=949.5Âµs
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8570,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
grafana-1          | logger=migrator t=2025-11-07T15:49:43.453110796Z level=info msg="Executing migration" id="add primary key to seed_assigment"
grafana-1          | logger=migrator t=2025-11-07T15:49:43.635106671Z level=info msg="Migration successfully executed" id="add primary key to seed_assigment" duration=181.994875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:43.839462755Z level=info msg="Executing migration" id="add origin column to seed_assignment"
grafana-1          | logger=migrator t=2025-11-07T15:49:43.868773088Z level=info msg="Migration successfully executed" id="add origin column to seed_assignment" duration=29.299042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:43.975489421Z level=info msg="Executing migration" id="add origin to plugin seed_assignment"
grafana-1          | logger=migrator t=2025-11-07T15:49:43.976234963Z level=info msg="Migration successfully executed" id="add origin to plugin seed_assignment" duration=745Âµs
kafka-1            | 	broker.heartbeat.interval.ms = 2000
kafka-1            | 	broker.id = 1
kafka-1            | 	broker.id.generation.enable = true
kafka-1            | 	broker.rack = null
grafana-1          | logger=migrator t=2025-11-07T15:49:44.05255363Z level=info msg="Executing migration" id="prevent seeding OnCall access"
grafana-1          | logger=migrator t=2025-11-07T15:49:44.052767838Z level=info msg="Migration successfully executed" id="prevent seeding OnCall access" duration=216.166Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:44.243503296Z level=info msg="Executing migration" id="managed folder permissions alert actions repeated fixed migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:44.243861755Z level=info msg="Migration successfully executed" id="managed folder permissions alert actions repeated fixed migration" duration=352.25Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:44.498636297Z level=info msg="Executing migration" id="managed folder permissions library panel actions migration"
kafka-1            | 	broker.session.timeout.ms = 9000
kafka-1            | 	client.quota.callback.class = null
kafka-1            | 	compression.type = producer
kafka-1            | 	connection.failed.authentication.delay.ms = 100
kafka-1            | 	connections.max.idle.ms = 600000
kafka-1            | 	connections.max.reauth.ms = 0
kafka-1            | 	control.plane.listener.name = null
kafka-1            | 	controlled.shutdown.enable = true
kafka-1            | 	controlled.shutdown.max.retries = 3
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
grafana-1          | logger=migrator t=2025-11-07T15:49:44.498896297Z level=info msg="Migration successfully executed" id="managed folder permissions library panel actions migration" duration=261.083Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:44.60054388Z level=info msg="Executing migration" id="migrate external alertmanagers to datsourcse"
grafana-1          | logger=migrator t=2025-11-07T15:49:44.60088388Z level=info msg="Migration successfully executed" id="migrate external alertmanagers to datsourcse" duration=340.958Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:45.029063672Z level=info msg="Executing migration" id="create folder table"
grafana-1          | logger=migrator t=2025-11-07T15:49:45.029907088Z level=info msg="Migration successfully executed" id="create folder table" duration=844Âµs
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
kafka-1            | 	controlled.shutdown.retry.backoff.ms = 5000
kafka-1            | 	controller.listener.names = null
kafka-1            | 	controller.quorum.append.linger.ms = 25
kafka-1            | 	controller.quorum.election.backoff.max.ms = 1000
kafka-1            | 	controller.quorum.election.timeout.ms = 1000
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:42:52.564Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
kafka-1            | 	controller.quorum.fetch.timeout.ms = 2000
kafka-1            | 	controller.quorum.request.timeout.ms = 2000
kafka-1            | 	controller.quorum.retry.backoff.ms = 20
kafka-1            | 	controller.quorum.voters = []
kafka-1            | 	controller.quota.window.num = 11
kafka-1            | 	controller.quota.window.size.seconds = 1
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:49:45.062911255Z level=info msg="Executing migration" id="Add index for parent_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:45.070045297Z level=info msg="Migration successfully executed" id="Add index for parent_uid" duration=7.134ms
grafana-1          | logger=migrator t=2025-11-07T15:49:45.300221922Z level=info msg="Executing migration" id="Add unique index for folder.uid and folder.org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:45.302323297Z level=info msg="Migration successfully executed" id="Add unique index for folder.uid and folder.org_id" duration=2.101ms
grafana-1          | logger=migrator t=2025-11-07T15:49:45.583549047Z level=info msg="Executing migration" id="Update folder title length"
kafka-1            | 	controller.socket.timeout.ms = 30000
kafka-1            | 	create.topic.policy.class.name = null
kafka-1            | 	default.replication.factor = 1
grafana-1          | logger=migrator t=2025-11-07T15:49:45.584122089Z level=info msg="Migration successfully executed" id="Update folder title length" duration=574.709Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:45.680714172Z level=info msg="Executing migration" id="Add unique index for folder.title and folder.parent_uid"
kafka-1            | 	delegation.token.expiry.check.interval.ms = 3600000
kafka-1            | 	delegation.token.expiry.time.ms = 86400000
kafka-1            | 	delegation.token.master.key = null
kafka-1            | 	delegation.token.max.lifetime.ms = 604800000
kafka-1            | 	delegation.token.secret.key = null
grafana-1          | logger=migrator t=2025-11-07T15:49:45.705141797Z level=info msg="Migration successfully executed" id="Add unique index for folder.title and folder.parent_uid" duration=24.428833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:46.175633214Z level=info msg="Executing migration" id="Remove unique index for folder.title and folder.parent_uid"
kafka-1            | 	delete.records.purgatory.purge.interval.requests = 1
kafka-1            | 	delete.topic.enable = true
kafka-1            | 	fetch.max.bytes = 57671680
kafka-1            | 	fetch.purgatory.purge.interval.requests = 1000
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
kafka-1            | 	group.initial.rebalance.delay.ms = 0
kafka-1            | 	group.max.session.timeout.ms = 1800000
kafka-1            | 	group.max.size = 2147483647
kafka-1            | 	group.min.session.timeout.ms = 6000
kafka-1            | 	initial.broker.registration.timeout.ms = 60000
kafka-1            | 	inter.broker.listener.name = null
product-service-1  |   retryTime: 8570,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
grafana-1          | logger=migrator t=2025-11-07T15:49:46.180677839Z level=info msg="Migration successfully executed" id="Remove unique index for folder.title and folder.parent_uid" duration=5.045083ms
grafana-1          | logger=migrator t=2025-11-07T15:49:46.674634339Z level=info msg="Executing migration" id="Add unique index for title, parent_uid, and org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:46.676703714Z level=info msg="Migration successfully executed" id="Add unique index for title, parent_uid, and org_id" duration=2.082834ms
grafana-1          | logger=migrator t=2025-11-07T15:49:47.105613089Z level=info msg="Executing migration" id="Sync dashboard and folder table"
grafana-1          | logger=migrator t=2025-11-07T15:49:47.106094756Z level=info msg="Migration successfully executed" id="Sync dashboard and folder table" duration=482.5Âµs
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	inter.broker.protocol.version = 3.0-IV1
kafka-1            | 	kafka.metrics.polling.interval.secs = 10
kafka-1            | 	kafka.metrics.reporters = []
grafana-1          | logger=migrator t=2025-11-07T15:49:47.362985631Z level=info msg="Executing migration" id="Remove ghost folders from the folder table"
grafana-1          | logger=migrator t=2025-11-07T15:49:47.36343084Z level=info msg="Migration successfully executed" id="Remove ghost folders from the folder table" duration=445.667Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:47.650432173Z level=info msg="Executing migration" id="Remove unique index UQE_folder_uid_org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:47.65941209Z level=info msg="Migration successfully executed" id="Remove unique index UQE_folder_uid_org_id" duration=8.980584ms
grafana-1          | logger=migrator t=2025-11-07T15:49:47.76255984Z level=info msg="Executing migration" id="Add unique index UQE_folder_org_id_uid"
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ› ï¸  run anywhere with `dotenvx run -- yourcommand`
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:41:06.920Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
kafka-1            | 	leader.imbalance.check.interval.seconds = 300
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	leader.imbalance.per.broker.percentage = 10
kafka-1            | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
kafka-1            | 	listeners = PLAINTEXT://0.0.0.0:9092
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	log.cleaner.backoff.ms = 15000
kafka-1            | 	log.cleaner.dedupe.buffer.size = 134217728
kafka-1            | 	log.cleaner.delete.retention.ms = 86400000
kafka-1            | 	log.cleaner.enable = true
kafka-1            | 	log.cleaner.io.buffer.load.factor = 0.9
kafka-1            | 	log.cleaner.io.buffer.size = 524288
kafka-1            | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
grafana-1          | logger=migrator t=2025-11-07T15:49:47.764098173Z level=info msg="Migration successfully executed" id="Add unique index UQE_folder_org_id_uid" duration=1.546541ms
grafana-1          | logger=migrator t=2025-11-07T15:49:48.018565298Z level=info msg="Executing migration" id="Remove unique index UQE_folder_title_parent_uid_org_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:48.020091423Z level=info msg="Migration successfully executed" id="Remove unique index UQE_folder_title_parent_uid_org_id" duration=1.520791ms
grafana-1          | logger=migrator t=2025-11-07T15:49:48.395649548Z level=info msg="Executing migration" id="Add unique index UQE_folder_org_id_parent_uid_title"
grafana-1          | logger=migrator t=2025-11-07T15:49:48.397095507Z level=info msg="Migration successfully executed" id="Add unique index UQE_folder_org_id_parent_uid_title" duration=1.447125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:48.743679965Z level=info msg="Executing migration" id="Remove index IDX_folder_parent_uid_org_id"
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
kafka-1            | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka-1            | 	log.cleaner.min.cleanable.ratio = 0.5
kafka-1            | 	log.cleaner.min.compaction.lag.ms = 0
kafka-1            | 	log.cleaner.threads = 1
kafka-1            | 	log.cleanup.policy = [delete]
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:49:48.74529209Z level=info msg="Migration successfully executed" id="Remove index IDX_folder_parent_uid_org_id" duration=1.61275ms
grafana-1          | logger=migrator t=2025-11-07T15:49:48.961264299Z level=info msg="Executing migration" id="Remove unique index UQE_folder_org_id_parent_uid_title"
grafana-1          | logger=migrator t=2025-11-07T15:49:48.962765465Z level=info msg="Migration successfully executed" id="Remove unique index UQE_folder_org_id_parent_uid_title" duration=1.502917ms
grafana-1          | logger=migrator t=2025-11-07T15:49:49.12797309Z level=info msg="Executing migration" id="Add index IDX_folder_org_id_parent_uid"
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
kafka-1            | 	log.dir = /tmp/kafka-logs
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:52.646Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:52.647Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":310}
kafka-1            | 	log.dirs = /var/lib/kafka/data
kafka-1            | 	log.flush.interval.messages = 9223372036854775807
kafka-1            | 	log.flush.interval.ms = null
kafka-1            | 	log.flush.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.flush.scheduler.interval.ms = 9223372036854775807
kafka-1            | 	log.flush.start.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.index.interval.bytes = 4096
kafka-1            | 	log.index.size.max.bytes = 10485760
kafka-1            | 	log.message.downconversion.enable = true
kafka-1            | 	log.message.format.version = 3.0-IV1
kafka-1            | 	log.message.timestamp.difference.max.ms = 9223372036854775807
kafka-1            | 	log.message.timestamp.type = CreateTime
kafka-1            | 	log.preallocate = false
kafka-1            | 	log.retention.bytes = -1
kafka-1            | 	log.retention.check.interval.ms = 300000
kafka-1            | 	log.retention.hours = 168
kafka-1            | 	log.retention.minutes = null
kafka-1            | 	log.retention.ms = null
kafka-1            | 	log.roll.hours = 168
kafka-1            | 	log.roll.jitter.hours = 0
kafka-1            | 	log.roll.jitter.ms = null
kafka-1            | 	log.roll.ms = null
kafka-1            | 	log.segment.bytes = 1073741824
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
kafka-1            | 	log.segment.delete.delay.ms = 60000
kafka-1            | 	max.connection.creation.rate = 2147483647
kafka-1            | 	max.connections = 2147483647
kafka-1            | 	max.connections.per.ip = 2147483647
kafka-1            | 	max.connections.per.ip.overrides = 
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:52.968Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:52.970Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":722}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:53.703Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:53.703Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1506}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:55.216Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	max.incremental.fetch.session.cache.slots = 1000
kafka-1            | 	message.max.bytes = 1048588
kafka-1            | 	metadata.log.dir = null
kafka-1            | 	metadata.log.max.record.bytes.between.snapshots = 20971520
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:55.216Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3480}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:58.709Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:42:58.710Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":7318}
kafka-1            | 	metadata.log.segment.bytes = 1073741824
kafka-1            | 	metadata.log.segment.min.bytes = 8388608
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.045Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.047Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":15468}
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=migrator t=2025-11-07T15:49:49.164770174Z level=info msg="Migration successfully executed" id="Add index IDX_folder_org_id_parent_uid" duration=36.798375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:49.368035049Z level=info msg="Executing migration" id="create anon_device table"
grafana-1          | logger=migrator t=2025-11-07T15:49:49.371420424Z level=info msg="Migration successfully executed" id="create anon_device table" duration=3.384417ms
grafana-1          | logger=migrator t=2025-11-07T15:49:49.585518632Z level=info msg="Executing migration" id="add unique index anon_device.device_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:49.587055591Z level=info msg="Migration successfully executed" id="add unique index anon_device.device_id" duration=1.557167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:49.796088549Z level=info msg="Executing migration" id="add index anon_device.updated_at"
product-service-1  | Database synced
grafana-1          | logger=migrator t=2025-11-07T15:49:49.798530924Z level=info msg="Migration successfully executed" id="add index anon_device.updated_at" duration=2.655209ms
grafana-1          | logger=migrator t=2025-11-07T15:49:49.982657132Z level=info msg="Executing migration" id="create signing_key table"
grafana-1          | logger=migrator t=2025-11-07T15:49:49.989413966Z level=info msg="Migration successfully executed" id="create signing_key table" duration=6.767334ms
grafana-1          | logger=migrator t=2025-11-07T15:49:50.144932341Z level=info msg="Executing migration" id="add unique index signing_key.key_id"
grafana-1          | logger=migrator t=2025-11-07T15:49:50.147002174Z level=info msg="Migration successfully executed" id="add unique index signing_key.key_id" duration=2.071ms
kafka-1            | 	metadata.log.segment.ms = 604800000
kafka-1            | 	metadata.max.retention.bytes = -1
kafka-1            | 	metadata.max.retention.ms = 604800000
kafka-1            | 	metric.reporters = []
kafka-1            | 	metrics.num.samples = 2
kafka-1            | 	metrics.recording.level = INFO
kafka-1            | 	metrics.sample.window.ms = 30000
kafka-1            | 	min.insync.replicas = 1
grafana-1          | logger=migrator t=2025-11-07T15:49:50.553445216Z level=info msg="Executing migration" id="set legacy alert migration status in kvstore"
grafana-1          | logger=migrator t=2025-11-07T15:49:50.555912591Z level=info msg="Migration successfully executed" id="set legacy alert migration status in kvstore" duration=2.473625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:50.741991466Z level=info msg="Executing migration" id="migrate record of created folders during legacy migration to kvstore"
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.005Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.006Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":270}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.293Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.294Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":648}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.956Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:07.957Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1064}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:09.037Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	node.id = -1
kafka-1            | 	num.io.threads = 8
kafka-1            | 	num.network.threads = 3
kafka-1            | 	num.partitions = 1
kafka-1            | 	num.recovery.threads.per.data.dir = 1
kafka-1            | 	num.replica.alter.log.dirs.threads = null
grafana-1          | logger=migrator t=2025-11-07T15:49:50.744321883Z level=info msg="Migration successfully executed" id="migrate record of created folders during legacy migration to kvstore" duration=2.332125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:50.761684841Z level=info msg="Executing migration" id="Add folder_uid for dashboard"
grafana-1          | logger=migrator t=2025-11-07T15:49:50.774246133Z level=info msg="Migration successfully executed" id="Add folder_uid for dashboard" duration=12.55725ms
grafana-1          | logger=migrator t=2025-11-07T15:49:50.785494591Z level=info msg="Executing migration" id="Populate dashboard folder_uid column"
grafana-1          | logger=migrator t=2025-11-07T15:49:50.78706105Z level=info msg="Migration successfully executed" id="Populate dashboard folder_uid column" duration=1.570375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:50.793387216Z level=info msg="Executing migration" id="Add unique index for dashboard_org_id_folder_uid_title"
kafka-1            | 	num.replica.fetchers = 1
kafka-1            | 	offset.metadata.max.bytes = 4096
kafka-1            | 	offsets.commit.required.acks = -1
kafka-1            | 	offsets.commit.timeout.ms = 5000
kafka-1            | 	offsets.load.buffer.size = 5242880
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:43:06.050Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.065Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.066Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":246}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.320Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.321Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":400}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:09.038Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2542}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:11.594Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.731Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.731Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":912}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:07.651Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:07.651Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2166}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:43:08 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:43:08.554Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:09.827Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:09.828Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5100}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:14.946Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:49:50.7997503Z level=info msg="Migration successfully executed" id="Add unique index for dashboard_org_id_folder_uid_title" duration=6.363584ms
grafana-1          | logger=migrator t=2025-11-07T15:49:50.819027091Z level=info msg="Executing migration" id="Delete unique index for dashboard_org_id_folder_id_title"
grafana-1          | logger=migrator t=2025-11-07T15:49:50.827218508Z level=info msg="Migration successfully executed" id="Delete unique index for dashboard_org_id_folder_id_title" duration=8.16225ms
grafana-1          | logger=migrator t=2025-11-07T15:49:50.850389258Z level=info msg="Executing migration" id="Delete unique index for dashboard_org_id_folder_uid_title"
grafana-1          | logger=migrator t=2025-11-07T15:49:50.850592341Z level=info msg="Migration successfully executed" id="Delete unique index for dashboard_org_id_folder_uid_title" duration=207.75Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:50.988299925Z level=info msg="Executing migration" id="Add unique index for dashboard_org_id_folder_uid_title_is_folder"
grafana-1          | logger=migrator t=2025-11-07T15:49:50.992722716Z level=info msg="Migration successfully executed" id="Add unique index for dashboard_org_id_folder_uid_title_is_folder" duration=4.422375ms
grafana-1          | logger=migrator t=2025-11-07T15:49:51.010923425Z level=info msg="Executing migration" id="Restore index for dashboard_org_id_folder_id_title"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.012818258Z level=info msg="Migration successfully executed" id="Restore index for dashboard_org_id_folder_id_title" duration=1.89725ms
grafana-1          | logger=migrator t=2025-11-07T15:49:51.056860216Z level=info msg="Executing migration" id="Remove unique index for dashboard_org_id_folder_uid_title_is_folder"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.058945508Z level=info msg="Migration successfully executed" id="Remove unique index for dashboard_org_id_folder_uid_title_is_folder" duration=2.085417ms
grafana-1          | logger=migrator t=2025-11-07T15:49:51.1550958Z level=info msg="Executing migration" id="create sso_setting table"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.15699705Z level=info msg="Migration successfully executed" id="create sso_setting table" duration=1.901416ms
grafana-1          | logger=migrator t=2025-11-07T15:49:51.338995841Z level=info msg="Executing migration" id="copy kvstore migration status to each org"
kafka-1            | 	offsets.retention.check.interval.ms = 600000
kafka-1            | 	offsets.retention.minutes = 10080
kafka-1            | 	offsets.topic.compression.codec = 0
kafka-1            | 	offsets.topic.num.partitions = 50
kafka-1            | 	offsets.topic.replication.factor = 1
kafka-1            | 	offsets.topic.segment.bytes = 104857600
kafka-1            | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:14.947Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9932}
order-service-1    | node:internal/process/promises:288
kafka-1            | 	password.encoder.iterations = 4096
grafana-1          | logger=migrator t=2025-11-07T15:49:51.34839455Z level=info msg="Migration successfully executed" id="copy kvstore migration status to each org" duration=9.400667ms
grafana-1          | logger=migrator t=2025-11-07T15:49:51.389052883Z level=info msg="Executing migration" id="add back entry for orgid=0 migrated status"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.389653591Z level=info msg="Migration successfully executed" id="add back entry for orgid=0 migrated status" duration=603.625Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:51.426477342Z level=info msg="Executing migration" id="managed dashboard permissions annotation actions migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.427459675Z level=info msg="Migration successfully executed" id="managed dashboard permissions annotation actions migration" duration=984.792Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:51.522640175Z level=info msg="Executing migration" id="create cloud_migration table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.523450342Z level=info msg="Migration successfully executed" id="create cloud_migration table v1" duration=810.417Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:51.625446717Z level=info msg="Executing migration" id="create cloud_migration_run table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.626282675Z level=info msg="Migration successfully executed" id="create cloud_migration_run table v1" duration=836.875Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:51.695924467Z level=info msg="Executing migration" id="add stack_id column"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.721797383Z level=info msg="Migration successfully executed" id="add stack_id column" duration=25.851041ms
grafana-1          | logger=migrator t=2025-11-07T15:49:51.89812205Z level=info msg="Executing migration" id="add region_slug column"
grafana-1          | logger=migrator t=2025-11-07T15:49:51.938883633Z level=info msg="Migration successfully executed" id="add region_slug column" duration=40.757959ms
grafana-1          | logger=migrator t=2025-11-07T15:49:52.19744555Z level=info msg="Executing migration" id="add cluster_slug column"
grafana-1          | logger=migrator t=2025-11-07T15:49:52.203684175Z level=info msg="Migration successfully executed" id="add cluster_slug column" duration=6.239833ms
grafana-1          | logger=migrator t=2025-11-07T15:49:52.28002705Z level=info msg="Executing migration" id="add migration uid column"
grafana-1          | logger=migrator t=2025-11-07T15:49:52.297121592Z level=info msg="Migration successfully executed" id="add migration uid column" duration=17.078875ms
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:11.596Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4422}
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
grafana-1          | logger=migrator t=2025-11-07T15:49:52.359919217Z level=info msg="Executing migration" id="Update uid column values for migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:52.369341884Z level=info msg="Migration successfully executed" id="Update uid column values for migration" duration=9.419459ms
grafana-1          | logger=migrator t=2025-11-07T15:49:52.484292634Z level=info msg="Executing migration" id="Add unique index migration_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:52.485223259Z level=info msg="Migration successfully executed" id="Add unique index migration_uid" duration=934.75Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:52.584901259Z level=info msg="Executing migration" id="add migration run uid column"
grafana-1          | logger=migrator t=2025-11-07T15:49:52.594356592Z level=info msg="Migration successfully executed" id="add migration run uid column" duration=9.4545ms
grafana-1          | logger=migrator t=2025-11-07T15:49:52.660933092Z level=info msg="Executing migration" id="Update uid column values for migration run"
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:16.033Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:16.036Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8198}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 9932,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ› ï¸  run anywhere with `dotenvx run -- yourcommand`
grafana-1          | logger=migrator t=2025-11-07T15:49:52.661141842Z level=info msg="Migration successfully executed" id="Update uid column values for migration run" duration=210Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:52.723691592Z level=info msg="Executing migration" id="Add unique index migration_run_uid"
grafana-1          | logger=migrator t=2025-11-07T15:49:52.738333134Z level=info msg="Migration successfully executed" id="Add unique index migration_run_uid" duration=14.640792ms
grafana-1          | logger=migrator t=2025-11-07T15:49:52.860471926Z level=info msg="Executing migration" id="Rename table cloud_migration to cloud_migration_session_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:52.919967509Z level=info msg="Migration successfully executed" id="Rename table cloud_migration to cloud_migration_session_tmp_qwerty - v1" duration=59.469167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:53.050333176Z level=info msg="Executing migration" id="create cloud_migration_session v2"
kafka-1            | 	password.encoder.key.length = 128
kafka-1            | 	password.encoder.keyfactory.algorithm = null
kafka-1            | 	password.encoder.old.secret = null
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:43:15.673Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | 	password.encoder.secret = null
kafka-1            | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka-1            | 	process.roles = []
kafka-1            | 	producer.purgatory.purge.interval.requests = 1000
kafka-1            | 	queued.max.request.bytes = -1
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8198,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
grafana-1          | logger=migrator t=2025-11-07T15:49:53.051968051Z level=info msg="Migration successfully executed" id="create cloud_migration_session v2" duration=1.632791ms
grafana-1          | logger=migrator t=2025-11-07T15:49:53.155532634Z level=info msg="Executing migration" id="create index UQE_cloud_migration_session_uid - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.156655134Z level=info msg="Migration successfully executed" id="create index UQE_cloud_migration_session_uid - v2" duration=1.113583ms
grafana-1          | logger=migrator t=2025-11-07T15:49:53.249244384Z level=info msg="Executing migration" id="copy cloud_migration_session v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.250720759Z level=info msg="Migration successfully executed" id="copy cloud_migration_session v1 to v2" duration=1.474583ms
grafana-1          | logger=migrator t=2025-11-07T15:49:53.283119801Z level=info msg="Executing migration" id="drop cloud_migration_session_tmp_qwerty"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.285369176Z level=info msg="Migration successfully executed" id="drop cloud_migration_session_tmp_qwerty" duration=2.252791ms
grafana-1          | logger=migrator t=2025-11-07T15:49:53.367487301Z level=info msg="Executing migration" id="Rename table cloud_migration_run to cloud_migration_snapshot_tmp_qwerty - v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.422294592Z level=info msg="Migration successfully executed" id="Rename table cloud_migration_run to cloud_migration_snapshot_tmp_qwerty - v1" duration=54.762917ms
grafana-1          | logger=migrator t=2025-11-07T15:49:53.478494259Z level=info msg="Executing migration" id="create cloud_migration_snapshot v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.479315801Z level=info msg="Migration successfully executed" id="create cloud_migration_snapshot v2" duration=822.583Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:53.546968426Z level=info msg="Executing migration" id="create index UQE_cloud_migration_snapshot_uid - v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.548919218Z level=info msg="Migration successfully executed" id="create index UQE_cloud_migration_snapshot_uid - v2" duration=1.95275ms
grafana-1          | logger=migrator t=2025-11-07T15:49:53.726995301Z level=info msg="Executing migration" id="copy cloud_migration_snapshot v1 to v2"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.727557801Z level=info msg="Migration successfully executed" id="copy cloud_migration_snapshot v1 to v2" duration=562.75Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:53.897283634Z level=info msg="Executing migration" id="drop cloud_migration_snapshot_tmp_qwerty"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.898116301Z level=info msg="Migration successfully executed" id="drop cloud_migration_snapshot_tmp_qwerty" duration=833.583Âµs
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:49:53.964138759Z level=info msg="Executing migration" id="add snapshot upload_url column"
grafana-1          | logger=migrator t=2025-11-07T15:49:53.998511426Z level=info msg="Migration successfully executed" id="add snapshot upload_url column" duration=34.335916ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.043626551Z level=info msg="Executing migration" id="add snapshot status column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.062546801Z level=info msg="Migration successfully executed" id="add snapshot status column" duration=18.916959ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.129826176Z level=info msg="Executing migration" id="add snapshot local_directory column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.143271134Z level=info msg="Migration successfully executed" id="add snapshot local_directory column" duration=13.44475ms
kafka-1            | 	queued.max.requests = 500
kafka-1            | 	quota.window.num = 11
kafka-1            | 	quota.window.size.seconds = 1
kafka-1            | 	remote.log.index.file.cache.total.size.bytes = 1073741824
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
kafka-1            | 	remote.log.manager.task.interval.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.max.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.ms = 500
kafka-1            | 	remote.log.manager.task.retry.jitter = 0.2
kafka-1            | 	remote.log.manager.thread.pool.size = 10
kafka-1            | 	remote.log.metadata.manager.class.name = null
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2025-11-07T15:49:54.189017593Z level=info msg="Executing migration" id="add snapshot gms_snapshot_uid column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.213181051Z level=info msg="Migration successfully executed" id="add snapshot gms_snapshot_uid column" duration=24.15975ms
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
grafana-1          | logger=migrator t=2025-11-07T15:49:54.292377593Z level=info msg="Executing migration" id="add snapshot encryption_key column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.309613885Z level=info msg="Migration successfully executed" id="add snapshot encryption_key column" duration=17.2455ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.340564926Z level=info msg="Executing migration" id="add snapshot error_string column"
kafka-1            | 	remote.log.metadata.manager.class.path = null
kafka-1            | 	remote.log.metadata.manager.impl.prefix = null
kafka-1            | 	remote.log.metadata.manager.listener.name = null
kafka-1            | 	remote.log.reader.max.pending.tasks = 100
kafka-1            | 	remote.log.reader.threads = 10
kafka-1            | 	remote.log.storage.manager.class.name = null
kafka-1            | 	remote.log.storage.manager.class.path = null
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
kafka-1            | 	remote.log.storage.manager.impl.prefix = null
kafka-1            | 	remote.log.storage.system.enable = false
kafka-1            | 	replica.fetch.backoff.ms = 1000
kafka-1            | 	replica.fetch.max.bytes = 1048576
kafka-1            | 	replica.fetch.min.bytes = 1
grafana-1          | logger=migrator t=2025-11-07T15:49:54.354003926Z level=info msg="Migration successfully executed" id="add snapshot error_string column" duration=13.43525ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.385378301Z level=info msg="Executing migration" id="create cloud_migration_resource table v1"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.386723843Z level=info msg="Migration successfully executed" id="create cloud_migration_resource table v1" duration=1.345541ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.423541551Z level=info msg="Executing migration" id="delete cloud_migration_snapshot.result column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.47402251Z level=info msg="Migration successfully executed" id="delete cloud_migration_snapshot.result column" duration=50.480708ms
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
kafka-1            | 	replica.fetch.response.max.bytes = 10485760
kafka-1            | 	replica.fetch.wait.max.ms = 500
kafka-1            | 	replica.high.watermark.checkpoint.interval.ms = 5000
kafka-1            | 	replica.lag.time.max.ms = 30000
kafka-1            | 	replica.selector.class = null
kafka-1            | 	replica.socket.receive.buffer.bytes = 65536
kafka-1            | 	replica.socket.timeout.ms = 30000
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
kafka-1            | 	replication.quota.window.num = 11
kafka-1            | 	replication.quota.window.size.seconds = 1
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:49:54.505546718Z level=info msg="Executing migration" id="add cloud_migration_resource.name column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.51299726Z level=info msg="Migration successfully executed" id="add cloud_migration_resource.name column" duration=7.451ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.531518343Z level=info msg="Executing migration" id="add cloud_migration_resource.parent_name column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.547519718Z level=info msg="Migration successfully executed" id="add cloud_migration_resource.parent_name column" duration=15.997209ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.610826385Z level=info msg="Executing migration" id="add cloud_migration_session.org_id column"
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8198,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
grafana-1          | logger=migrator t=2025-11-07T15:49:54.618076135Z level=info msg="Migration successfully executed" id="add cloud_migration_session.org_id column" duration=7.250417ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.675868968Z level=info msg="Executing migration" id="add cloud_migration_resource.error_code column"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.690616968Z level=info msg="Migration successfully executed" id="add cloud_migration_resource.error_code column" duration=14.747958ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.739499635Z level=info msg="Executing migration" id="increase resource_uid column length"
grafana-1          | logger=migrator t=2025-11-07T15:49:54.73954026Z level=info msg="Migration successfully executed" id="increase resource_uid column length" duration=43.042Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:54.81195626Z level=info msg="Executing migration" id="create cloud_migration_snapshot_partition table v1"
kafka-1            | 	request.timeout.ms = 30000
kafka-1            | 	reserved.broker.max.id = 1000
kafka-1            | 	sasl.client.callback.handler.class = null
kafka-1            | 	sasl.enabled.mechanisms = [GSSAPI]
kafka-1            | 	sasl.jaas.config = null
kafka-1            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:15.773Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:15.774Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":274}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:16.059Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:49:54.814096468Z level=info msg="Migration successfully executed" id="create cloud_migration_snapshot_partition table v1" duration=2.139667ms
grafana-1          | logger=migrator t=2025-11-07T15:49:54.998658677Z level=info msg="Executing migration" id="add cloud_migration_snapshot_partition srp_unique index"
grafana-1          | logger=migrator t=2025-11-07T15:49:55.000190885Z level=info msg="Migration successfully executed" id="add cloud_migration_snapshot_partition srp_unique index" duration=1.537917ms
grafana-1          | logger=migrator t=2025-11-07T15:49:55.08225151Z level=info msg="Executing migration" id="add resource_storage_type column to cloud_migration_snapshot table"
grafana-1          | logger=migrator t=2025-11-07T15:49:55.110844593Z level=info msg="Migration successfully executed" id="add resource_storage_type column to cloud_migration_snapshot table" duration=28.588125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:55.289660135Z level=info msg="Executing migration" id="add encryption_algo column to cloud_migration_snapshot table"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:16.060Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":534}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:16.603Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:16.604Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":920}
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
grafana-1          | logger=migrator t=2025-11-07T15:49:55.300473427Z level=info msg="Migration successfully executed" id="add encryption_algo column to cloud_migration_snapshot table" duration=10.810166ms
grafana-1          | logger=migrator t=2025-11-07T15:49:55.470429302Z level=info msg="Executing migration" id="add metadata column to cloud_migration_snapshot table"
grafana-1          | logger=migrator t=2025-11-07T15:49:55.49361676Z level=info msg="Migration successfully executed" id="add metadata column to cloud_migration_snapshot table" duration=23.184708ms
grafana-1          | logger=migrator t=2025-11-07T15:49:55.64341001Z level=info msg="Executing migration" id="add public_key column to cloud_migration_snapshot table"
grafana-1          | logger=migrator t=2025-11-07T15:49:55.657865844Z level=info msg="Migration successfully executed" id="add public_key column to cloud_migration_snapshot table" duration=14.453625ms
grafana-1          | logger=migrator t=2025-11-07T15:49:55.887297552Z level=info msg="Executing migration" id="alter kv_store.value to longtext"
grafana-1          | logger=migrator t=2025-11-07T15:49:55.887322302Z level=info msg="Migration successfully executed" id="alter kv_store.value to longtext" duration=28.459Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:56.197709885Z level=info msg="Executing migration" id="add notification_settings column to alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:56.219103177Z level=info msg="Migration successfully executed" id="add notification_settings column to alert_rule table" duration=21.390084ms
grafana-1          | logger=migrator t=2025-11-07T15:49:56.385938511Z level=info msg="Executing migration" id="add notification_settings column to alert_rule_version table"
grafana-1          | logger=migrator t=2025-11-07T15:49:56.406857261Z level=info msg="Migration successfully executed" id="add notification_settings column to alert_rule_version table" duration=20.917959ms
grafana-1          | logger=migrator t=2025-11-07T15:49:56.443501719Z level=info msg="Executing migration" id="removing scope from alert.instances:read action migration"
kafka-1            | 	sasl.kerberos.min.time.before.relogin = 60000
kafka-1            | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:18.284Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:18.321Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2182}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:30.043Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:30.044Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5178}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.255Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.263Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10660}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:43:35.282Z"}
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.458Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.459Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":244}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.716Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.718Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":484}
kafka-1            | 	sasl.kerberos.service.name = null
kafka-1            | 	sasl.kerberos.ticket.renew.jitter = 0.05
kafka-1            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
kafka-1            | 	sasl.login.callback.handler.class = null
kafka-1            | 	sasl.login.class = null
kafka-1            | 	sasl.login.refresh.buffer.seconds = 300
kafka-1            | 	sasl.login.refresh.min.period.seconds = 60
kafka-1            | 	sasl.login.refresh.window.factor = 0.8
kafka-1            | 	sasl.login.refresh.window.jitter = 0.05
kafka-1            | 	sasl.mechanism.controller.protocol = GSSAPI
kafka-1            | 	sasl.mechanism.inter.broker.protocol = GSSAPI
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:36.214Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:36.215Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":978}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:37.207Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:37.208Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1726}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:38.946Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:38.948Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3204}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.161Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.161Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6314}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
grafana-1          | logger=migrator t=2025-11-07T15:49:56.444172469Z level=info msg="Migration successfully executed" id="removing scope from alert.instances:read action migration" duration=673.708Âµs
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  override existing env vars with { override: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:41:17.379Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
kafka-1            | 	sasl.server.callback.handler.class = null
kafka-1            | 	security.inter.broker.protocol = PLAINTEXT
kafka-1            | 	security.providers = null
grafana-1          | logger=migrator t=2025-11-07T15:49:56.504341094Z level=info msg="Executing migration" id="managed folder permissions alerting silences actions migration"
grafana-1          | logger=migrator t=2025-11-07T15:49:56.504855677Z level=info msg="Migration successfully executed" id="managed folder permissions alerting silences actions migration" duration=517.208Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:56.595289511Z level=info msg="Executing migration" id="add record column to alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:56.611312552Z level=info msg="Migration successfully executed" id="add record column to alert_rule table" duration=16.019708ms
grafana-1          | logger=migrator t=2025-11-07T15:49:56.756570052Z level=info msg="Executing migration" id="add record column to alert_rule_version table"
grafana-1          | logger=migrator t=2025-11-07T15:49:56.777869427Z level=info msg="Migration successfully executed" id="add record column to alert_rule_version table" duration=21.298542ms
grafana-1          | logger=migrator t=2025-11-07T15:49:56.820987344Z level=info msg="Executing migration" id="add resolved_at column to alert_instance table"
grafana-1          | logger=migrator t=2025-11-07T15:49:56.835679344Z level=info msg="Migration successfully executed" id="add resolved_at column to alert_instance table" duration=14.692458ms
grafana-1          | logger=migrator t=2025-11-07T15:49:56.893209677Z level=info msg="Executing migration" id="add last_sent_at column to alert_instance table"
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 6314,
kafka-1            | 	socket.connection.setup.timeout.max.ms = 30000
kafka-1            | 	socket.connection.setup.timeout.ms = 10000
kafka-1            | 	socket.receive.buffer.bytes = 102400
kafka-1            | 	socket.request.max.bytes = 104857600
kafka-1            | 	socket.send.buffer.bytes = 102400
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
kafka-1            | 	ssl.cipher.suites = []
kafka-1            | 	ssl.client.auth = none
kafka-1            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka-1            | 	ssl.endpoint.identification.algorithm = https
grafana-1          | logger=migrator t=2025-11-07T15:49:56.961664802Z level=info msg="Migration successfully executed" id="add last_sent_at column to alert_instance table" duration=68.451667ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.001965802Z level=info msg="Executing migration" id="Add scope to alert.notifications.receivers:read and alert.notifications.receivers.secrets:read"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.003225427Z level=info msg="Migration successfully executed" id="Add scope to alert.notifications.receivers:read and alert.notifications.receivers.secrets:read" duration=1.259958ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.018460136Z level=info msg="Executing migration" id="add metadata column to alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.032917261Z level=info msg="Migration successfully executed" id="add metadata column to alert_rule table" duration=14.441666ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.038746136Z level=info msg="Executing migration" id="add metadata column to alert_rule_version table"
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
grafana-1          | logger=migrator t=2025-11-07T15:49:57.052760761Z level=info msg="Migration successfully executed" id="add metadata column to alert_rule_version table" duration=14.011291ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.063693678Z level=info msg="Executing migration" id="delete orphaned service account permissions"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.067082386Z level=info msg="Migration successfully executed" id="delete orphaned service account permissions" duration=2.997334ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.076856803Z level=info msg="Executing migration" id="adding action set permissions"
kafka-1            | 	ssl.engine.factory.class = null
kafka-1            | 	ssl.key.password = null
kafka-1            | 	ssl.keymanager.algorithm = SunX509
kafka-1            | 	ssl.keystore.certificate.chain = null
kafka-1            | 	ssl.keystore.key = null
kafka-1            | 	ssl.keystore.location = null
kafka-1            | 	ssl.keystore.password = null
grafana-1          | logger=migrator t=2025-11-07T15:49:57.086490053Z level=info msg="Migration successfully executed" id="adding action set permissions" duration=9.635875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.097402553Z level=info msg="Executing migration" id="create user_external_session table"
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
grafana-1          | logger=migrator t=2025-11-07T15:49:57.102050386Z level=info msg="Migration successfully executed" id="create user_external_session table" duration=4.650125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.111300803Z level=info msg="Executing migration" id="increase name_id column length to 1024"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.111334719Z level=info msg="Migration successfully executed" id="increase name_id column length to 1024" duration=34.917Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:57.122927136Z level=info msg="Executing migration" id="increase session_id column length to 1024"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.122965886Z level=info msg="Migration successfully executed" id="increase session_id column length to 1024" duration=41.292Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:57.134823344Z level=info msg="Executing migration" id="remove scope from alert.notifications.receivers:create"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.147825219Z level=info msg="Migration successfully executed" id="remove scope from alert.notifications.receivers:create" duration=13.000041ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.252329678Z level=info msg="Executing migration" id="add created_by column to alert_rule_version table"
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
kafka-1            | 	ssl.keystore.type = JKS
kafka-1            | 	ssl.principal.mapping.rules = DEFAULT
kafka-1            | 	ssl.protocol = TLSv1.3
kafka-1            | 	ssl.provider = null
kafka-1            | 	ssl.secure.random.implementation = null
kafka-1            | 	ssl.trustmanager.algorithm = PKIX
kafka-1            | 	ssl.truststore.certificates = null
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | 	ssl.truststore.location = null
kafka-1            | 	ssl.truststore.password = null
kafka-1            | 	ssl.truststore.type = JKS
kafka-1            | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
kafka-1            | 	transaction.max.timeout.ms = 900000
kafka-1            | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
kafka-1            | 	transaction.state.log.load.buffer.size = 5242880
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:43:42.793Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2025-11-07T15:49:57.368888094Z level=info msg="Migration successfully executed" id="add created_by column to alert_rule_version table" duration=117.339083ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.828929886Z level=info msg="Executing migration" id="add updated_by column to alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.860709178Z level=info msg="Migration successfully executed" id="add updated_by column to alert_rule table" duration=31.776459ms
grafana-1          | logger=migrator t=2025-11-07T15:49:57.963097761Z level=info msg="Executing migration" id="add alert_rule_state table"
grafana-1          | logger=migrator t=2025-11-07T15:49:57.971114761Z level=info msg="Migration successfully executed" id="add alert_rule_state table" duration=7.988167ms
grafana-1          | logger=migrator t=2025-11-07T15:49:58.088141345Z level=info msg="Executing migration" id="add index to alert_rule_state on org_id and rule_uid columns"
kafka-1            | 	transaction.state.log.min.isr = 2
kafka-1            | 	transaction.state.log.num.partitions = 50
kafka-1            | 	transaction.state.log.replication.factor = 3
kafka-1            | 	transaction.state.log.segment.bytes = 104857600
grafana-1          | logger=migrator t=2025-11-07T15:49:58.090526011Z level=info msg="Migration successfully executed" id="add index to alert_rule_state on org_id and rule_uid columns" duration=2.252125ms
grafana-1          | logger=migrator t=2025-11-07T15:49:58.213164845Z level=info msg="Executing migration" id="add guid column to alert_rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:58.231226761Z level=info msg="Migration successfully executed" id="add guid column to alert_rule table" duration=18.072791ms
grafana-1          | logger=migrator t=2025-11-07T15:49:58.317506928Z level=info msg="Executing migration" id="add rule_guid column to alert_rule_version table"
kafka-1            | 	transactional.id.expiration.ms = 604800000
kafka-1            | 	unclean.leader.election.enable = false
kafka-1            | 	zookeeper.clientCnxnSocket = null
kafka-1            | 	zookeeper.connect = zookeeper:2181
kafka-1            | 	zookeeper.connection.timeout.ms = null
kafka-1            | 	zookeeper.max.in.flight.requests = 10
kafka-1            | 	zookeeper.session.timeout.ms = 18000
grafana-1          | logger=migrator t=2025-11-07T15:49:58.334527886Z level=info msg="Migration successfully executed" id="add rule_guid column to alert_rule_version table" duration=17.015041ms
grafana-1          | logger=migrator t=2025-11-07T15:49:58.436607595Z level=info msg="Executing migration" id="cleanup alert_rule_version table"
grafana-1          | logger=migrator t=2025-11-07T15:49:58.43668872Z level=info msg="Rule version record limit is not set, fallback to 100" limit=0
grafana-1          | logger=migrator t=2025-11-07T15:49:58.437126178Z level=info msg="Cleaning up table `alert_rule_version`" batchSize=50 batches=0 keepVersions=100
grafana-1          | logger=migrator t=2025-11-07T15:49:58.437151262Z level=info msg="Migration successfully executed" id="cleanup alert_rule_version table" duration=545Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:58.567280887Z level=info msg="Executing migration" id="populate rule guid in alert rule table"
grafana-1          | logger=migrator t=2025-11-07T15:49:58.568171262Z level=info msg="Migration successfully executed" id="populate rule guid in alert rule table" duration=891Âµs
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	zookeeper.set.acl = false
kafka-1            | 	zookeeper.ssl.cipher.suites = null
kafka-1            | 	zookeeper.ssl.client.enable = false
kafka-1            | 	zookeeper.ssl.crl.enable = false
kafka-1            | 	zookeeper.ssl.enabled.protocols = null
kafka-1            | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka-1            | 	zookeeper.ssl.keystore.location = null
kafka-1            | 	zookeeper.ssl.keystore.password = null
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	zookeeper.ssl.keystore.type = null
kafka-1            | 	zookeeper.ssl.ocsp.enable = false
kafka-1            | 	zookeeper.ssl.protocol = TLSv1.2
kafka-1            | 	zookeeper.ssl.truststore.location = null
kafka-1            | 	zookeeper.ssl.truststore.password = null
kafka-1            | 	zookeeper.ssl.truststore.type = null
kafka-1            | 	zookeeper.sync.time.ms = 2000
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=migrator t=2025-11-07T15:49:58.641042262Z level=info msg="Executing migration" id="drop index in alert_rule_version table on rule_org_id, rule_uid and version columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:58.648365095Z level=info msg="Migration successfully executed" id="drop index in alert_rule_version table on rule_org_id, rule_uid and version columns" duration=7.30325ms
grafana-1          | logger=migrator t=2025-11-07T15:49:58.75374822Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_org_id, rule_uid, rule_guid and version columns"
kafka-1            |  (kafka.server.KafkaConfig)
kafka-1            | [2025-11-07 15:49:38,707] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2025-11-07 15:49:38,713] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2025-11-07 15:49:38,714] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2025-11-07 15:49:38,860] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2025-11-07 15:49:39,133] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:49:39,140] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:49:39,374] INFO Loaded 0 logs in 240ms. (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:49:39,443] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:49:39,466] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:49:39,556] INFO Starting the log cleaner (kafka.log.LogCleaner)
kafka-1            | [2025-11-07 15:49:39,860] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
kafka-1            | [2025-11-07 15:49:44,020] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
grafana-1          | logger=migrator t=2025-11-07T15:49:58.755288928Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_org_id, rule_uid, rule_guid and version columns" duration=1.541583ms
grafana-1          | logger=migrator t=2025-11-07T15:49:58.879808678Z level=info msg="Executing migration" id="add index in alert_rule_version table on rule_guid and version columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:58.881611053Z level=info msg="Migration successfully executed" id="add index in alert_rule_version table on rule_guid and version columns" duration=1.802375ms
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.906Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.908Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":359}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:43.279Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:43.280Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":584}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:43.877Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:43.877Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1210}
kafka-1            | [2025-11-07 15:49:48,071] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
kafka-1            | [2025-11-07 15:49:48,179] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:45.101Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:45.103Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2340}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:47.453Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:47.454Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4052}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:51.518Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:51.519Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8026}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:43:51.520Z"}
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:17.467Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=migrator t=2025-11-07T15:49:58.940781428Z level=info msg="Executing migration" id="add index in alert_rule table on guid columns"
grafana-1          | logger=migrator t=2025-11-07T15:49:58.943029553Z level=info msg="Migration successfully executed" id="add index in alert_rule table on guid columns" duration=2.242083ms
grafana-1          | logger=migrator t=2025-11-07T15:49:59.017473178Z level=info msg="Executing migration" id="add keep_firing_for column to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.032567053Z level=info msg="Migration successfully executed" id="add keep_firing_for column to alert_rule" duration=15.091667ms
grafana-1          | logger=migrator t=2025-11-07T15:49:59.115168429Z level=info msg="Executing migration" id="add keep_firing_for column to alert_rule_version"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.128362679Z level=info msg="Migration successfully executed" id="add keep_firing_for column to alert_rule_version" duration=13.192875ms
grafana-1          | logger=migrator t=2025-11-07T15:49:59.215978054Z level=info msg="Executing migration" id="add missing_series_evals_to_resolve column to alert_rule"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.231742304Z level=info msg="Migration successfully executed" id="add missing_series_evals_to_resolve column to alert_rule" duration=15.763042ms
grafana-1          | logger=migrator t=2025-11-07T15:49:59.376045262Z level=info msg="Executing migration" id="add missing_series_evals_to_resolve column to alert_rule_version"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.388386304Z level=info msg="Migration successfully executed" id="add missing_series_evals_to_resolve column to alert_rule_version" duration=12.338542ms
grafana-1          | logger=migrator t=2025-11-07T15:49:59.527753762Z level=info msg="Executing migration" id="remove the datasources:drilldown action"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.528135845Z level=info msg="Removed 0 datasources:drilldown permissions"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.52814622Z level=info msg="Migration successfully executed" id="remove the datasources:drilldown action" duration=394.958Âµs
grafana-1          | logger=migrator t=2025-11-07T15:49:59.642288012Z level=info msg="Executing migration" id="remove title in folder unique index"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.64402172Z level=info msg="Migration successfully executed" id="remove title in folder unique index" duration=1.734667ms
grafana-1          | logger=migrator t=2025-11-07T15:49:59.799568179Z level=info msg="Executing migration" id="add fired_at column to alert_instance table"
grafana-1          | logger=migrator t=2025-11-07T15:49:59.816871887Z level=info msg="Migration successfully executed" id="add fired_at column to alert_instance table" duration=17.302792ms
grafana-1          | logger=migrator t=2025-11-07T15:49:59.976583846Z level=info msg="migrations completed" performed=674 skipped=0 duration=2m20.972084815s
grafana-1          | logger=migrator t=2025-11-07T15:49:59.977328012Z level=info msg="Unlocking database"
grafana-1          | logger=sqlstore t=2025-11-07T15:49:59.992671804Z level=info msg="Created default admin" user=admin
grafana-1          | logger=sqlstore t=2025-11-07T15:49:59.992863429Z level=info msg="Created default organization"
grafana-1          | logger=secrets t=2025-11-07T15:50:00.134465721Z level=info msg="Envelope encryption state" enabled=true currentprovider=secretKey.v1
grafana-1          | logger=plugin.angulardetectorsprovider.dynamic t=2025-11-07T15:50:00.373845596Z level=info msg="Restored cache from database" duration=5.446583ms
grafana-1          | logger=resource-db t=2025-11-07T15:50:00.403782387Z level=info msg="Using database section" db_type=sqlite3
grafana-1          | logger=resource-db t=2025-11-07T15:50:00.404351887Z level=info msg="Initializing Resource DB" db_type=sqlite3 open_conn=0 in_use_conn=0 idle_conn=0 max_open_conn=0
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.413133679Z level=info msg="Locking database"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.413148512Z level=info msg="Starting DB migrations"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.428582137Z level=info msg="Executing migration" id="create resource_migration_log table"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.429902596Z level=info msg="Migration successfully executed" id="create resource_migration_log table" duration=1.3215ms
kafka-1            | [2025-11-07 15:49:49,077] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka-1            | [2025-11-07 15:49:49,247] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2025-11-07 15:49:49,852] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:49,937] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:49,856] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:49,866] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:50,185] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka-1            | [2025-11-07 15:49:50,445] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
kafka-1            | [2025-11-07 15:49:51,053] INFO Stat of the created znode at /brokers/ids/1 is: 27,27,1762530590775,1762530590775,1,0,0,72057986241986561,194,0,27
kafka-1            |  (kafka.zk.KafkaZkClient)
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.525774138Z level=info msg="Executing migration" id="Initialize resource tables"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.525803596Z level=info msg="Migration successfully executed" id="Initialize resource tables" duration=31.458Âµs
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.612152263Z level=info msg="Executing migration" id="drop table resource"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.612357054Z level=info msg="Migration successfully executed" id="drop table resource" duration=204.25Âµs
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.888414971Z level=info msg="Executing migration" id="create table resource"
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:51.535Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:51.537Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":327}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:51.873Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:51.874Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":678}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.565Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.566Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1568}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:54.143Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:54.143Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2710}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:56.862Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:49:51,055] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://kafka:9092, czxid (broker epoch): 27 (kafka.zk.KafkaZkClient)
kafka-1            | [2025-11-07 15:49:51,576] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
kafka-1            | [2025-11-07 15:49:51,616] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:51,638] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:51,640] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:51,679] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:49:51,767] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:17.468Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":264}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:17.742Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:17.742Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":484}
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:00.889903513Z level=info msg="Migration successfully executed" id="create table resource" duration=1.487ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.002284221Z level=info msg="Executing migration" id="create table resource, index: 0"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.004439804Z level=info msg="Migration successfully executed" id="create table resource, index: 0" duration=2.1545ms
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:18.234Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:18.234Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1052}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:19.304Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:19.306Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2334}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:21.656Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:21.657Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4528}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:26.192Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:43:56.863Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4658}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:01.526Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:01.527Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7506}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:26.193Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10440}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
kafka-1            | [2025-11-07 15:49:51,925] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
kafka-1            | [2025-11-07 15:49:52,039] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
kafka-1            | [2025-11-07 15:49:52,045] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
kafka-1            | [2025-11-07 15:49:52,068] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
kafka-1            | [2025-11-07 15:49:52,526] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2025-11-07 15:49:52,578] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:49:52,648] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
kafka-1            | [2025-11-07 15:49:53,091] INFO [Controller id=1] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{}) (kafka.controller.KafkaController)
product-service-1  |   retryTime: 10440,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2025-11-07 15:49:53,395] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)
kafka-1            | [2025-11-07 15:49:53,681] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache)
kafka-1            | [2025-11-07 15:49:53,812] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.099028054Z level=info msg="Executing migration" id="drop table resource_history"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.099172888Z level=info msg="Migration successfully executed" id="drop table resource_history" duration=147.875Âµs
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.163568554Z level=info msg="Executing migration" id="create table resource_history"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.165535179Z level=info msg="Migration successfully executed" id="create table resource_history" duration=1.966917ms
kafka-1            | [2025-11-07 15:49:53,957] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:49:54,035] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Starting socket server acceptors and processors (kafka.network.SocketServer)
kafka-1            | [2025-11-07 15:49:54,043] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:49:54,093] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka-1            | [2025-11-07 15:49:54,114] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Started socket server acceptors and processors (kafka.network.SocketServer)
kafka-1            | [2025-11-07 15:49:54,139] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:49:56,001] INFO Kafka version: 7.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
kafka-1            | [2025-11-07 15:49:56,004] INFO Kafka commitId: b7e52413e7cb3e8b (org.apache.kafka.common.utils.AppInfoParser)
kafka-1            | [2025-11-07 15:49:56,381] INFO Kafka startTimeMs: 1762530594130 (org.apache.kafka.common.utils.AppInfoParser)
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.237001388Z level=info msg="Executing migration" id="create table resource_history, index: 0"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.238687471Z level=info msg="Migration successfully executed" id="create table resource_history, index: 0" duration=1.686875ms
kafka-1            | [2025-11-07 15:49:56,167] INFO [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 27) (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:49:57,054] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
kafka-1            | [2025-11-07 15:49:57,074] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:49:58,555] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
kafka-1            | [2025-11-07 15:50:00,415] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
kafka-1            | [2025-11-07 15:50:00,422] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
order-service-1    |   retryTime: 7506,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | [2025-11-07 15:50:00,477] INFO [Controller id=1] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:01,082] INFO [Controller id=1] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:01,192] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:01,893] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:02,370] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:02,442] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.314149388Z level=info msg="Executing migration" id="create table resource_history, index: 1"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.316653638Z level=info msg="Migration successfully executed" id="create table resource_history, index: 1" duration=2.503208ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.334964096Z level=info msg="Executing migration" id="drop table resource_version"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.335207638Z level=info msg="Migration successfully executed" id="drop table resource_version" duration=226.291Âµs
kafka-1            | [2025-11-07 15:50:02,567] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
kafka-1            | [2025-11-07 15:50:02,574] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.34152518Z level=info msg="Executing migration" id="create table resource_version"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.344162555Z level=info msg="Migration successfully executed" id="create table resource_version" duration=2.672917ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.352860513Z level=info msg="Executing migration" id="create table resource_version, index: 0"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.35536543Z level=info msg="Migration successfully executed" id="create table resource_version, index: 0" duration=2.507ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.378117513Z level=info msg="Executing migration" id="drop table resource_blob"
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.378777971Z level=info msg="Migration successfully executed" id="drop table resource_blob" duration=661.292Âµs
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.387188471Z level=info msg="Executing migration" id="create table resource_blob"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.388671638Z level=info msg="Migration successfully executed" id="create table resource_blob" duration=1.481833ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.484795305Z level=info msg="Executing migration" id="create table resource_blob, index: 0"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.486636346Z level=info msg="Migration successfully executed" id="create table resource_blob, index: 0" duration=1.841417ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.524152263Z level=info msg="Executing migration" id="create table resource_blob, index: 1"
product-service-1  |   retryTime: 10440,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |   }
order-service-1    | }
kafka-1            | [2025-11-07 15:50:03,442] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:03,501] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
kafka-1            | [2025-11-07 15:50:03,572] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
kafka-1            | [2025-11-07 15:50:03,574] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
kafka-1            | [2025-11-07 15:50:03,575] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
kafka-1            | [2025-11-07 15:50:03,616] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
order-service-1    | 
order-service-1    | Node.js v18.20.8
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.526940138Z level=info msg="Migration successfully executed" id="create table resource_blob, index: 1" duration=2.789791ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.593463305Z level=info msg="Executing migration" id="Add column previous_resource_version in resource_history"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.673190805Z level=info msg="Migration successfully executed" id="Add column previous_resource_version in resource_history" duration=79.722791ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.781826763Z level=info msg="Executing migration" id="Add column previous_resource_version in resource"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.79802793Z level=info msg="Migration successfully executed" id="Add column previous_resource_version in resource" duration=16.197916ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.933096888Z level=info msg="Executing migration" id="Add index to resource_history for polling"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:01.934836513Z level=info msg="Migration successfully executed" id="Add index to resource_history for polling" duration=1.738667ms
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
kafka-1            | [2025-11-07 15:50:04,973] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
kafka-1            | [2025-11-07 15:50:05,014] INFO [RequestSendThread controllerId=1] Controller 1 connected to kafka:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
kafka-1            | [2025-11-07 15:50:05,544] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
kafka-1            | [2025-11-07 15:50:05,656] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  write to custom object with { processEnv: myObject }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:44:02.246Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:05,739] INFO [Admin Manager on Broker 1]: Error processing create topic request CreatableTopic(name='order-events', numPartitions=1, replicationFactor=1, assignments=[], configs=[]) (kafka.server.ZkAdminManager)
kafka-1            | org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 1 larger than available brokers: 0.
kafka-1            | [2025-11-07 15:50:07,873] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Recorded new controller, from now on will use broker kafka:9092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2025-11-07 15:50:07,873] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Recorded new controller, from now on will use broker kafka:9092 (id: 1 rack: null) (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2025-11-07 15:50:07,947] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:07,948] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:07,948] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
kafka-1            | [2025-11-07 15:50:07,948] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:07,950] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:07,973] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 0 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2025-11-07 15:50:09,189] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:14,225] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:14,226] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:18,020] INFO Creating topic order-events with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
kafka-1            | [2025-11-07 15:50:18,402] INFO [Controller id=1] New topics: [Set(order-events)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(order-events,Some(zHT0-AUUTHG_9kd8-uAViQ),Map(order-events-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:18,417] INFO [Controller id=1] New partition creation callback for order-events-0 (kafka.controller.KafkaController)
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:18,441] INFO [Controller id=1 epoch=1] Changed partition order-events-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:18,441] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:18,539] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition order-events-0 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:18,539] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:19,285] INFO [Controller id=1 epoch=1] Changed partition order-events-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.046536722Z level=info msg="Executing migration" id="Add index to resource for loading"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.048057013Z level=info msg="Migration successfully executed" id="Add index to resource for loading" duration=1.52275ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.086062513Z level=info msg="Executing migration" id="Add column folder in resource_history"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.092097972Z level=info msg="Migration successfully executed" id="Add column folder in resource_history" duration=6.037083ms
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.125385513Z level=info msg="Executing migration" id="Add column folder in resource"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.15483818Z level=info msg="Migration successfully executed" id="Add column folder in resource" duration=29.449459ms
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  override existing env vars with { override: true }
kafka-1            | [2025-11-07 15:50:19,403] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='order-events', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition order-events-0 (state.change.logger)
kafka-1            | [2025-11-07 15:50:19,421] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:41:28.344Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.25877568Z level=info msg="Executing migration" id="Migrate DeletionMarkers to real Resource objects"
grafana-1          | logger=deletion-marker-migrator t=2025-11-07T15:50:02.258830472Z level=info msg="finding any deletion markers"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.259419097Z level=info msg="Migration successfully executed" id="Migrate DeletionMarkers to real Resource objects" duration=633.167Âµs
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.38970593Z level=info msg="Executing migration" id="Add index to resource_history for get trash"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.392746097Z level=info msg="Migration successfully executed" id="Add index to resource_history for get trash" duration=3.038ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.449226388Z level=info msg="Executing migration" id="Add generation to resource history"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.469744347Z level=info msg="Migration successfully executed" id="Add generation to resource history" duration=20.517875ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.504731763Z level=info msg="Executing migration" id="Add generation index to resource history"
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.506769888Z level=info msg="Migration successfully executed" id="Add generation index to resource history" duration=2.037ms
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.530237638Z level=info msg="migrations completed" performed=26 skipped=0 duration=2.101848084s
grafana-1          | logger=resource-migrator t=2025-11-07T15:50:02.531063763Z level=info msg="Unlocking database"
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
kafka-1            | [2025-11-07 15:50:19,469] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:19,472] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition order-events-0 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:19,472] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:19,500] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 for 1 partitions (state.change.logger)
grafana-1          | t=2025-11-07T15:50:02.534611513Z level=info caller=logger.go:214 time=2025-11-07T15:50:02.534380555Z msg="Using channel notifier" logger=sql-resource-server
grafana-1          | logger=plugin.store t=2025-11-07T15:50:02.562798263Z level=info msg="Loading plugins..."
grafana-1          | logger=plugin.store t=2025-11-07T15:50:02.730310389Z level=info msg="Plugins loaded" count=52 duration=167.512292ms
grafana-1          | logger=query_data t=2025-11-07T15:50:02.75278918Z level=info msg="Query Service initialization"
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
kafka-1            | [2025-11-07 15:50:19,501] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='order-events', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:19,647] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition order-events-0 (state.change.logger)
kafka-1            | [2025-11-07 15:50:19,649] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(order-events-0) (kafka.server.ReplicaFetcherManager)
grafana-1          | logger=live.push_http t=2025-11-07T15:50:02.809901347Z level=info msg="Live Push Gateway initialization"
grafana-1          | logger=ngalert.notifier component=alertmanager orgID=1 t=2025-11-07T15:50:02.859705764Z level=info msg="Applying new configuration to Alertmanager" configHash=d2c56faca6af2a5772ff4253222f7386
grafana-1          | logger=ngalert.writer t=2025-11-07T15:50:03.109568139Z level=info msg="Setting up remote write using data sources" timeout=30s default_datasource_uid=
grafana-1          | logger=ngalert t=2025-11-07T15:50:03.109646472Z level=info msg="Using protobuf-based alert instance store"
kafka-1            | [2025-11-07 15:50:19,649] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:21,251] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:21,459] INFO Created log for partition order-events-0 in /var/lib/kafka/data/order-events-0 with properties {} (kafka.log.LogManager)
grafana-1          | logger=ngalert.state.manager.persist t=2025-11-07T15:50:03.10965543Z level=info msg="Using rule state persister"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.163893139Z level=info msg="Locking database"
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.163915555Z level=info msg="Starting DB migrations"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.165077222Z level=info msg="Executing migration" id="create secret_migration_log table"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.166269014Z level=info msg="Migration successfully executed" id="create secret_migration_log table" duration=1.193542ms
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:21,466] INFO [Partition order-events-0 broker=1] No checkpointed highwatermark is found for partition order-events-0 (kafka.cluster.Partition)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:02.333Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:21,467] INFO [Partition order-events-0 broker=1] Log loaded for partition order-events-0 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:21,490] INFO [Broker id=1] Leader order-events-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:21,620] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition order-events-0 (state.change.logger)
kafka-1            | [2025-11-07 15:50:21,642] INFO [Broker id=1] Finished LeaderAndIsr request in 2148ms correlationId 1 from controller 1 for 1 partitions (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:21,685] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=zHT0-AUUTHG_9kd8-uAViQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 1 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2025-11-07 15:50:22,179] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='order-events', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition order-events-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
kafka-1            | [2025-11-07 15:50:22,180] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
kafka-1            | [2025-11-07 15:50:22,183] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 2 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2025-11-07 15:50:23,462] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1), 25 -> ArrayBuffer(1), 26 -> ArrayBuffer(1), 27 -> ArrayBuffer(1), 28 -> ArrayBuffer(1), 29 -> ArrayBuffer(1), 30 -> ArrayBuffer(1), 31 -> ArrayBuffer(1), 32 -> ArrayBuffer(1), 33 -> ArrayBuffer(1), 34 -> ArrayBuffer(1), 35 -> ArrayBuffer(1), 36 -> ArrayBuffer(1), 37 -> ArrayBuffer(1), 38 -> ArrayBuffer(1), 39 -> ArrayBuffer(1), 40 -> ArrayBuffer(1), 41 -> ArrayBuffer(1), 42 -> ArrayBuffer(1), 43 -> ArrayBuffer(1), 44 -> ArrayBuffer(1), 45 -> ArrayBuffer(1), 46 -> ArrayBuffer(1), 47 -> ArrayBuffer(1), 48 -> ArrayBuffer(1), 49 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:02.334Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":310}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:02.659Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:02.661Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":638}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:03.312Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:03.313Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1462}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:04.787Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:04.789Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2618}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:07.423Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:07.424Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5534}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.969Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.971Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9644}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:44:12.973Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.988Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.989Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":351}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:13.353Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:24,079] INFO [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(lUJe58ubQHKzh0NJNktnNQ),HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))))] (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:24,080] INFO [Controller id=1] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:50:24,088] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:13.354Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":618}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:13.982Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.422064583Z level=info msg="Executing migration" id="Initialize secrets tables"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.422092916Z level=info msg="Migration successfully executed" id="Initialize secrets tables" duration=31.667Âµs
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.649096375Z level=info msg="Executing migration" id="drop table secret_secure_value"
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.649226416Z level=info msg="Migration successfully executed" id="drop table secret_secure_value" duration=132.667Âµs
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.738719416Z level=info msg="Executing migration" id="create table secret_secure_value"
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:13.983Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1450}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:15.445Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:15.446Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2484}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:17.938Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:17.939Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5508}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:44:18 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:44:18.040Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:23.462Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:23.463Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11610}
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.740410958Z level=info msg="Migration successfully executed" id="create table secret_secure_value" duration=1.924334ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.851716291Z level=info msg="Executing migration" id="create table secret_secure_value, index: 0"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:03.852815208Z level=info msg="Migration successfully executed" id="create table secret_secure_value, index: 0" duration=1.1ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.018007166Z level=info msg="Executing migration" id="create table secret_secure_value, index: 1"
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.019650875Z level=info msg="Migration successfully executed" id="create table secret_secure_value, index: 1" duration=1.64525ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.146733875Z level=info msg="Executing migration" id="drop table secret_keeper"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.146890667Z level=info msg="Migration successfully executed" id="drop table secret_keeper" duration=158.208Âµs
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.23245975Z level=info msg="Executing migration" id="create table secret_keeper"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.233369667Z level=info msg="Migration successfully executed" id="create table secret_keeper" duration=911.334Âµs
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.369867583Z level=info msg="Executing migration" id="create table secret_keeper, index: 0"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.371525042Z level=info msg="Migration successfully executed" id="create table secret_keeper, index: 0" duration=1.657625ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.511443583Z level=info msg="Executing migration" id="drop table secret_data_key"
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.511697375Z level=info msg="Migration successfully executed" id="drop table secret_data_key" duration=255.75Âµs
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.5970335Z level=info msg="Executing migration" id="create table secret_data_key"
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:28.440Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:28.441Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":274}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:28.727Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:28.728Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":640}
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,089] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.598478917Z level=info msg="Migration successfully executed" id="create table secret_data_key" duration=1.446542ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.64084325Z level=info msg="Executing migration" id="drop table secret_encrypted_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.64106775Z level=info msg="Migration successfully executed" id="drop table secret_encrypted_value" duration=252.041Âµs
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.694109333Z level=info msg="Executing migration" id="create table secret_encrypted_value"
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.696019917Z level=info msg="Migration successfully executed" id="create table secret_encrypted_value" duration=1.910125ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.764461709Z level=info msg="Executing migration" id="create table secret_encrypted_value, index: 0"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.76748575Z level=info msg="Migration successfully executed" id="create table secret_encrypted_value, index: 0" duration=3.025292ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.899026125Z level=info msg="Executing migration" id="create index for list on secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:04.900715917Z level=info msg="Migration successfully executed" id="create index for list on secret_secure_value" duration=1.691459ms
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:29.380Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:29.381Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1228}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:30.620Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:30.622Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2532}
order-service-1    |             ^
order-service-1    | 
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:33.168Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:33.169Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4264}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:37.450Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:37.451Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10098}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.016627667Z level=info msg="Executing migration" id="create index for list and read current on secret_data_key"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.017551375Z level=info msg="Migration successfully executed" id="create index for list and read current on secret_data_key" duration=927.375Âµs
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.078089625Z level=info msg="Executing migration" id="add owner_reference_api_group column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.114195Z level=info msg="Migration successfully executed" id="add owner_reference_api_group column to secret_secure_value" duration=36.100916ms
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.190838167Z level=info msg="Executing migration" id="add owner_reference_api_version column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.200483709Z level=info msg="Migration successfully executed" id="add owner_reference_api_version column to secret_secure_value" duration=7.496834ms
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,090] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.329612875Z level=info msg="Executing migration" id="add owner_reference_kind column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.344244334Z level=info msg="Migration successfully executed" id="add owner_reference_kind column to secret_secure_value" duration=14.627334ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.467702834Z level=info msg="Executing migration" id="add owner_reference_name column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.477675876Z level=info msg="Migration successfully executed" id="add owner_reference_name column to secret_secure_value" duration=9.973208ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.572097959Z level=info msg="Executing migration" id="add lease_token column to secret_secure_value"
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   retryTime: 11610,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.602208209Z level=info msg="Migration successfully executed" id="add lease_token column to secret_secure_value" duration=30.107167ms
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10098,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.719310417Z level=info msg="Executing migration" id="add lease_token index to secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.721765251Z level=info msg="Migration successfully executed" id="add lease_token index to secret_secure_value" duration=2.454875ms
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,100] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,101] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,101] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,101] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.811580959Z level=info msg="Executing migration" id="add lease_created column to secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.830929876Z level=info msg="Migration successfully executed" id="add lease_created column to secret_secure_value" duration=19.345333ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.937291876Z level=info msg="Executing migration" id="add lease_created index to secret_secure_value"
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:05.939383584Z level=info msg="Migration successfully executed" id="add lease_created index to secret_secure_value" duration=2.091125ms
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:06.028320792Z level=info msg="migrations completed" performed=24 skipped=0 duration=2.839650209s
grafana-1          | logger=secret-migrator t=2025-11-07T15:50:06.028782501Z level=info msg="Unlocking database"
product-service-1  |   }
product-service-1  | }
grafana-1          | logger=infra.usagestats.collector t=2025-11-07T15:50:06.033461667Z level=info msg="registering usage stat providers" usageStatsProvidersLen=2
grafana-1          | logger=grafanaStorageLogger t=2025-11-07T15:50:06.041854292Z level=info msg="Storage starting"
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:06.049135001Z level=info msg="Installing plugin" pluginId=grafana-metricsdrilldown-app version=
grafana-1          | logger=ngalert.state.manager t=2025-11-07T15:50:06.055504084Z level=info msg="Warming state cache for startup"
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
grafana-1          | logger=ngalert.multiorg.alertmanager t=2025-11-07T15:50:06.068842001Z level=info msg="Starting MultiOrg Alertmanager"
grafana-1          | logger=http.server t=2025-11-07T15:50:06.103744376Z level=info msg="HTTP Server Listen" address=[::]:3000 protocol=http subUrl= socket=
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10098,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  write to custom object with { processEnv: myObject }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:44:24.266Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
grafana-1          | logger=grafana.update.checker t=2025-11-07T15:50:06.451333126Z level=info msg="Update check succeeded" duration=392.535375ms
grafana-1          | logger=plugins.update.checker t=2025-11-07T15:50:06.451406376Z level=info msg="Update check succeeded" duration=381.745417ms
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  enable debug logging with { debug: true }
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
grafana-1          | logger=ngalert.state.manager t=2025-11-07T15:50:06.959520793Z level=info msg="State cache has been initialized" states=0 duration=904.016584ms
grafana-1          | logger=ngalert.scheduler t=2025-11-07T15:50:06.960110251Z level=info msg="Starting scheduler" tickInterval=10s maxAttempts=3
grafana-1          | logger=ngalert.scheduler t=2025-11-07T15:50:06.96672096Z level=info msg=starting component=ticker first_tick=2025-11-07T15:50:10Z
grafana-1          | logger=provisioning.alerting t=2025-11-07T15:50:06.995674835Z level=info msg="starting to provision alerting"
grafana-1          | logger=provisioning.alerting t=2025-11-07T15:50:06.995698418Z level=info msg="finished to provision alerting"
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:41:41.234Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
grafana-1          | logger=provisioning.dashboard t=2025-11-07T15:50:06.99633496Z level=info msg="starting to provision dashboards"
grafana-1          | logger=provisioning.dashboard t=2025-11-07T15:50:06.99634046Z level=info msg="finished to provision dashboards"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.157180751Z level=info msg="Adding GroupVersion folder.grafana.app v1beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.16668821Z level=info msg="Adding GroupVersion iam.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.172373293Z level=info msg="Adding GroupVersion userstorage.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.172806918Z level=info msg="Adding GroupVersion features.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.175405168Z level=info msg="Adding GroupVersion notifications.alerting.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.230756418Z level=info msg="Adding GroupVersion dashboard.grafana.app v1beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.231228876Z level=info msg="Adding GroupVersion dashboard.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.231703585Z level=info msg="Adding GroupVersion dashboard.grafana.app v2beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.23236546Z level=info msg="Adding GroupVersion dashboard.grafana.app v2alpha1 to ResourceManager"
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.232795918Z level=info msg="Adding GroupVersion playlist.grafana.app v0alpha1 to ResourceManager"
grafana-1          | t=2025-11-07T15:50:07.23284596Z level=info caller=logger.go:214 time=2025-11-07T15:50:07.23284246Z msg="Installed APIs for app" app=playlist
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,102] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
grafana-1          | logger=grafana-apiserver t=2025-11-07T15:50:07.232921543Z level=info msg="Skipping API plugins.grafana.app/v0alpha1 because it has no resources."
grafana-1          | t=2025-11-07T15:50:07.232943793Z level=info caller=logger.go:214 time=2025-11-07T15:50:07.232941626Z msg="Installed APIs for app" app=plugins
grafana-1          | t=2025-11-07T15:50:07.407056626Z level=info caller=logger.go:214 time=2025-11-07T15:50:07.407054585Z msg="App initialized" app=playlist
grafana-1          | t=2025-11-07T15:50:07.40734346Z level=info caller=logger.go:214 time=2025-11-07T15:50:07.407341293Z msg="App initialized" app=plugins
grafana-1          | logger=app-registry t=2025-11-07T15:50:07.408172168Z level=info msg="app registry initialized"
grafana-1          | logger=plugin.installer t=2025-11-07T15:50:07.490072335Z level=info msg="Installing plugin" pluginId=grafana-metricsdrilldown-app version=
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
grafana-1          | logger=installer.fs t=2025-11-07T15:50:07.822109918Z level=info msg="Downloaded and extracted grafana-metricsdrilldown-app v1.0.21 zip successfully to /var/lib/grafana/plugins/grafana-metricsdrilldown-app"
grafana-1          | logger=plugin.angulardetectorsprovider.dynamic t=2025-11-07T15:50:08.213632002Z level=info msg="Patterns update finished" duration=2.155974667s
grafana-1          | logger=plugins.registration t=2025-11-07T15:50:08.215982877Z level=info msg="Plugin registered" pluginId=grafana-metricsdrilldown-app
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:24,103] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,359] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:08.216001252Z level=info msg="Plugin successfully installed" pluginId=grafana-metricsdrilldown-app version= duration=2.166852834s
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:08.216021627Z level=info msg="Installing plugin" pluginId=grafana-lokiexplore-app version=
grafana-1          | logger=plugin.installer t=2025-11-07T15:50:12.356265212Z level=info msg="Installing plugin" pluginId=grafana-lokiexplore-app version=
grafana-1          | logger=installer.fs t=2025-11-07T15:50:13.184110379Z level=info msg="Downloaded and extracted grafana-lokiexplore-app v1.0.30 zip successfully to /var/lib/grafana/plugins/grafana-lokiexplore-app"
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
grafana-1          | logger=plugins.registration t=2025-11-07T15:50:13.235216004Z level=info msg="Plugin registered" pluginId=grafana-lokiexplore-app
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:13.235237963Z level=info msg="Plugin successfully installed" pluginId=grafana-lokiexplore-app version= duration=5.019206461s
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:13.235284296Z level=info msg="Installing plugin" pluginId=grafana-pyroscope-app version=
grafana-1          | logger=plugin.installer t=2025-11-07T15:50:13.771299088Z level=info msg="Installing plugin" pluginId=grafana-pyroscope-app version=
grafana-1          | logger=installer.fs t=2025-11-07T15:50:14.088246796Z level=info msg="Downloaded and extracted grafana-pyroscope-app v1.11.0 zip successfully to /var/lib/grafana/plugins/grafana-pyroscope-app"
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.362Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.363Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":346}
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,360] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
grafana-1          | logger=plugins.registration t=2025-11-07T15:50:14.141520755Z level=info msg="Plugin registered" pluginId=grafana-pyroscope-app
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:14.141554671Z level=info msg="Plugin successfully installed" pluginId=grafana-pyroscope-app version= duration=906.264459ms
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:14.141574088Z level=info msg="Installing plugin" pluginId=grafana-exploretraces-app version=
grafana-1          | logger=plugin.installer t=2025-11-07T15:50:14.489710046Z level=info msg="Installing plugin" pluginId=grafana-exploretraces-app version=
grafana-1          | logger=installer.fs t=2025-11-07T15:50:14.739805172Z level=info msg="Downloaded and extracted grafana-exploretraces-app v1.2.0 zip successfully to /var/lib/grafana/plugins/grafana-exploretraces-app"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.725Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.727Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":570}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:25.309Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:25.310Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1140}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:26.463Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=plugins.registration t=2025-11-07T15:50:14.762102713Z level=info msg="Plugin registered" pluginId=grafana-exploretraces-app
grafana-1          | logger=plugin.backgroundinstaller t=2025-11-07T15:50:14.762176463Z level=info msg="Plugin successfully installed" pluginId=grafana-exploretraces-app version= duration=620.599042ms
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:41.332Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:41.333Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":266}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:41.612Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:41.613Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":638}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:42.263Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:42.264Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1100}
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:43.381Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:43.383Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1884}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:45.284Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:45.285Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3074}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:48.378Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:26.467Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2094}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:28.574Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:28.575Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4024}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:48.379Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6802}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:32.617Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:32.619Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8792}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:44:32.621Z"}
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6802,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
grafana-1          | logger=infra.usagestats t=2025-11-07T15:51:22.105883967Z level=info msg="Usage stats are ready to report"
grafana-1          | logger=cleanup t=2025-11-07T16:00:06.099955917Z level=info msg="Completed cleanup jobs" duration=28.620541ms
grafana-1          | logger=plugins.update.checker t=2025-11-07T16:00:06.597559626Z level=info msg="Update check succeeded" duration=138.426875ms
grafana-1          | logger=sqlstore.transactions t=2025-11-07T16:05:02.880685513Z level=info msg="Database locked, sleeping then retrying" error="database is locked" retry=0
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
grafana-1          | logger=cleanup t=2025-11-07T16:10:06.119573459Z level=info msg="Completed cleanup jobs" duration=47.173875ms
grafana-1          | logger=plugins.update.checker t=2025-11-07T16:10:06.598234918Z level=info msg="Update check succeeded" duration=143.039084ms
grafana-1          | logger=cleanup t=2025-11-07T16:20:06.094749542Z level=info msg="Completed cleanup jobs" duration=18.748916ms
grafana-1          | logger=plugins.update.checker t=2025-11-07T16:20:06.625351834Z level=info msg="Update check succeeded" duration=164.157292ms
grafana-1          | logger=infra.usagestats t=2025-11-07T16:21:22.108389133Z level=info msg="Usage stats are ready to report"
grafana-1          | logger=cleanup t=2025-11-07T16:30:06.105685292Z level=info msg="Completed cleanup jobs" duration=22.650833ms
grafana-1          | logger=plugins.update.checker t=2025-11-07T16:30:06.732281668Z level=info msg="Update check succeeded" duration=264.447583ms
grafana-1          | logger=cleanup t=2025-11-07T16:42:23.632525876Z level=info msg="Completed cleanup jobs" duration=34.015709ms
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:32.637Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:32.638Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":244}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:32.896Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:32.897Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":418}
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,361] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:44:33 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:44:33.054Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:33.329Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:33.330Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":930}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:34.272Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:34.273Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1546}
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
grafana-1          | logger=plugins.update.checker t=2025-11-07T16:42:24.190010209Z level=info msg="Update check succeeded" duration=207.194083ms
grafana-1          | logger=server t=2025-11-07T16:43:36.720018136Z level=info msg="Shutdown started" reason="System signal: terminated"
grafana-1          | logger=tracing t=2025-11-07T16:43:36.859018886Z level=info msg="Closing tracing"
grafana-1          | logger=ngalert.scheduler t=2025-11-07T16:43:36.903169052Z level=info msg=stopped component=ticker last_tick=2025-11-07T16:43:30Z
grafana-1          | logger=grafana-apiserver t=2025-11-07T16:43:36.906783011Z level=info msg="StorageObjectCountTracker pruner is exiting"
kafka-1            | [2025-11-07 15:50:30,373] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-13 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-46 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-9 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-42 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-21 (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:35.830Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:35.831Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3184}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:39.026Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:39.027Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":5636}
grafana-1          | logger=settings t=2026-02-25T10:39:26.991303753Z level=info msg="Starting Grafana" version=12.2.1 commit=563109b696e9c1cbaf345f2ab7a11f7f78422982 branch=release-12.2.1 compiled=2026-02-25T10:39:26Z
grafana-1          | logger=settings t=2026-02-25T10:39:26.992303169Z level=info msg="Config loaded from" file=/usr/share/grafana/conf/defaults.ini
grafana-1          | logger=settings t=2026-02-25T10:39:26.992322586Z level=info msg="Config loaded from" file=/etc/grafana/grafana.ini
grafana-1          | logger=settings t=2026-02-25T10:39:26.992325503Z level=info msg="Config overridden from command line" arg="default.paths.data=/var/lib/grafana"
grafana-1          | logger=settings t=2026-02-25T10:39:26.992327586Z level=info msg="Config overridden from command line" arg="default.paths.logs=/var/log/grafana"
grafana-1          | logger=settings t=2026-02-25T10:39:26.992329086Z level=info msg="Config overridden from command line" arg="default.paths.plugins=/var/lib/grafana/plugins"
grafana-1          | logger=settings t=2026-02-25T10:39:26.992330669Z level=info msg="Config overridden from command line" arg="default.paths.provisioning=/etc/grafana/provisioning"
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-17 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-30 (state.change.logger)
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 5636,
grafana-1          | logger=settings t=2026-02-25T10:39:26.992333961Z level=info msg="Config overridden from command line" arg="default.log.mode=console"
grafana-1          | logger=settings t=2026-02-25T10:39:26.992339753Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_DATA=/var/lib/grafana"
grafana-1          | logger=settings t=2026-02-25T10:39:26.992342336Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_LOGS=/var/log/grafana"
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-26 (state.change.logger)
grafana-1          | logger=settings t=2026-02-25T10:39:26.992344211Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PLUGINS=/var/lib/grafana/plugins"
grafana-1          | logger=settings t=2026-02-25T10:39:26.992345794Z level=info msg="Config overridden from Environment variable" var="GF_PATHS_PROVISIONING=/etc/grafana/provisioning"
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6802,
grafana-1          | logger=settings t=2026-02-25T10:39:26.992347753Z level=info msg=Target target=[all]
grafana-1          | logger=settings t=2026-02-25T10:39:26.992368378Z level=info msg="Path Home" path=/usr/share/grafana
grafana-1          | logger=settings t=2026-02-25T10:39:26.992370336Z level=info msg="Path Data" path=/var/lib/grafana
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
grafana-1          | logger=settings t=2026-02-25T10:39:26.992371794Z level=info msg="Path Logs" path=/var/log/grafana
grafana-1          | logger=settings t=2026-02-25T10:39:26.992373294Z level=info msg="Path Plugins" path=/var/lib/grafana/plugins
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-5 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-38 (state.change.logger)
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  specify custom .env file path with { path: '/custom/path/.env' }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:41:55.268Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
grafana-1          | logger=settings t=2026-02-25T10:39:26.992374878Z level=info msg="Path Provisioning" path=/etc/grafana/provisioning
grafana-1          | logger=settings t=2026-02-25T10:39:26.992376628Z level=info msg="App mode production"
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-34 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-16 (state.change.logger)
grafana-1          | logger=featuremgmt t=2026-02-25T10:39:26.992794169Z level=info msg=FeatureToggles transformationsRedesign=true logsInfiniteScrolling=true alertingSaveStateCompressed=true alertingUIOptimizeReducer=true prometheusAzureOverrideAudience=true skipTokenRotationIfRecent=true dashboardDsAdHocFiltering=true newFiltersUI=true azureMonitorEnableUserAuth=true logsExploreTableVisualisation=true groupToNestedTableTransformation=true annotationPermissionUpdate=true cloudWatchNewLabelParsing=true lokiQuerySplitting=true cloudWatchCrossAccountQuerying=true pinNavItems=true unifiedStorageHistoryPruner=true kubernetesDashboards=true alertingQueryAndExpressionsStepMode=true preinstallAutoUpdate=true awsAsyncQueryCaching=true addFieldFromCalculationStatFunctions=true onPremToCloudMigrations=true adhocFiltersInTooltips=true unifiedRequestLog=true lokiLabelNamesQueryApi=true cloudWatchRoundUpEndTime=true alertingImportYAMLUI=true dashboardSceneSolo=true newPDFRendering=true dashgpt=true alertRuleRestore=true correlations=true improvedExternalSessionHandlingSAML=true grafanaconThemes=true alertingBulkActionsInUI=true ssoSettingsLDAP=true alertingMigrationUI=true formatString=true alertingRuleVersionHistoryRestore=true alertingRulePermanentlyDelete=true alertingRuleRecoverDeleted=true tlsMemcached=true influxdbBackendMigration=true grafanaAssistantInProfilesDrilldown=true dataplaneFrontendFallback=true awsDatasourcesTempCredentials=true logRowsPopoverMenu=true newDashboardSharingComponent=true logsContextDatasourceUi=true recordedQueriesMulti=true useSessionStorageForRedirection=true publicDashboardsScene=true improvedExternalSessionHandling=true logsPanelControls=true panelMonitoring=true alertingNotificationsStepMode=true dashboardSceneForViewers=true dashboardScene=true promQLScope=true azureMonitorPrometheusExemplars=true
grafana-1          | logger=sqlstore t=2026-02-25T10:39:26.992863086Z level=info msg="Connecting to DB" dbtype=sqlite3
grafana-1          | logger=migrator t=2026-02-25T10:39:27.018956836Z level=info msg="Locking database"
grafana-1          | logger=migrator t=2026-02-25T10:39:27.018999836Z level=info msg="Starting DB migrations"
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-45 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-12 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-41 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-24 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-20 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-49 (state.change.logger)
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:44:39.725Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
grafana-1          | logger=migrator t=2026-02-25T10:39:27.041710669Z level=info msg="migrations completed" performed=0 skipped=674 duration=460.416Âµs
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
grafana-1          | logger=migrator t=2026-02-25T10:39:27.042325294Z level=info msg="Unlocking database"
grafana-1          | logger=secrets t=2026-02-25T10:39:27.043125544Z level=info msg="Envelope encryption state" enabled=true currentprovider=secretKey.v1
grafana-1          | logger=plugin.angulardetectorsprovider.dynamic t=2026-02-25T10:39:27.108510086Z level=info msg="Restored cache from database" duration=1.451458ms
grafana-1          | logger=resource-db t=2026-02-25T10:39:27.114930128Z level=info msg="Using database section" db_type=sqlite3
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-0 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-29 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-25 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-8 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,374] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-37 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-33 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=resource-db t=2026-02-25T10:39:27.115036044Z level=info msg="Initializing Resource DB" db_type=sqlite3 open_conn=0 in_use_conn=0 idle_conn=0 max_open_conn=0
grafana-1          | logger=resource-migrator t=2026-02-25T10:39:27.121622628Z level=info msg="Locking database"
grafana-1          | logger=resource-migrator t=2026-02-25T10:39:27.121643253Z level=info msg="Starting DB migrations"
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
grafana-1          | logger=resource-migrator t=2026-02-25T10:39:27.130869836Z level=info msg="migrations completed" performed=0 skipped=26 duration=1.475042ms
grafana-1          | logger=resource-migrator t=2026-02-25T10:39:27.131287003Z level=info msg="Unlocking database"
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-15 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-48 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-11 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-44 (state.change.logger)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:55.354Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:55.355Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":358}
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
grafana-1          | t=2026-02-25T10:39:27.132249753Z level=info caller=logger.go:214 time=2026-02-25T10:39:27.131767711Z msg="Using channel notifier" logger=sql-resource-server
grafana-1          | logger=plugin.store t=2026-02-25T10:39:27.134776461Z level=info msg="Loading plugins..."
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:27.247797003Z level=info msg="Plugin registered" pluginId=grafana-exploretraces-app
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:27.293016461Z level=info msg="Plugin registered" pluginId=grafana-lokiexplore-app
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-23 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-19 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-32 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-28 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-7 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-40 (state.change.logger)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:55.727Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:55.728Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":722}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:56.472Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:56.473Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1596}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:58.080Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-3 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-36 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-47 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-14 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-43 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-10 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-22 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-18 (state.change.logger)
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:27.322368336Z level=info msg="Plugin registered" pluginId=grafana-metricsdrilldown-app
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:27.341845836Z level=info msg="Plugin registered" pluginId=grafana-pyroscope-app
grafana-1          | logger=plugin.store t=2026-02-25T10:39:27.341868669Z level=info msg="Plugins loaded" count=56 duration=207.093625ms
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=query_data t=2026-02-25T10:39:27.350918169Z level=info msg="Query Service initialization"
grafana-1          | logger=live.push_http t=2026-02-25T10:39:27.355587544Z level=info msg="Live Push Gateway initialization"
grafana-1          | logger=ngalert.notifier component=alertmanager orgID=1 t=2026-02-25T10:39:27.361572128Z level=info msg="Applying new configuration to Alertmanager" configHash=d2c56faca6af2a5772ff4253222f7386
grafana-1          | logger=ngalert.writer t=2026-02-25T10:39:27.372453503Z level=info msg="Setting up remote write using data sources" timeout=30s default_datasource_uid=
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:41:58.081Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2696}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:00.790Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:00.791Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6380}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:07.183Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:07.184Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12670}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:39.809Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:39.810Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":284}
grafana-1          | logger=ngalert t=2026-02-25T10:39:27.373269586Z level=info msg="Using protobuf-based alert instance store"
grafana-1          | logger=ngalert.state.manager.persist t=2026-02-25T10:39:27.373283753Z level=info msg="Using rule state persister"
grafana-1          | logger=secret-migrator t=2026-02-25T10:39:27.377361753Z level=info msg="Locking database"
grafana-1          | logger=secret-migrator t=2026-02-25T10:39:27.377377628Z level=info msg="Starting DB migrations"
grafana-1          | logger=secret-migrator t=2026-02-25T10:39:27.378848961Z level=info msg="migrations completed" performed=0 skipped=24 duration=26.875Âµs
grafana-1          | logger=secret-migrator t=2026-02-25T10:39:27.379164878Z level=info msg="Unlocking database"
grafana-1          | logger=infra.usagestats.collector t=2026-02-25T10:39:27.379349586Z level=info msg="registering usage stat providers" usageStatsProvidersLen=2
grafana-1          | logger=ngalert.state.manager t=2026-02-25T10:39:27.38070317Z level=info msg="Warming state cache for startup"
grafana-1          | logger=ngalert.multiorg.alertmanager t=2026-02-25T10:39:27.382563753Z level=info msg="Starting MultiOrg Alertmanager"
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
grafana-1          | logger=grafanaStorageLogger t=2026-02-25T10:39:27.382628336Z level=info msg="Storage starting"
grafana-1          | logger=http.server t=2026-02-25T10:39:27.384980378Z level=info msg="HTTP Server Listen" address=[::]:3000 protocol=http subUrl= socket=
grafana-1          | logger=provisioning.alerting t=2026-02-25T10:39:27.454185045Z level=info msg="starting to provision alerting"
grafana-1          | logger=provisioning.alerting t=2026-02-25T10:39:27.454228836Z level=info msg="finished to provision alerting"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:40.102Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:40.103Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":562}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:40.674Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:40.674Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1166}
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12670,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-31 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-27 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,375] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-39 (state.change.logger)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
grafana-1          | logger=provisioning.dashboard t=2026-02-25T10:39:27.454681753Z level=info msg="starting to provision dashboards"
grafana-1          | logger=provisioning.dashboard t=2026-02-25T10:39:27.454699836Z level=info msg="finished to provision dashboards"
grafana-1          | logger=ngalert.state.manager t=2026-02-25T10:39:27.457480628Z level=info msg="State cache has been initialized" states=0 duration=76.777125ms
kafka-1            | [2025-11-07 15:50:30,376] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-6 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,376] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-35 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,376] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-2 (state.change.logger)
grafana-1          | logger=ngalert.scheduler t=2026-02-25T10:39:27.457600628Z level=info msg="Starting scheduler" tickInterval=10s maxAttempts=3
grafana-1          | logger=ngalert.scheduler t=2026-02-25T10:39:27.458130045Z level=info msg=starting component=ticker first_tick=2026-02-25T10:39:30Z
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.463369461Z level=info msg="Adding GroupVersion features.grafana.app v0alpha1 to ResourceManager"
kafka-1            | [2025-11-07 15:50:30,376] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 50 become-leader and 0 become-follower partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,378] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 50 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,386] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.464499628Z level=info msg="Adding GroupVersion notifications.alerting.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.46815067Z level=info msg="Adding GroupVersion dashboard.grafana.app v1beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.468336628Z level=info msg="Adding GroupVersion dashboard.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.468521711Z level=info msg="Adding GroupVersion dashboard.grafana.app v2beta1 to ResourceManager"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:41.847Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:41.848Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2396}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:44.258Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:44.259Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4084}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.355Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.358Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7424}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:44:48.360Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.374Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.375Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":356}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.780Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.782Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":636}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:49.431Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.468711295Z level=info msg="Adding GroupVersion dashboard.grafana.app v2alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.469183503Z level=info msg="Adding GroupVersion folder.grafana.app v1beta1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.47052192Z level=info msg="Adding GroupVersion iam.grafana.app v0alpha1 to ResourceManager"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:49.433Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1428}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:50.870Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:50.870Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2614}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:53.494Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.470941253Z level=info msg="Adding GroupVersion userstorage.grafana.app v0alpha1 to ResourceManager"
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.471779253Z level=info msg="Adding GroupVersion playlist.grafana.app v0alpha1 to ResourceManager"
grafana-1          | t=2026-02-25T10:39:27.471821253Z level=info caller=logger.go:214 time=2026-02-25T10:39:27.471813503Z msg="Installed APIs for app" app=playlist
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:53.494Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5106}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:58.612Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:58.613Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9678}
grafana-1          | logger=grafana-apiserver t=2026-02-25T10:39:27.471895253Z level=info msg="Skipping API plugins.grafana.app/v0alpha1 because it has no resources."
grafana-1          | t=2026-02-25T10:39:27.471932086Z level=info caller=logger.go:214 time=2026-02-25T10:39:27.471930086Z msg="Installed APIs for app" app=plugins
grafana-1          | t=2026-02-25T10:39:27.514064586Z level=info caller=logger.go:214 time=2026-02-25T10:39:27.514054295Z msg="App initialized" app=plugins
grafana-1          | t=2026-02-25T10:39:27.514573961Z level=info caller=logger.go:214 time=2026-02-25T10:39:27.514558045Z msg="App initialized" app=playlist
grafana-1          | logger=app-registry t=2026-02-25T10:39:27.514774253Z level=info msg="app registry initialized"
grafana-1          | logger=plugins.update.checker t=2026-02-25T10:39:27.583800045Z level=info msg="Update check succeeded" duration=201.972083ms
grafana-1          | logger=grafana.update.checker t=2026-02-25T10:39:27.585665503Z level=info msg="Update check succeeded" duration=204.952208ms
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
grafana-1          | logger=plugin.angulardetectorsprovider.dynamic t=2026-02-25T10:39:27.59478542Z level=info msg="Patterns update finished" duration=175.725417ms
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:27.723859795Z level=info msg="Installing plugin" pluginId=grafana-exploretraces-app version=
grafana-1          | logger=plugin.installer t=2026-02-25T10:39:27.893108878Z level=info msg="Updating plugin" pluginId=grafana-exploretraces-app from=1.2.0 to=1.3.2
grafana-1          | logger=installer.fs t=2026-02-25T10:39:28.598178753Z level=info msg="Downloaded and extracted grafana-exploretraces-app v1.3.2 zip successfully to /var/lib/grafana/plugins/grafana-exploretraces-app"
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:28.612599753Z level=info msg="Plugin registered" pluginId=grafana-exploretraces-app
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:28.612621045Z level=info msg="Plugin successfully installed" pluginId=grafana-exploretraces-app version= duration=888.739084ms
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 9678,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:28.892107795Z level=info msg="Installing plugin" pluginId=grafana-metricsdrilldown-app version=
grafana-1          | logger=plugin.installer t=2026-02-25T10:39:29.063273087Z level=info msg="Updating plugin" pluginId=grafana-metricsdrilldown-app from=1.0.21 to=1.0.31
grafana-1          | logger=installer.fs t=2026-02-25T10:39:29.40373442Z level=info msg="Downloaded and extracted grafana-metricsdrilldown-app v1.0.31 zip successfully to /var/lib/grafana/plugins/grafana-metricsdrilldown-app"
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12670,
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  specify custom .env file path with { path: '/custom/path/.env' }
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,442] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,443] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,443] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:29.427767754Z level=info msg="Plugin registered" pluginId=grafana-metricsdrilldown-app
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:29.42779617Z level=info msg="Plugin successfully installed" pluginId=grafana-metricsdrilldown-app version= duration=535.664ms
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:29.602307837Z level=info msg="Installing plugin" pluginId=grafana-lokiexplore-app version=
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:44:59.296Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
grafana-1          | logger=plugin.installer t=2026-02-25T10:39:29.822383546Z level=info msg="Updating plugin" pluginId=grafana-lokiexplore-app from=1.0.30 to=1.0.37
grafana-1          | logger=installer.fs t=2026-02-25T10:39:30.247513838Z level=info msg="Downloaded and extracted grafana-lokiexplore-app v1.0.37 zip successfully to /var/lib/grafana/plugins/grafana-lokiexplore-app"
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:30.271196213Z level=info msg="Plugin registered" pluginId=grafana-lokiexplore-app
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:30.271218296Z level=info msg="Plugin successfully installed" pluginId=grafana-lokiexplore-app version= duration=668.889167ms
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:30.509566754Z level=info msg="Installing plugin" pluginId=grafana-pyroscope-app version=
grafana-1          | logger=plugin.installer t=2026-02-25T10:39:30.929060838Z level=info msg="Updating plugin" pluginId=grafana-pyroscope-app from=1.11.0 to=1.17.0
grafana-1          | logger=installer.fs t=2026-02-25T10:39:31.159475046Z level=info msg="Downloaded and extracted grafana-pyroscope-app v1.17.0 zip successfully to /var/lib/grafana/plugins/grafana-pyroscope-app"
grafana-1          | logger=plugins.registration t=2026-02-25T10:39:31.171744171Z level=info msg="Plugin registered" pluginId=grafana-pyroscope-app
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
grafana-1          | logger=plugin.backgroundinstaller t=2026-02-25T10:39:31.171767005Z level=info msg="Plugin successfully installed" pluginId=grafana-pyroscope-app version= duration=662.179292ms
grafana-1          | logger=infra.usagestats t=2026-02-25T10:40:08.40588655Z level=info msg="Usage stats are ready to report"
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
grafana-1          | logger=cleanup t=2026-02-25T10:49:37.007478336Z level=info msg="Completed cleanup jobs" duration=108.289417ms
grafana-1          | logger=plugins.update.checker t=2026-02-25T10:49:37.287077503Z level=info msg="Update check succeeded" duration=182.256792ms
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
kafka-1            | [2025-11-07 15:50:30,443] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,443] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,443] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,443] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,483] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,492] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
kafka-1            | [2025-11-07 15:50:30,565] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:42:07.860Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,566] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,593] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 for 50 partitions (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:59.381Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:59.381Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":284}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:59.679Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:44:59.680Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":500}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,606] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:00.192Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:00.192Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1026}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:01.229Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:07.961Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:07.962Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":324}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:08.299Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:01.231Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1710}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:02.949Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:02.950Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3898}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:06.866Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:06.868Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8756}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:45:06.870Z"}
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
order-service-1    | Server is running on port 3001
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:08.300Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":532}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:08.845Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:08.846Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1050}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:09.909Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:09.911Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2438}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:12.362Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:06.887Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:12.362Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5470}
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:06.888Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":278}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:07.174Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:07.174Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":502}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:07.689Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:07.690Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1064}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:08.767Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:08.767Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2022}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:10.796Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:10.797Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4792}
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.602Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:17.839Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:17.840Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12984}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,607] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:30,608] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,058] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.603Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10922}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12984,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
kafka-1            | [2025-11-07 15:50:31,060] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,060] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,060] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | [2025-11-07 15:50:31,060] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
product-service-1  |     [cause]: undefined
product-service-1  |   }
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |   retryTime: 10922,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12984,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
order-service-1    | }
order-service-1    | 
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,061] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,062] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,062] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,062] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,090] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
kafka-1            | [2025-11-07 15:50:31,111] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 1 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,283] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
kafka-1            | [2025-11-07 15:50:31,304] INFO Created log for partition __consumer_offsets-3 in /var/lib/kafka/data/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:31,380] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:31,794] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  enable debug logging with { debug: true }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:45:16.233Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:31,794] INFO [Broker id=1] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:31,889] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:31,891] INFO Created log for partition __consumer_offsets-18 in /var/lib/kafka/data/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
kafka-1            | [2025-11-07 15:50:31,891] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:31,891] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  specify custom .env file path with { path: '/custom/path/.env' }
kafka-1            | [2025-11-07 15:50:31,891] INFO [Broker id=1] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:32,011] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:32,013] INFO Created log for partition __consumer_offsets-41 in /var/lib/kafka/data/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:32,013] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,013] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,013] INFO [Broker id=1] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:32,085] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:32,087] INFO Created log for partition __consumer_offsets-10 in /var/lib/kafka/data/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:32,087] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,087] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,088] INFO [Broker id=1] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:32,166] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:32,168] INFO Created log for partition __consumer_offsets-33 in /var/lib/kafka/data/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:32,168] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,168] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,168] INFO [Broker id=1] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:32,215] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:32,217] INFO Created log for partition __consumer_offsets-48 in /var/lib/kafka/data/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:32,217] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:42:18.463Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:32,217] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,220] INFO [Broker id=1] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:32,468] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:32,534] INFO Created log for partition __consumer_offsets-19 in /var/lib/kafka/data/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:32,547] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,612] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,699] INFO [Broker id=1] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:32,994] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
kafka-1            | [2025-11-07 15:50:32,998] INFO Created log for partition __consumer_offsets-34 in /var/lib/kafka/data/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:32,998] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,998] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:32,998] INFO [Broker id=1] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
kafka-1            | [2025-11-07 15:50:33,282] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:33,299] INFO Created log for partition __consumer_offsets-4 in /var/lib/kafka/data/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:33,300] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:33,300] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:33,300] INFO [Broker id=1] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:33,965] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:33,994] INFO Created log for partition __consumer_offsets-11 in /var/lib/kafka/data/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:33,994] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:33,994] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:33,994] INFO [Broker id=1] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:34,117] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:34,118] INFO Created log for partition __consumer_offsets-26 in /var/lib/kafka/data/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:34,119] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,119] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,119] INFO [Broker id=1] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:34,304] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:34,307] INFO Created log for partition __consumer_offsets-49 in /var/lib/kafka/data/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:34,307] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,307] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,307] INFO [Broker id=1] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:34,392] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:34,396] INFO Created log for partition __consumer_offsets-39 in /var/lib/kafka/data/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:16.318Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:16.318Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":269}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:16.607Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:16.611Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":508}
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:18.551Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:34,396] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:18.552Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":312}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:18.877Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:18.877Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":728}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:19.617Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:19.619Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1332}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:20.964Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:20.966Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2610}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:23.593Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:17.130Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:17.131Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":916}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:18.054Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:18.055Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1762}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:19.826Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:19.827Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3546}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:23.594Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4908}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:28.547Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.381Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.382Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7630}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:45:23.383Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.393Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.393Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":345}
kafka-1            | [2025-11-07 15:50:34,397] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,397] INFO [Broker id=1] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:28.551Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10948}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2025-11-07 15:50:34,461] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:34,467] INFO Created log for partition __consumer_offsets-9 in /var/lib/kafka/data/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:34,467] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,467] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,467] INFO [Broker id=1] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:34,534] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.746Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.746Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":710}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:24.465Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:34,542] INFO Created log for partition __consumer_offsets-24 in /var/lib/kafka/data/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:34,544] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,544] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:34,544] INFO [Broker id=1] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:34,787] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:35,016] INFO Created log for partition __consumer_offsets-31 in /var/lib/kafka/data/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | [2025-11-07 15:50:35,019] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,019] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,019] INFO [Broker id=1] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10948,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
kafka-1            | [2025-11-07 15:50:35,109] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:24.466Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1164}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:25.643Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:35,114] INFO Created log for partition __consumer_offsets-46 in /var/lib/kafka/data/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:35,114] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,114] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,114] INFO [Broker id=1] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:35,185] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:35,188] INFO Created log for partition __consumer_offsets-1 in /var/lib/kafka/data/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:25.644Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2300}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:27.956Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:27.957Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4410}
kafka-1            | [2025-11-07 15:50:35,194] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,194] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,194] INFO [Broker id=1] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:35,263] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:35,267] INFO Created log for partition __consumer_offsets-16 in /var/lib/kafka/data/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:35,267] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,267] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,268] INFO [Broker id=1] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:35,401] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:35,402] INFO Created log for partition __consumer_offsets-2 in /var/lib/kafka/data/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:32.382Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:32.383Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8880}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2025-11-07 15:50:35,402] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,402] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,402] INFO [Broker id=1] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:35,497] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:35,499] INFO Created log for partition __consumer_offsets-25 in /var/lib/kafka/data/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:35,499] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,499] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,499] INFO [Broker id=1] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:35,663] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:35,669] INFO Created log for partition __consumer_offsets-40 in /var/lib/kafka/data/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:35,670] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,670] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,670] INFO [Broker id=1] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:35,864] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 8880,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2025-11-07 15:50:35,866] INFO Created log for partition __consumer_offsets-47 in /var/lib/kafka/data/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:35,867] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,867] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,867] INFO [Broker id=1] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:35,905] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:35,906] INFO Created log for partition __consumer_offsets-17 in /var/lib/kafka/data/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2025-11-07 15:50:35,907] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,907] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:35,907] INFO [Broker id=1] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,089] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
order-service-1    | > node index.js
order-service-1    | 
kafka-1            | [2025-11-07 15:50:36,092] INFO Created log for partition __consumer_offsets-32 in /var/lib/kafka/data/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:36,092] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,092] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,092] INFO [Broker id=1] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,156] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:36,177] INFO Created log for partition __consumer_offsets-37 in /var/lib/kafka/data/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:36,179] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,179] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,179] INFO [Broker id=1] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,301] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:36,301] INFO Created log for partition __consumer_offsets-7 in /var/lib/kafka/data/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:36,301] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,301] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,301] INFO [Broker id=1] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,374] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:36,382] INFO Created log for partition __consumer_offsets-22 in /var/lib/kafka/data/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:36,383] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,383] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,383] INFO [Broker id=1] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,449] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:36,450] INFO Created log for partition __consumer_offsets-29 in /var/lib/kafka/data/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10948,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ” prevent building .env in docker: https://dotenvx.com/prebuild
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:45:33.147Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:36,450] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,450] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,450] INFO [Broker id=1] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,508] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
kafka-1            | [2025-11-07 15:50:36,510] INFO Created log for partition __consumer_offsets-44 in /var/lib/kafka/data/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:36,510] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,510] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,511] INFO [Broker id=1] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,575] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:36,576] INFO Created log for partition __consumer_offsets-14 in /var/lib/kafka/data/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:36,576] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,576] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,576] INFO [Broker id=1] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
kafka-1            | [2025-11-07 15:50:36,662] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:36,663] INFO Created log for partition __consumer_offsets-23 in /var/lib/kafka/data/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:36,663] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,663] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,663] INFO [Broker id=1] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:36,911] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:36,913] INFO Created log for partition __consumer_offsets-38 in /var/lib/kafka/data/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
kafka-1            | [2025-11-07 15:50:36,913] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,913] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:36,913] INFO [Broker id=1] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:37,054] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:37,055] INFO Created log for partition __consumer_offsets-8 in /var/lib/kafka/data/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:37,055] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”‘ add access controls to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:42:29.295Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:37,055] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,055] INFO [Broker id=1] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:37,122] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
kafka-1            | [2025-11-07 15:50:37,126] INFO Created log for partition __consumer_offsets-45 in /var/lib/kafka/data/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:37,126] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,126] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,126] INFO [Broker id=1] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
kafka-1            | [2025-11-07 15:50:37,158] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:37,159] INFO Created log for partition __consumer_offsets-15 in /var/lib/kafka/data/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:37,159] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,159] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,159] INFO [Broker id=1] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:37,256] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
kafka-1            | [2025-11-07 15:50:37,257] INFO Created log for partition __consumer_offsets-30 in /var/lib/kafka/data/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:37,257] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,257] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,258] INFO [Broker id=1] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:33.243Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:33.244Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":324}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:33.581Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:33.583Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":724}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:34.320Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:34.320Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1502}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:35.832Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:35.833Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2430}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:38.273Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:38.273Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5464}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:43.753Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:43.755Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9838}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:45:43.759Z"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:43.773Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:43.774Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":268}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:44.055Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:44.056Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":566}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:44.636Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:44.637Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1056}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:45.703Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:45.704Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2418}
kafka-1            | [2025-11-07 15:50:37,547] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:37,566] INFO Created log for partition __consumer_offsets-0 in /var/lib/kafka/data/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:45:48 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:45:48.044Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:48.128Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:48.129Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4540}
kafka-1            | [2025-11-07 15:50:37,566] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,566] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,566] INFO [Broker id=1] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:37,695] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:37,703] INFO Created log for partition __consumer_offsets-35 in /var/lib/kafka/data/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:37,703] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:29.399Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:29.401Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":249}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:29.668Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:52.684Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:52.685Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7784}
order-service-1    | node:internal/process/promises:288
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:29.670Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":408}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:30.094Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:30.095Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":682}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:30.788Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:30.790Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1184}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:31.991Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:37,703] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:37,703] INFO [Broker id=1] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:38,055] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:38,057] INFO Created log for partition __consumer_offsets-5 in /var/lib/kafka/data/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:31.992Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":1920}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:33.928Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:33.929Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":3390}
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 3390,
kafka-1            | [2025-11-07 15:50:38,057] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,057] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,057] INFO [Broker id=1] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:38,311] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:38,313] INFO Created log for partition __consumer_offsets-20 in /var/lib/kafka/data/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:38,313] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,313] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,313] INFO [Broker id=1] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:38,511] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:38,515] INFO Created log for partition __consumer_offsets-27 in /var/lib/kafka/data/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:38,515] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,515] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,515] INFO [Broker id=1] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:38,815] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:38,825] INFO Created log for partition __consumer_offsets-42 in /var/lib/kafka/data/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
kafka-1            | [2025-11-07 15:50:38,825] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,825] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,825] INFO [Broker id=1] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:38,939] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
kafka-1            | [2025-11-07 15:50:38,942] INFO Created log for partition __consumer_offsets-12 in /var/lib/kafka/data/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:38,943] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,943] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:38,944] INFO [Broker id=1] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
kafka-1            | [2025-11-07 15:50:39,156] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:39,157] INFO Created log for partition __consumer_offsets-21 in /var/lib/kafka/data/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:39,157] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:39,157] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:39,157] INFO [Broker id=1] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:39,451] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:39,451] INFO Created log for partition __consumer_offsets-36 in /var/lib/kafka/data/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:39,451] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:39,451] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
kafka-1            | [2025-11-07 15:50:39,451] INFO [Broker id=1] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:39,701] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:39,702] INFO Created log for partition __consumer_offsets-6 in /var/lib/kafka/data/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:39,702] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 3390,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2025-11-07 15:50:39,702] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:39,702] INFO [Broker id=1] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:40,277] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:40,280] INFO Created log for partition __consumer_offsets-43 in /var/lib/kafka/data/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
kafka-1            | [2025-11-07 15:50:40,280] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:40,280] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  override existing env vars with { override: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:42:34.663Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 7784,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:40,280] INFO [Broker id=1] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:40,488] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | [2025-11-07 15:50:40,489] INFO Created log for partition __consumer_offsets-13 in /var/lib/kafka/data/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:40,489] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:40,489] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | [2025-11-07 15:50:40,489] INFO [Broker id=1] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
kafka-1            | [2025-11-07 15:50:40,801] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2025-11-07 15:50:40,802] INFO Created log for partition __consumer_offsets-28 in /var/lib/kafka/data/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", segment.bytes=104857600} (kafka.log.LogManager)
kafka-1            | [2025-11-07 15:50:40,802] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:34.751Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:34.752Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":308}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:35.078Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:35.079Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":654}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:35.751Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:35.752Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1566}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:37.332Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:37.334Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2760}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:40.117Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:40.118Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6028}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:46.154Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:46.155Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":14338}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 14338,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2025-11-07 15:50:40,802] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
kafka-1            | [2025-11-07 15:50:40,802] INFO [Broker id=1] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
kafka-1            | [2025-11-07 15:50:41,083] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,083] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:45:53.580Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
kafka-1            | [2025-11-07 15:50:41,101] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,103] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,103] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
kafka-1            | [2025-11-07 15:50:41,103] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,103] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
product-service-1  |   retryTime: 14338,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:53.692Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:53.693Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":312}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:54.015Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:54.017Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":744}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:54.773Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:54.774Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1276}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:56.065Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:56.067Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2720}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:58.800Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:45:58.801Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6362}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.175Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.178Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10344}
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:42:46.820Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:46:05.180Z"}
order-service-1    | Server is running on port 3001
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.195Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.196Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":356}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.561Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.562Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":630}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:06.203Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:06.205Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1332}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:07.548Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:07.549Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2288}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:09.860Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:09.861Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5450}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:15.324Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:15.325Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9142}
order-service-1    | node:internal/process/promises:288
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,104] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,109] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,149] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,178] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,178] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:46.907Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:46.908Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":315}
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 9142,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:47.235Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:47.237Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":644}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:47.892Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:47.893Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1268}
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:49.173Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:49.174Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2192}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:51.382Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:51.383Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3640}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:55.039Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ› ï¸  run anywhere with `dotenvx run -- yourcommand`
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:46:16.039Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:55.041Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6152}
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,180] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6152,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6152,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:16.127Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:16.128Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":327}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:16.469Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” prevent building .env in docker: https://dotenvx.com/prebuild
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:42:55.780Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
kafka-1            | [2025-11-07 15:50:41,181] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:16.471Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":756}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:17.237Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:17.238Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1756}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:19.010Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:19.012Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":4056}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:23.083Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:23.084Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":8584}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:31.715Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:31.773Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":17720}
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:46:31.783Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:31.871Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:31.877Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":313}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:32.205Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:32.206Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":508}
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
kafka-1            | [2025-11-07 15:50:41,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:41,187] INFO [Broker id=1] Finished LeaderAndIsr request in 10595ms correlationId 3 from controller 1 for 50 partitions (state.change.logger)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:32.731Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:32.733Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":966}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:46:33 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:46:33.062Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:33.709Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:33.709Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2176}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:35.903Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:35.905Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3564}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:39.483Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:39.483Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7214}
kafka-1            | [2025-11-07 15:50:41,238] TRACE [Controller id=1 epoch=1] Received response LeaderAndIsrResponseData(errorCode=0, partitionErrors=[], topics=[LeaderAndIsrTopicError(topicId=lUJe58ubQHKzh0NJNktnNQ, partitionErrors=[LeaderAndIsrPartitionError(topicName='', partitionIndex=13, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=46, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=9, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=42, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=21, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=17, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=30, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=26, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=5, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=38, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=1, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=34, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=16, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=45, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=12, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=41, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=24, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=20, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=49, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=0, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=29, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=25, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=8, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=37, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=4, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=33, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=15, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=48, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=11, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=44, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=23, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=19, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=32, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=28, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=7, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=40, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=3, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=36, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=47, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=14, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=43, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=10, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=22, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=18, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=31, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=27, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=39, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=6, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=35, errorCode=0), LeaderAndIsrPartitionError(topicName='', partitionIndex=2, errorCode=0)])]) for request LEADER_AND_ISR with correlation id 3 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2025-11-07 15:50:41,199] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 48 milliseconds for epoch 0, of which 38 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,147] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 953 milliseconds for epoch 0, of which 934 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
kafka-1            | [2025-11-07 15:50:42,219] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 1039 milliseconds for epoch 0, of which 1038 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,219] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 1039 milliseconds for epoch 0, of which 1039 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,219] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 1039 milliseconds for epoch 0, of which 1039 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,219] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 1039 milliseconds for epoch 0, of which 1039 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 1039 milliseconds for epoch 0, of which 1039 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:55.874Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:55.875Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":288}
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,220] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 1041 milliseconds for epoch 0, of which 1041 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 1041 milliseconds for epoch 0, of which 1041 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 1041 milliseconds for epoch 0, of which 1041 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 1041 milliseconds for epoch 0, of which 1041 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 1041 milliseconds for epoch 0, of which 1041 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,181] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:56.174Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:56.175Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":628}
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:56.818Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:56.819Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1182}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:58.011Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2025-11-07 15:50:42,231] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:42:58.013Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2804}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:00.841Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:00.843Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5972}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.829Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:06.831Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10288}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10288,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |   retryTime: 7214,
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ” encrypt with Dotenvx: https://dotenvx.com
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:46:40.301Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,232] INFO [Broker id=1] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
kafka-1            | [2025-11-07 15:50:42,221] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 1040 milliseconds for epoch 0, of which 1040 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,238] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 1057 milliseconds for epoch 0, of which 1056 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,238] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 1057 milliseconds for epoch 0, of which 1057 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,238] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 1057 milliseconds for epoch 0, of which 1057 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,239] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 1058 milliseconds for epoch 0, of which 1057 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,239] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10288,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
kafka-1            | [2025-11-07 15:50:42,239] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,239] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,239] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2025-11-07 15:50:42,239] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,239] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,240] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 1059 milliseconds for epoch 0, of which 1059 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,240] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 1059 milliseconds for epoch 0, of which 1059 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
kafka-1            | [2025-11-07 15:50:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 1060 milliseconds for epoch 0, of which 1059 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
kafka-1            | [2025-11-07 15:50:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 1060 milliseconds for epoch 0, of which 1060 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 1055 milliseconds for epoch 0, of which 1055 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,241] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 1055 milliseconds for epoch 0, of which 1055 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
kafka-1            | [2025-11-07 15:50:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 1058 milliseconds for epoch 0, of which 1056 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:40.398Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:40.399Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":295}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:40.703Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:40.704Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":658}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:41.370Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:41.371Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1288}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:42.671Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:42.672Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2916}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:45.605Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:43:07.567Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
kafka-1            | [2025-11-07 15:50:42,244] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 1058 milliseconds for epoch 0, of which 1058 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
kafka-1            | [2025-11-07 15:50:42,251] TRACE [Controller id=1 epoch=1] Received response UpdateMetadataResponseData(errorCode=0) for request UPDATE_METADATA with correlation id 4 sent to broker kafka:9092 (id: 1 rack: null) (state.change.logger)
kafka-1            | [2025-11-07 15:55:14,261] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:45.606Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6830}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:52.451Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:52.454Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10980}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:46:52.456Z"}
order-service-1    | Server is running on port 3001
kafka-1            | [2025-11-07 15:55:14,264] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:55:14,286] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 15:55:14,298] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:52.475Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:52.476Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":303}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:52.793Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:52.794Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":672}
kafka-1            | [2025-11-07 16:00:14,282] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:00:14,285] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:00:14,298] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:00:14,299] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:53.481Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:53.483Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1412}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:54.910Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:54.912Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2804}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:57.728Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:46:57.729Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5396}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:47:03 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:47:03.062Z"}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:03.138Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:03.139Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9326}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
kafka-1            | [2025-11-07 16:05:14,313] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:05:14,316] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:05:14,327] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:05:14,327] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:10:14,337] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:10:14,338] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:10:14,353] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:10:14,353] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:15:14,358] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:15:14,361] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 16:15:14,373] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:15:14,374] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:20:14,387] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
kafka-1            | [2025-11-07 16:20:14,388] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:20:14,397] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:20:14,397] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 9326,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2025-11-07 16:25:14,410] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:25:14,411] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:25:14,430] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:25:14,430] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:30:14,436] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:30:14,436] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:30:14,458] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
kafka-1            | [2025-11-07 16:30:14,459] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:35:14,466] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:35:14,467] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:35:14,482] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:35:14,482] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:42:32,000] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:42:32,003] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2025-11-07 16:42:32,012] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:42:32,012] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
kafka-1            | [2025-11-07 16:43:37,707] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
kafka-1            | ===> User
kafka-1            | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
kafka-1            | ===> Configuring ...
kafka-1            | ===> Running preflight checks ... 
kafka-1            | ===> Check if /var/lib/kafka/data is writable ...
kafka-1            | ===> Check if Zookeeper is healthy ...
kafka-1            | SLF4J: Class path contains multiple SLF4J bindings.
kafka-1            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | SLF4J: Found binding in [jar:file:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
kafka-1            | SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
kafka-1            | SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=363a57706629
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.13
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/usr/share/java/cp-base-new/lz4-java-1.7.1.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.12.3.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-1.0.0.jar:/usr/share/java/cp-base-new/zstd-jni-1.5.0-2.jar:/usr/share/java/cp-base-new/jolokia-core-1.6.2.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/kafka-metadata-7.0.1-ccs.jar:/usr/share/java/cp-base-new/json-simple-1.1.1.jar:/usr/share/java/cp-base-new/kafka-raft-7.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-databind-2.12.3.jar:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar:/usr/share/java/cp-base-new/kafka_2.13-7.0.1-ccs.jar:/usr/share/java/cp-base-new/kafka-server-common-7.0.1-ccs.jar:/usr/share/java/cp-base-new/metrics-core-4.1.12.1.jar:/usr/share/java/cp-base-new/snakeyaml-1.27.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.12.3.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/kafka-clients-7.0.1-ccs.jar:/usr/share/java/cp-base-new/confluent-log4j-1.2.17-cp2.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.3.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/jmx_prometheus_javaagent-0.14.0.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.12.3.jar:/usr/share/java/cp-base-new/kafka-storage-api-7.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.12.3.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.4.4.jar:/usr/share/java/cp-base-new/snappy-java-1.1.8.1.jar:/usr/share/java/cp-base-new/gson-2.8.6.jar:/usr/share/java/cp-base-new/jackson-annotations-2.12.3.jar:/usr/share/java/cp-base-new/slf4j-log4j12-1.7.30.jar:/usr/share/java/cp-base-new/disk-usage-agent-7.0.1.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.30.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.6.3.jar:/usr/share/java/cp-base-new/jackson-core-2.12.3.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.5.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/kafka-storage-7.0.1-ccs.jar:/usr/share/java/cp-base-new/common-utils-7.0.1.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/jolokia-jvm-1.6.2-agent.jar:/usr/share/java/cp-base-new/zookeeper-3.6.3.jar:/usr/share/java/cp-base-new/utility-belt-7.0.1.jar:/usr/share/java/cp-base-new/scala-library-2.13.5.jar
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=6.10.14-linuxkit
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/home/appuser
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/home/appuser
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=117MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=1960MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=124MB
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@a7e666
kafka-1            | [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
kafka-1            | [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 1048575 Bytes
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
kafka-1            | [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=false
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.6:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - Session 0x0 for sever zookeeper/172.18.0.6:2181, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ›¡ï¸ auth for agents: https://vestauth.com
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:47:03.930Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | java.net.ConnectException: Connection refused
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
kafka-1            | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:344)
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.18.0.6:2181.
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - SASL config status: Will not attempt to authenticate using SASL (unknown error)
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.18.0.7:59632, server: zookeeper/172.18.0.6:2181
kafka-1            | [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.18.0.6:2181, session id = 0x1000000ab340000, negotiated timeout = 40000
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:07.675Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:07.675Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":300}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:07.992Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
kafka-1            | [main-SendThread(zookeeper:2181)] WARN org.apache.zookeeper.ClientCnxn - An exception was thrown while closing send thread for session 0x1000000ab340000.
kafka-1            | EndOfStreamException: Unable to read additional data from server sessionid 0x1000000ab340000, likely server has closed socket
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77)
kafka-1            | 	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:07.993Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":574}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:08.585Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:08.586Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1200}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:09.801Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:09.804Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2076}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:11.889Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:11.890Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4432}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:16.337Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
kafka-1            | [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x1000000ab340000 closed
kafka-1            | [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x1000000ab340000
kafka-1            | ===> Launching ... 
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:16.338Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8418}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8418,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | ===> Launching kafka ... 
kafka-1            | [2026-02-25 10:39:36,516] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 10:39:37,249] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
kafka-1            | [2026-02-25 10:39:37,408] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
kafka-1            | [2026-02-25 10:39:37,420] INFO starting (kafka.server.KafkaServer)
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 10:39:37,422] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
kafka-1            | [2026-02-25 10:39:37,457] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:host.name=363a57706629 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:java.version=11.0.13 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-json-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-2.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/kafka-raft-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/netty-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/connect-transforms-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/kafka-shell-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-clients-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/trogdor-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.68.Final.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.68.Final.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.19.3.jar:/usr/bin/../share/java/kafka/kafka-streams-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.68.Final.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/kafka-tools-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.68.Final.jar:/usr/bin/../share/java/kafka/kafka-storage-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.43.v20210629.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.0.1-ccs.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/connect-mirror-7.0.1-ccs.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:os.version=6.10.14-linuxkit (org.apache.zookeeper.ZooKeeper)
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8418,
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:os.memory.free=1010MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,466] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 10:39:37,469] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@77b14724 (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:37,477] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
kafka-1            | [2026-02-25 10:39:37,487] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:04.044Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:04.046Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":336}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:04.395Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:04.396Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":556}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:04.966Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:04.967Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1034}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:06.012Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2026-02-25 10:39:37,493] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2026-02-25 10:39:37,518] INFO Opening socket connection to server zookeeper/172.18.0.6:2181. (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 10:39:37,519] INFO SASL config status: Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 10:39:37,531] INFO Socket connection established, initiating session, client: /172.18.0.7:59646, server: zookeeper/172.18.0.6:2181 (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 10:39:37,549] INFO Session establishment complete on server zookeeper/172.18.0.6:2181, session id = 0x1000000ab340001, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 10:39:37,554] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:06.015Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2250}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:08.273Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:08.274Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3650}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:11.936Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:11.936Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7776}
kafka-1            | [2026-02-25 10:39:37,662] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
kafka-1            | [2026-02-25 10:39:37,857] INFO Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0). (kafka.server.FinalizedFeatureCache)
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:47:11.939Z"}
order-service-1    | Server is running on port 3001
kafka-1            | [2026-02-25 10:39:37,865] INFO Cluster ID = qmOUJ3StQVKTJWzG6j8-Fw (kafka.server.KafkaServer)
kafka-1            | [2026-02-25 10:39:37,931] INFO KafkaConfig values: 
kafka-1            | 	advertised.listeners = PLAINTEXT://kafka:9092
kafka-1            | 	alter.config.policy.class.name = null
kafka-1            | 	alter.log.dirs.replication.quota.window.num = 11
product-service-1  |   }
product-service-1  | }
product-service-1  | 
kafka-1            | 	alter.log.dirs.replication.quota.window.size.seconds = 1
kafka-1            | 	authorizer.class.name = 
kafka-1            | 	auto.create.topics.enable = true
kafka-1            | 	auto.leader.rebalance.enable = true
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ‘¥ sync secrets across teammates & machines: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:43:17.011Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
kafka-1            | 	background.threads = 10
kafka-1            | 	broker.heartbeat.interval.ms = 2000
kafka-1            | 	broker.id = 1
kafka-1            | 	broker.id.generation.enable = true
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
kafka-1            | 	broker.rack = null
kafka-1            | 	broker.session.timeout.ms = 9000
kafka-1            | 	client.quota.callback.class = null
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
kafka-1            | 	compression.type = producer
kafka-1            | 	connection.failed.authentication.delay.ms = 100
kafka-1            | 	connections.max.idle.ms = 600000
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
kafka-1            | 	connections.max.reauth.ms = 0
kafka-1            | 	control.plane.listener.name = null
kafka-1            | 	controlled.shutdown.enable = true
kafka-1            | 	controlled.shutdown.max.retries = 3
kafka-1            | 	controlled.shutdown.retry.backoff.ms = 5000
kafka-1            | 	controller.listener.names = null
kafka-1            | 	controller.quorum.append.linger.ms = 25
kafka-1            | 	controller.quorum.election.backoff.max.ms = 1000
kafka-1            | 	controller.quorum.election.timeout.ms = 1000
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
kafka-1            | 	controller.quorum.fetch.timeout.ms = 2000
kafka-1            | 	controller.quorum.request.timeout.ms = 2000
kafka-1            | 	controller.quorum.retry.backoff.ms = 20
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:11.953Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:11.954Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":313}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:12.277Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:12.279Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":522}
kafka-1            | 	controller.quorum.voters = []
kafka-1            | 	controller.quota.window.num = 11
kafka-1            | 	controller.quota.window.size.seconds = 1
kafka-1            | 	controller.socket.timeout.ms = 30000
kafka-1            | 	create.topic.policy.class.name = null
kafka-1            | 	default.replication.factor = 1
kafka-1            | 	delegation.token.expiry.check.interval.ms = 3600000
kafka-1            | 	delegation.token.expiry.time.ms = 86400000
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:12.814Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:12.816Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":992}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:13.821Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:13.822Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2044}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:15.878Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:15.879Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4642}
kafka-1            | 	delegation.token.master.key = null
kafka-1            | 	delegation.token.max.lifetime.ms = 604800000
kafka-1            | 	delegation.token.secret.key = null
kafka-1            | 	delete.records.purgatory.purge.interval.requests = 1
kafka-1            | 	delete.topic.enable = true
kafka-1            | 	fetch.max.bytes = 57671680
kafka-1            | 	fetch.purgatory.purge.interval.requests = 1000
kafka-1            | 	group.initial.rebalance.delay.ms = 0
kafka-1            | 	group.max.session.timeout.ms = 1800000
kafka-1            | 	group.max.size = 2147483647
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | 	group.min.session.timeout.ms = 6000
kafka-1            | 	initial.broker.registration.timeout.ms = 60000
kafka-1            | 	inter.broker.listener.name = null
kafka-1            | 	inter.broker.protocol.version = 3.0-IV1
kafka-1            | 	kafka.metrics.polling.interval.secs = 10
kafka-1            | 	kafka.metrics.reporters = []
kafka-1            | 	leader.imbalance.check.interval.seconds = 300
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:47:18 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:47:18.046Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:20.540Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	leader.imbalance.per.broker.percentage = 10
kafka-1            | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:20.541Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7932}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | 	listeners = PLAINTEXT://0.0.0.0:9092
kafka-1            | 	log.cleaner.backoff.ms = 15000
kafka-1            | 	log.cleaner.dedupe.buffer.size = 134217728
kafka-1            | 	log.cleaner.delete.retention.ms = 86400000
kafka-1            | 	log.cleaner.enable = true
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
order-service-1    |   retryTime: 7932,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | 	log.cleaner.io.buffer.load.factor = 0.9
kafka-1            | 	log.cleaner.io.buffer.size = 524288
kafka-1            | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka-1            | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
kafka-1            | 	log.cleaner.min.cleanable.ratio = 0.5
kafka-1            | 	log.cleaner.min.compaction.lag.ms = 0
kafka-1            | 	log.cleaner.threads = 1
kafka-1            | 	log.cleanup.policy = [delete]
kafka-1            | 	log.dir = /tmp/kafka-logs
kafka-1            | 	log.dirs = /var/lib/kafka/data
kafka-1            | 	log.flush.interval.messages = 9223372036854775807
order-service-1    |   }
order-service-1    | }
order-service-1    | 
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:17.111Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:17.112Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":248}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:17.370Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:17.370Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":400}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:18.284Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ” prevent building .env in docker: https://dotenvx.com/prebuild
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:47:21.321Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:18.288Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":722}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:19.035Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:19.037Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1428}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:30.021Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:30.033Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3162}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:33.226Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:33.235Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6666}
kafka-1            | 	log.flush.interval.ms = null
kafka-1            | 	log.flush.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.flush.scheduler.interval.ms = 9223372036854775807
kafka-1            | 	log.flush.start.offset.checkpoint.interval.ms = 60000
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
kafka-1            | 	log.index.interval.bytes = 4096
kafka-1            | 	log.index.size.max.bytes = 10485760
kafka-1            | 	log.message.downconversion.enable = true
kafka-1            | 	log.message.format.version = 3.0-IV1
kafka-1            | 	log.message.timestamp.difference.max.ms = 9223372036854775807
kafka-1            | 	log.message.timestamp.type = CreateTime
kafka-1            | 	log.preallocate = false
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | 	log.retention.bytes = -1
kafka-1            | 	log.retention.check.interval.ms = 300000
kafka-1            | 	log.retention.hours = 168
kafka-1            | 	log.retention.minutes = null
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6666,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
kafka-1            | 	log.retention.ms = null
kafka-1            | 	log.roll.hours = 168
kafka-1            | 	log.roll.jitter.hours = 0
kafka-1            | 	log.roll.jitter.ms = null
kafka-1            | 	log.roll.ms = null
kafka-1            | 	log.segment.bytes = 1073741824
kafka-1            | 	log.segment.delete.delay.ms = 60000
kafka-1            | 	max.connection.creation.rate = 2147483647
kafka-1            | 	max.connections = 2147483647
kafka-1            | 	max.connections.per.ip = 2147483647
kafka-1            | 	max.connections.per.ip.overrides = 
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	max.incremental.fetch.session.cache.slots = 1000
kafka-1            | 	message.max.bytes = 1048588
kafka-1            | 	metadata.log.dir = null
kafka-1            | 	metadata.log.max.record.bytes.between.snapshots = 20971520
kafka-1            | 	metadata.log.segment.bytes = 1073741824
kafka-1            | 	metadata.log.segment.min.bytes = 8388608
kafka-1            | 	metadata.log.segment.ms = 604800000
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
kafka-1            | 	metadata.max.retention.bytes = -1
kafka-1            | 	metadata.max.retention.ms = 604800000
kafka-1            | 	metric.reporters = []
kafka-1            | 	metrics.num.samples = 2
kafka-1            | 	metrics.recording.level = INFO
kafka-1            | 	metrics.sample.window.ms = 30000
kafka-1            | 	min.insync.replicas = 1
kafka-1            | 	node.id = -1
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6666,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	num.io.threads = 8
kafka-1            | 	num.network.threads = 3
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | 	num.partitions = 1
kafka-1            | 	num.recovery.threads.per.data.dir = 1
kafka-1            | 	num.replica.alter.log.dirs.threads = null
kafka-1            | 	num.replica.fetchers = 1
kafka-1            | 	offset.metadata.max.bytes = 4096
kafka-1            | 	offsets.commit.required.acks = -1
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	offsets.commit.timeout.ms = 5000
kafka-1            | 	offsets.load.buffer.size = 5242880
kafka-1            | 	offsets.retention.check.interval.ms = 600000
kafka-1            | 	offsets.retention.minutes = 10080
kafka-1            | 	offsets.topic.compression.codec = 0
kafka-1            | 	offsets.topic.num.partitions = 50
kafka-1            | 	offsets.topic.replication.factor = 1
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | 	offsets.topic.segment.bytes = 104857600
kafka-1            | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka-1            | 	password.encoder.iterations = 4096
kafka-1            | 	password.encoder.key.length = 128
kafka-1            | 	password.encoder.keyfactory.algorithm = null
kafka-1            | 	password.encoder.old.secret = null
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
kafka-1            | 	password.encoder.secret = null
kafka-1            | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka-1            | 	process.roles = []
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:21.451Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:21.453Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":336}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:21.807Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:21.809Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":624}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:22.445Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:22.446Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1014}
kafka-1            | 	producer.purgatory.purge.interval.requests = 1000
kafka-1            | 	queued.max.request.bytes = -1
kafka-1            | 	queued.max.requests = 500
kafka-1            | 	quota.window.num = 11
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
kafka-1            | 	quota.window.size.seconds = 1
kafka-1            | 	remote.log.index.file.cache.total.size.bytes = 1073741824
kafka-1            | 	remote.log.manager.task.interval.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.max.ms = 30000
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” prevent committing .env to code: https://dotenvx.com/precommit
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:43:34.155Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
kafka-1            | 	remote.log.manager.task.retry.backoff.ms = 500
kafka-1            | 	remote.log.manager.task.retry.jitter = 0.2
kafka-1            | 	remote.log.manager.thread.pool.size = 10
kafka-1            | 	remote.log.metadata.manager.class.name = null
kafka-1            | 	remote.log.metadata.manager.class.path = null
kafka-1            | 	remote.log.metadata.manager.impl.prefix = null
kafka-1            | 	remote.log.metadata.manager.listener.name = null
kafka-1            | 	remote.log.reader.max.pending.tasks = 100
kafka-1            | 	remote.log.reader.threads = 10
kafka-1            | 	remote.log.storage.manager.class.name = null
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
kafka-1            | 	remote.log.storage.manager.class.path = null
kafka-1            | 	remote.log.storage.manager.impl.prefix = null
kafka-1            | 	remote.log.storage.system.enable = false
kafka-1            | 	replica.fetch.backoff.ms = 1000
kafka-1            | 	replica.fetch.max.bytes = 1048576
kafka-1            | 	replica.fetch.min.bytes = 1
kafka-1            | 	replica.fetch.response.max.bytes = 10485760
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
kafka-1            | 	replica.fetch.wait.max.ms = 500
kafka-1            | 	replica.high.watermark.checkpoint.interval.ms = 5000
kafka-1            | 	replica.lag.time.max.ms = 30000
kafka-1            | 	replica.selector.class = null
kafka-1            | 	replica.socket.receive.buffer.bytes = 65536
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
kafka-1            | 	replica.socket.timeout.ms = 30000
kafka-1            | 	replication.quota.window.num = 11
kafka-1            | 	replication.quota.window.size.seconds = 1
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
kafka-1            | 	request.timeout.ms = 30000
kafka-1            | 	reserved.broker.max.id = 1000
kafka-1            | 	sasl.client.callback.handler.class = null
kafka-1            | 	sasl.enabled.mechanisms = [GSSAPI]
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	sasl.jaas.config = null
kafka-1            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka-1            | 	sasl.kerberos.min.time.before.relogin = 60000
kafka-1            | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka-1            | 	sasl.kerberos.service.name = null
kafka-1            | 	sasl.kerberos.ticket.renew.jitter = 0.05
kafka-1            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:34.276Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:23.472Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:23.474Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2360}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:25.849Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:25.851Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5494}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:31.363Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:34.276Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":272}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:34.567Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:34.569Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":500}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.087Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:31.365Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9404}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:47:31.367Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:31.382Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	sasl.login.callback.handler.class = null
kafka-1            | 	sasl.login.class = null
kafka-1            | 	sasl.login.refresh.buffer.seconds = 300
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:35.088Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":988}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:36.091Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:36.093Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1624}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:37.723Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:37.724Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3550}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:41.284Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:41.284Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8172}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:31.383Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":249}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:31.643Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:31.644Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":558}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:32.216Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:32.218Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1336}
product-service-1  |     ... 3 lines matching cause stack trace ...
kafka-1            | 	sasl.login.refresh.min.period.seconds = 60
kafka-1            | 	sasl.login.refresh.window.factor = 0.8
kafka-1            | 	sasl.login.refresh.window.jitter = 0.05
kafka-1            | 	sasl.mechanism.controller.protocol = GSSAPI
kafka-1            | 	sasl.mechanism.inter.broker.protocol = GSSAPI
kafka-1            | 	sasl.server.callback.handler.class = null
kafka-1            | 	security.inter.broker.protocol = PLAINTEXT
kafka-1            | 	security.providers = null
kafka-1            | 	socket.connection.setup.timeout.max.ms = 30000
kafka-1            | 	socket.connection.setup.timeout.ms = 10000
kafka-1            | 	socket.receive.buffer.bytes = 102400
kafka-1            | 	socket.request.max.bytes = 104857600
kafka-1            | 	socket.send.buffer.bytes = 102400
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:47:33 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:47:33.047Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:33.567Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:33.568Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2518}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:36.101Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:36.103Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4652}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:40.765Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:40.765Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10098}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8172,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | 	ssl.cipher.suites = []
kafka-1            | 	ssl.client.auth = none
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 10098,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka-1            | 	ssl.endpoint.identification.algorithm = https
kafka-1            | 	ssl.engine.factory.class = null
kafka-1            | 	ssl.key.password = null
kafka-1            | 	ssl.keymanager.algorithm = SunX509
kafka-1            | 	ssl.keystore.certificate.chain = null
kafka-1            | 	ssl.keystore.key = null
kafka-1            | 	ssl.keystore.location = null
kafka-1            | 	ssl.keystore.password = null
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
kafka-1            | 	ssl.keystore.type = JKS
kafka-1            | 	ssl.principal.mapping.rules = DEFAULT
kafka-1            | 	ssl.protocol = TLSv1.3
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ¤– agentic secret storage: https://dotenvx.com/as2
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:47:41.737Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
kafka-1            | 	ssl.provider = null
kafka-1            | 	ssl.secure.random.implementation = null
kafka-1            | 	ssl.trustmanager.algorithm = PKIX
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
kafka-1            | 	ssl.truststore.certificates = null
kafka-1            | 	ssl.truststore.location = null
kafka-1            | 	ssl.truststore.password = null
kafka-1            | 	ssl.truststore.type = JKS
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
kafka-1            | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
kafka-1            | 	transaction.max.timeout.ms = 900000
kafka-1            | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8172,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:41.857Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:41.858Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":328}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:42.200Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	transaction.state.log.load.buffer.size = 5242880
kafka-1            | 	transaction.state.log.min.isr = 2
kafka-1            | 	transaction.state.log.num.partitions = 50
kafka-1            | 	transaction.state.log.replication.factor = 3
kafka-1            | 	transaction.state.log.segment.bytes = 104857600
kafka-1            | 	transactional.id.expiration.ms = 604800000
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:42.204Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":598}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:42.815Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:42.816Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1358}
kafka-1            | 	unclean.leader.election.enable = false
kafka-1            | 	zookeeper.clientCnxnSocket = null
kafka-1            | 	zookeeper.connect = zookeeper:2181
kafka-1            | 	zookeeper.connection.timeout.ms = null
kafka-1            | 	zookeeper.max.in.flight.requests = 10
kafka-1            | 	zookeeper.session.timeout.ms = 18000
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:44.182Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:44.183Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2998}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:47.193Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:47.195Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6208}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:53.417Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:53.419Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12316}
kafka-1            | 	zookeeper.set.acl = false
kafka-1            | 	zookeeper.ssl.cipher.suites = null
kafka-1            | 	zookeeper.ssl.client.enable = false
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:47:53.421Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:53.437Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	zookeeper.ssl.crl.enable = false
kafka-1            | 	zookeeper.ssl.enabled.protocols = null
kafka-1            | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka-1            | 	zookeeper.ssl.keystore.location = null
kafka-1            | 	zookeeper.ssl.keystore.password = null
kafka-1            | 	zookeeper.ssl.keystore.type = null
kafka-1            | 	zookeeper.ssl.ocsp.enable = false
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:53.439Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":353}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:53.805Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:53.806Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":616}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:54.431Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	zookeeper.ssl.protocol = TLSv1.2
kafka-1            | 	zookeeper.ssl.truststore.location = null
kafka-1            | 	zookeeper.ssl.truststore.password = null
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:54.432Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1356}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:55.799Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:55.800Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3140}
kafka-1            | 	zookeeper.ssl.truststore.type = null
kafka-1            | 	zookeeper.sync.time.ms = 2000
kafka-1            |  (kafka.server.KafkaConfig)
kafka-1            | [2026-02-25 10:39:37,952] INFO KafkaConfig values: 
kafka-1            | 	advertised.listeners = PLAINTEXT://kafka:9092
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
kafka-1            | 	alter.config.policy.class.name = null
kafka-1            | 	alter.log.dirs.replication.quota.window.num = 11
kafka-1            | 	alter.log.dirs.replication.quota.window.size.seconds = 1
kafka-1            | 	authorizer.class.name = 
kafka-1            | 	auto.create.topics.enable = true
kafka-1            | 	auto.leader.rebalance.enable = true
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:58.970Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:47:58.972Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5482}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:48:03 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:48:03.087Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:04.464Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:04.464Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8890}
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:43:42.022Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
kafka-1            | 	background.threads = 10
kafka-1            | 	broker.heartbeat.interval.ms = 2000
kafka-1            | 	broker.id = 1
kafka-1            | 	broker.id.generation.enable = true
kafka-1            | 	broker.rack = null
kafka-1            | 	broker.session.timeout.ms = 9000
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	client.quota.callback.class = null
kafka-1            | 	compression.type = producer
kafka-1            | 	connection.failed.authentication.delay.ms = 100
kafka-1            | 	connections.max.idle.ms = 600000
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 8890,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | 	connections.max.reauth.ms = 0
kafka-1            | 	control.plane.listener.name = null
kafka-1            | 	controlled.shutdown.enable = true
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
kafka-1            | 	controlled.shutdown.max.retries = 3
kafka-1            | 	controlled.shutdown.retry.backoff.ms = 5000
kafka-1            | 	controller.listener.names = null
kafka-1            | 	controller.quorum.append.linger.ms = 25
kafka-1            | 	controller.quorum.election.backoff.max.ms = 1000
kafka-1            | 	controller.quorum.election.timeout.ms = 1000
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
kafka-1            | 	controller.quorum.fetch.timeout.ms = 2000
kafka-1            | 	controller.quorum.request.timeout.ms = 2000
kafka-1            | 	controller.quorum.retry.backoff.ms = 20
kafka-1            | 	controller.quorum.voters = []
order-service-1    |   }
order-service-1    | }
order-service-1    | 
kafka-1            | 	controller.quota.window.num = 11
kafka-1            | 	controller.quota.window.size.seconds = 1
kafka-1            | 	controller.socket.timeout.ms = 30000
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
kafka-1            | 	create.topic.policy.class.name = null
kafka-1            | 	default.replication.factor = 1
kafka-1            | 	delegation.token.expiry.check.interval.ms = 3600000
kafka-1            | 	delegation.token.expiry.time.ms = 86400000
kafka-1            | 	delegation.token.master.key = null
kafka-1            | 	delegation.token.max.lifetime.ms = 604800000
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
kafka-1            | 	delegation.token.secret.key = null
kafka-1            | 	delete.records.purgatory.purge.interval.requests = 1
kafka-1            | 	delete.topic.enable = true
kafka-1            | 	fetch.max.bytes = 57671680
kafka-1            | 	fetch.purgatory.purge.interval.requests = 1000
kafka-1            | 	group.initial.rebalance.delay.ms = 0
order-service-1    | > node index.js
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	group.max.session.timeout.ms = 1800000
kafka-1            | 	group.max.size = 2147483647
kafka-1            | 	group.min.session.timeout.ms = 6000
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ¤– agentic secret storage: https://dotenvx.com/as2
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:48:05.784Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | 	initial.broker.registration.timeout.ms = 60000
kafka-1            | 	inter.broker.listener.name = null
kafka-1            | 	inter.broker.protocol.version = 3.0-IV1
kafka-1            | 	kafka.metrics.polling.interval.secs = 10
kafka-1            | 	kafka.metrics.reporters = []
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	leader.imbalance.check.interval.seconds = 300
kafka-1            | 	leader.imbalance.per.broker.percentage = 10
kafka-1            | 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
kafka-1            | 	listeners = PLAINTEXT://0.0.0.0:9092
kafka-1            | 	log.cleaner.backoff.ms = 15000
kafka-1            | 	log.cleaner.dedupe.buffer.size = 134217728
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:05.980Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:05.981Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":259}
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.111Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.112Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":262}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:06.252Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:06.253Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":518}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:06.786Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:06.787Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":988}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:07.785Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:07.788Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2200}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:10.008Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:10.017Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4340}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:14.374Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:14.377Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10236}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.382Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.383Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":548}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.945Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:48:14.383Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:14.410Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:14.412Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":358}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:14.780Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:14.781Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":618}
kafka-1            | 	log.cleaner.delete.retention.ms = 86400000
kafka-1            | 	log.cleaner.enable = true
kafka-1            | 	log.cleaner.io.buffer.load.factor = 0.9
kafka-1            | 	log.cleaner.io.buffer.size = 524288
kafka-1            | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
kafka-1            | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
kafka-1            | 	log.cleaner.min.cleanable.ratio = 0.5
kafka-1            | 	log.cleaner.min.compaction.lag.ms = 0
kafka-1            | 	log.cleaner.threads = 1
kafka-1            | 	log.cleanup.policy = [delete]
kafka-1            | 	log.dir = /tmp/kafka-logs
kafka-1            | 	log.dirs = /var/lib/kafka/data
kafka-1            | 	log.flush.interval.messages = 9223372036854775807
kafka-1            | 	log.flush.interval.ms = null
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:42.946Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1238}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:44.202Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:44.205Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2368}
kafka-1            | 	log.flush.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.flush.scheduler.interval.ms = 9223372036854775807
kafka-1            | 	log.flush.start.offset.checkpoint.interval.ms = 60000
kafka-1            | 	log.index.interval.bytes = 4096
kafka-1            | 	log.index.size.max.bytes = 10485760
kafka-1            | 	log.message.downconversion.enable = true
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:15.411Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:15.412Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1304}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:16.729Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:16.730Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3058}
kafka-1            | 	log.message.format.version = 3.0-IV1
kafka-1            | 	log.message.timestamp.difference.max.ms = 9223372036854775807
kafka-1            | 	log.message.timestamp.type = CreateTime
kafka-1            | 	log.preallocate = false
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:48:18 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:48:18.050Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:19.811Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:19.813Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":7134}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:26.964Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:26.965Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":16782}
order-service-1    | node:internal/process/promises:288
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:46.585Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:46.586Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4332}
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
kafka-1            | 	log.retention.bytes = -1
kafka-1            | 	log.retention.check.interval.ms = 300000
kafka-1            | 	log.retention.hours = 168
kafka-1            | 	log.retention.minutes = null
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:50.930Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:50.932Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9324}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
kafka-1            | 	log.retention.ms = null
kafka-1            | 	log.roll.hours = 168
kafka-1            | 	log.roll.jitter.hours = 0
kafka-1            | 	log.roll.jitter.ms = null
kafka-1            | 	log.roll.ms = null
kafka-1            | 	log.segment.bytes = 1073741824
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
kafka-1            | 	log.segment.delete.delay.ms = 60000
kafka-1            | 	max.connection.creation.rate = 2147483647
kafka-1            | 	max.connections = 2147483647
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 16782,
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
kafka-1            | 	max.connections.per.ip = 2147483647
kafka-1            | 	max.connections.per.ip.overrides = 
kafka-1            | 	max.incremental.fetch.session.cache.slots = 1000
kafka-1            | 	message.max.bytes = 1048588
kafka-1            | 	metadata.log.dir = null
product-service-1  |   retryTime: 9324,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	metadata.log.max.record.bytes.between.snapshots = 20971520
kafka-1            | 	metadata.log.segment.bytes = 1073741824
kafka-1            | 	metadata.log.segment.min.bytes = 8388608
kafka-1            | 	metadata.log.segment.ms = 604800000
kafka-1            | 	metadata.max.retention.bytes = -1
kafka-1            | 	metadata.max.retention.ms = 604800000
kafka-1            | 	metric.reporters = []
kafka-1            | 	metrics.num.samples = 2
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
kafka-1            | 	metrics.recording.level = INFO
kafka-1            | 	metrics.sample.window.ms = 30000
kafka-1            | 	min.insync.replicas = 1
kafka-1            | 	node.id = -1
kafka-1            | 	num.io.threads = 8
kafka-1            | 	num.network.threads = 3
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:48:27.871Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
kafka-1            | 	num.partitions = 1
kafka-1            | 	num.recovery.threads.per.data.dir = 1
kafka-1            | 	num.replica.alter.log.dirs.threads = null
kafka-1            | 	num.replica.fetchers = 1
kafka-1            | 	offset.metadata.max.bytes = 4096
kafka-1            | 	offsets.commit.required.acks = -1
kafka-1            | 	offsets.commit.timeout.ms = 5000
kafka-1            | 	offsets.load.buffer.size = 5242880
kafka-1            | 	offsets.retention.check.interval.ms = 600000
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | 	offsets.retention.minutes = 10080
kafka-1            | 	offsets.topic.compression.codec = 0
kafka-1            | 	offsets.topic.num.partitions = 50
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 9324,
kafka-1            | 	offsets.topic.replication.factor = 1
kafka-1            | 	offsets.topic.segment.bytes = 104857600
kafka-1            | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
kafka-1            | 	password.encoder.iterations = 4096
kafka-1            | 	password.encoder.key.length = 128
kafka-1            | 	password.encoder.keyfactory.algorithm = null
kafka-1            | 	password.encoder.old.secret = null
kafka-1            | 	password.encoder.secret = null
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
kafka-1            | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
kafka-1            | 	process.roles = []
kafka-1            | 	producer.purgatory.purge.interval.requests = 1000
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	queued.max.request.bytes = -1
kafka-1            | 	queued.max.requests = 500
kafka-1            | 	quota.window.num = 11
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:28.018Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:28.019Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":356}
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  write to custom object with { processEnv: myObject }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:43:52.004Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:28.385Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:28.387Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":720}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:29.119Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:29.120Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1524}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:30.659Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:30.661Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2850}
kafka-1            | 	quota.window.size.seconds = 1
kafka-1            | 	remote.log.index.file.cache.total.size.bytes = 1073741824
kafka-1            | 	remote.log.manager.task.interval.ms = 30000
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:33.529Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:33.530Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4750}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.314Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	remote.log.manager.task.retry.backoff.max.ms = 30000
kafka-1            | 	remote.log.manager.task.retry.backoff.ms = 500
kafka-1            | 	remote.log.manager.task.retry.jitter = 0.2
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.316Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9514}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:48:38.317Z"}
order-service-1    | Server is running on port 3001
kafka-1            | 	remote.log.manager.thread.pool.size = 10
kafka-1            | 	remote.log.metadata.manager.class.name = null
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.329Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.330Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":297}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.646Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.647Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":712}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:39.373Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:39.375Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1526}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:40.941Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | 	remote.log.metadata.manager.class.path = null
kafka-1            | 	remote.log.metadata.manager.impl.prefix = null
kafka-1            | 	remote.log.metadata.manager.listener.name = null
kafka-1            | 	remote.log.reader.max.pending.tasks = 100
kafka-1            | 	remote.log.reader.threads = 10
kafka-1            | 	remote.log.storage.manager.class.name = null
kafka-1            | 	remote.log.storage.manager.class.path = null
kafka-1            | 	remote.log.storage.manager.impl.prefix = null
kafka-1            | 	remote.log.storage.system.enable = false
kafka-1            | 	replica.fetch.backoff.ms = 1000
kafka-1            | 	replica.fetch.max.bytes = 1048576
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
kafka-1            | 	replica.fetch.min.bytes = 1
kafka-1            | 	replica.fetch.response.max.bytes = 10485760
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	replica.fetch.wait.max.ms = 500
kafka-1            | 	replica.high.watermark.checkpoint.interval.ms = 5000
kafka-1            | 	replica.lag.time.max.ms = 30000
kafka-1            | 	replica.selector.class = null
kafka-1            | 	replica.socket.receive.buffer.bytes = 65536
kafka-1            | 	replica.socket.timeout.ms = 30000
kafka-1            | 	replication.quota.window.num = 11
kafka-1            | 	replication.quota.window.size.seconds = 1
kafka-1            | 	request.timeout.ms = 30000
kafka-1            | 	reserved.broker.max.id = 1000
kafka-1            | 	sasl.client.callback.handler.class = null
kafka-1            | 	sasl.enabled.mechanisms = [GSSAPI]
kafka-1            | 	sasl.jaas.config = null
kafka-1            | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
kafka-1            | 	sasl.kerberos.min.time.before.relogin = 60000
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:40.946Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2870}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:43.851Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:43.854Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6084}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:48:48 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:48:48.075Z"}
kafka-1            | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
kafka-1            | 	sasl.kerberos.service.name = null
kafka-1            | 	sasl.kerberos.ticket.renew.jitter = 0.05
kafka-1            | 	sasl.kerberos.ticket.renew.window.factor = 0.8
kafka-1            | 	sasl.login.callback.handler.class = null
kafka-1            | 	sasl.login.class = null
kafka-1            | 	sasl.login.refresh.buffer.seconds = 300
kafka-1            | 	sasl.login.refresh.min.period.seconds = 60
kafka-1            | 	sasl.login.refresh.window.factor = 0.8
kafka-1            | 	sasl.login.refresh.window.jitter = 0.05
kafka-1            | 	sasl.mechanism.controller.protocol = GSSAPI
kafka-1            | 	sasl.mechanism.inter.broker.protocol = GSSAPI
kafka-1            | 	sasl.server.callback.handler.class = null
kafka-1            | 	security.inter.broker.protocol = PLAINTEXT
kafka-1            | 	security.providers = null
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.113Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.114Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":256}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.385Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.387Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":518}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.918Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:52.918Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1234}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:54.167Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:54.169Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2104}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:56.289Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:49.996Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:49.998Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":14050}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:43:56.290Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3748}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:00.050Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:00.051Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6168}
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
kafka-1            | 	socket.connection.setup.timeout.max.ms = 30000
kafka-1            | 	socket.connection.setup.timeout.ms = 10000
kafka-1            | 	socket.receive.buffer.bytes = 102400
kafka-1            | 	socket.request.max.bytes = 104857600
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 14050,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |   retryTime: 6168,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
kafka-1            | 	socket.send.buffer.bytes = 102400
kafka-1            | 	ssl.cipher.suites = []
kafka-1            | 	ssl.client.auth = none
kafka-1            | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
kafka-1            | 	ssl.endpoint.identification.algorithm = https
kafka-1            | 	ssl.engine.factory.class = null
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | 	ssl.key.password = null
kafka-1            | 	ssl.keymanager.algorithm = SunX509
kafka-1            | 	ssl.keystore.certificate.chain = null
kafka-1            | 	ssl.keystore.key = null
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | 	ssl.keystore.location = null
kafka-1            | 	ssl.keystore.password = null
kafka-1            | 	ssl.keystore.type = JKS
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:48:51.268Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
kafka-1            | 	ssl.principal.mapping.rules = DEFAULT
kafka-1            | 	ssl.protocol = TLSv1.3
kafka-1            | 	ssl.provider = null
kafka-1            | 	ssl.secure.random.implementation = null
kafka-1            | 	ssl.trustmanager.algorithm = PKIX
kafka-1            | 	ssl.truststore.certificates = null
kafka-1            | 	ssl.truststore.location = null
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6168,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | 	ssl.truststore.password = null
kafka-1            | 	ssl.truststore.type = JKS
kafka-1            | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	transaction.max.timeout.ms = 900000
kafka-1            | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
kafka-1            | 	transaction.state.log.load.buffer.size = 5242880
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	transaction.state.log.min.isr = 2
kafka-1            | 	transaction.state.log.num.partitions = 50
kafka-1            | 	transaction.state.log.replication.factor = 3
kafka-1            | 	transaction.state.log.segment.bytes = 104857600
kafka-1            | 	transactional.id.expiration.ms = 604800000
kafka-1            | 	unclean.leader.election.enable = false
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ—‚ï¸ backup and recover secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:44:01.433Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
kafka-1            | 	zookeeper.clientCnxnSocket = null
kafka-1            | 	zookeeper.connect = zookeeper:2181
kafka-1            | 	zookeeper.connection.timeout.ms = null
kafka-1            | 	zookeeper.max.in.flight.requests = 10
kafka-1            | 	zookeeper.session.timeout.ms = 18000
kafka-1            | 	zookeeper.set.acl = false
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:51.420Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:51.421Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":360}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:51.795Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:51.796Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":704}
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | 	zookeeper.ssl.cipher.suites = null
kafka-1            | 	zookeeper.ssl.client.enable = false
kafka-1            | 	zookeeper.ssl.crl.enable = false
kafka-1            | 	zookeeper.ssl.enabled.protocols = null
kafka-1            | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
kafka-1            | 	zookeeper.ssl.keystore.location = null
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:52.512Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:52.513Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1582}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:54.112Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:54.115Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3426}
kafka-1            | 	zookeeper.ssl.keystore.password = null
kafka-1            | 	zookeeper.ssl.keystore.type = null
kafka-1            | 	zookeeper.ssl.ocsp.enable = false
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:57.555Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:48:57.556Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5602}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:03.175Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:03.179Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11814}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:49:03.182Z"}
order-service-1    | Server is running on port 3001
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:03.201Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:03.202Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":290}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:03.508Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:03.511Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":650}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:04.176Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:04.179Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1558}
kafka-1            | 	zookeeper.ssl.protocol = TLSv1.2
kafka-1            | 	zookeeper.ssl.truststore.location = null
kafka-1            | 	zookeeper.ssl.truststore.password = null
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:05.746Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:05.747Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2664}
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:01.545Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:08.424Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:08.428Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5038}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:13.486Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:13.487Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11288}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:01.546Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":323}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:01.879Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:01.880Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":626}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:02.517Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:02.518Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1100}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:03.630Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:03.632Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1968}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:05.612Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:05.613Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4568}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:10.190Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:10.191Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7562}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
kafka-1            | 	zookeeper.ssl.truststore.type = null
kafka-1            | 	zookeeper.sync.time.ms = 2000
kafka-1            |  (kafka.server.KafkaConfig)
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7562,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2026-02-25 10:39:38,003] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:38,004] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:38,008] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 11288,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
kafka-1            | [2026-02-25 10:39:38,011] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:38,076] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,092] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,181] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
kafka-1            | [2026-02-25 10:39:38,183] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,184] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,186] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Producer state recovery took 2ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,212] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,212] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,212] INFO [LogLoader partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    |     [cause]: undefined
product-service-1  |     [cause]: undefined
product-service-1  |   }
order-service-1    |   }
order-service-1    | }
order-service-1    | 
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2026-02-25 10:39:38,249] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-8, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=8, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 141ms (1/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,253] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,254] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,254] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ¤– agentic secret storage: https://dotenvx.com/as2
kafka-1            | [2026-02-25 10:39:38,254] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,256] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,256] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,256] INFO [LogLoader partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,261] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-38, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=38, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 12ms (2/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,265] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,266] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,266] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7562,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:49:14.426Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ‘¥ sync secrets across teammates & machines: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:44:12.278Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 10:39:38,266] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,271] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,273] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,273] INFO [LogLoader partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 10:39:38,277] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-13, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=13, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 15ms (3/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,281] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,281] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,281] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
kafka-1            | [2026-02-25 10:39:38,281] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,285] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
kafka-1            | [2026-02-25 10:39:38,285] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,285] INFO [LogLoader partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,292] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-4, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=4, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 16ms (4/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 10:39:38,294] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,296] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,296] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,296] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,298] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,298] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
kafka-1            | [2026-02-25 10:39:38,298] INFO [LogLoader partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,302] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-22, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=22, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 10ms (5/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 10:39:38,304] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,304] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,305] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:14.593Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:14.594Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":277}
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:14.883Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:14.885Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":580}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:15.476Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:15.478Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1342}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:16.838Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:16.841Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3106}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:19.962Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.363Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.364Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":318}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.701Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:12.702Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":558}
kafka-1            | [2026-02-25 10:39:38,305] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,307] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,307] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:19.965Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5140}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.117Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2026-02-25 10:39:38,307] INFO [LogLoader partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,311] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-29, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=29, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 9ms (6/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,314] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,314] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.119Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9254}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:49:25.122Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.135Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:13.275Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:13.276Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1138}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:14.431Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:14.433Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2212}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:16.663Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:16.665Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3802}
kafka-1            | [2026-02-25 10:39:38,314] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,314] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,316] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,316] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:20.474Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:20.475Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7532}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
kafka-1            | [2026-02-25 10:39:38,316] INFO [LogLoader partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,319] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-24, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=24, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (7/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,321] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,321] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7532,
kafka-1            | [2026-02-25 10:39:38,321] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,321] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,324] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
kafka-1            | [2026-02-25 10:39:38,324] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,324] INFO [LogLoader partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,328] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-40, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=40, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (8/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,375] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,376] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,376] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.136Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":278}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.424Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.425Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":564}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:26.002Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:26.004Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1074}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:27.091Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:27.092Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2196}
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:29.307Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:29.315Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4636}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:49:33 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:49:33.058Z"}
kafka-1            | [2026-02-25 10:39:38,376] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,379] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,379] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:33.966Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:33.968Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10554}
order-service-1    | node:internal/process/promises:288
kafka-1            | [2026-02-25 10:39:38,379] INFO [LogLoader partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,388] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-3, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=3, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 61ms (9/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,390] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,390] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,391] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7532,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
kafka-1            | [2026-02-25 10:39:38,391] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,393] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,393] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,393] INFO [LogLoader partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,397] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-39, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=39, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (10/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,400] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,400] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,401] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,401] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,403] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,403] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,403] INFO [LogLoader partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,407] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-15, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=15, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 9ms (11/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,410] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
kafka-1            | [2026-02-25 10:39:38,410] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,410] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,410] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    |   retryTime: 10554,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2026-02-25 10:39:38,412] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,412] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,412] INFO [LogLoader partition=order-events-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,415] INFO Completed load of Log(dir=/var/lib/kafka/data/order-events-0, topicId=zHT0-AUUTHG_9kd8-uAViQ, topic=order-events, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (12/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,418] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,418] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | [2026-02-25 10:39:38,418] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,418] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,420] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,420] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  write to custom object with { processEnv: myObject }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:44:24.274Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:49:35.166Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 10:39:38,420] INFO [LogLoader partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,424] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-5, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=5, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (13/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,425] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,425] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,425] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,426] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,428] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,428] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,428] INFO [LogLoader partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,432] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-49, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=49, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 9ms (14/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,435] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
kafka-1            | [2026-02-25 10:39:38,435] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,435] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,435] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,437] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,437] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,437] INFO [LogLoader partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,442] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-0, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 9ms (15/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,445] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,445] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | [2026-02-25 10:39:38,445] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,445] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,448] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,448] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,448] INFO [LogLoader partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,451] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-46, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=46, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 10ms (16/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,453] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,453] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,453] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,453] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
kafka-1            | [2026-02-25 10:39:38,455] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,455] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,455] INFO [LogLoader partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.373Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.374Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":305}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.696Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:24.698Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":710}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:25.420Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:25.421Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1292}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:26.727Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:26.728Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2780}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:29.525Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:29.525Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5038}
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 10:39:38,460] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-44, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=44, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (17/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,461] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,462] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
kafka-1            | [2026-02-25 10:39:38,462] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,462] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,464] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,464] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,464] INFO [LogLoader partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:34.582Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:34.584Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11968}
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:35.690Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:35.692Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":334}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:36.045Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:36.047Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":748}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:36.811Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:36.821Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1350}
kafka-1            | [2026-02-25 10:39:38,468] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-37, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=37, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (18/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,470] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,470] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:38.188Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:38.190Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2182}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:40.387Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:40.388Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5088}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:45.487Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2026-02-25 10:39:38,470] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,470] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,472] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,472] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,472] INFO [LogLoader partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,475] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-12, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=12, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (19/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,477] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,477] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,477] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,477] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,479] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,479] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,479] INFO [LogLoader partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,483] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-27, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=27, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (20/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,484] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,485] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,485] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,485] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:45.490Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11662}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:49:45.493Z"}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
kafka-1            | [2026-02-25 10:39:38,486] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,486] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,486] INFO [LogLoader partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,494] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-33, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=33, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 11ms (21/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,496] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,496] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,496] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,496] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,499] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,499] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,499] INFO [LogLoader partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,502] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-16, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=16, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (22/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,504] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,504] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,504] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,504] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,506] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,506] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,506] INFO [LogLoader partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,508] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-26, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=26, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 6ms (23/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,511] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,511] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,511] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,511] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,513] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,513] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,513] INFO [LogLoader partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,518] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-17, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=17, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 9ms (24/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,520] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,520] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,520] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,520] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,522] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,523] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,523] INFO [LogLoader partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:45.531Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:45.538Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":320}
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:45.869Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:45.870Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":668}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:46.548Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 11968,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2026-02-25 10:39:38,528] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-47, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=47, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 11ms (25/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,533] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,533] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,533] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,533] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,539] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,539] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,539] INFO [LogLoader partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 11968,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2026-02-25 10:39:38,542] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-21, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=21, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 13ms (26/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,548] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,548] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,548] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,548] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,551] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,552] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,552] INFO [LogLoader partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2026-02-25 10:39:38,556] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-18, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=18, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 14ms (27/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,558] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,558] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,558] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:46.548Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1394}
kafka-1            | [2026-02-25 10:39:38,558] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,560] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,561] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,561] INFO [LogLoader partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:47.957Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:47.957Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3202}
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:49:48 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:49:48.060Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:51.170Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:51.172Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":7214}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:58.405Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:58.407Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":14664}
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
kafka-1            | [2026-02-25 10:39:38,563] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-14, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=14, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (28/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,565] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,565] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,565] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  |   }
product-service-1  | }
kafka-1            | [2026-02-25 10:39:38,565] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,567] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
kafka-1            | [2026-02-25 10:39:38,567] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,567] INFO [LogLoader partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,571] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-41, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=41, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (29/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,572] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,572] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,572] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:44:35.198Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 14664,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
kafka-1            | [2026-02-25 10:39:38,572] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,574] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,574] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,574] INFO [LogLoader partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,576] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-20, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=20, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 6ms (30/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,578] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
kafka-1            | [2026-02-25 10:39:38,578] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,578] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ” encrypt with Dotenvx: https://dotenvx.com
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:49:59.260Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
kafka-1            | [2026-02-25 10:39:38,578] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,580] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 10:39:38,581] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,581] INFO [LogLoader partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,585] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-34, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=34, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (31/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,587] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 10:39:38,587] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,587] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,587] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,589] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,589] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
kafka-1            | [2026-02-25 10:39:38,589] INFO [LogLoader partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,590] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-10, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=10, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 5ms (32/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,593] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,593] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,593] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,593] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,595] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,595] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,595] INFO [LogLoader partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
kafka-1            | [2026-02-25 10:39:38,599] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-48, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=48, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (33/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,600] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,600] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,600] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,600] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,602] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,602] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,603] INFO [LogLoader partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,606] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-36, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=36, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (34/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,607] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,607] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
kafka-1            | [2026-02-25 10:39:38,607] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,608] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Producer state recovery took 1ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:35.315Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:35.316Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":331}
kafka-1            | [2026-02-25 10:39:38,610] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:35.658Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:35.659Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":614}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:36.283Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:36.283Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1084}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:37.380Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:37.383Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2156}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.396Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.396Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":300}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:39.546Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:39.546Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5056}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:44.616Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:44.617Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10812}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.711Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.713Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":550}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:00.327Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:00.344Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1270}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:01.628Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:01.630Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2984}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:04.635Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:04.637Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6840}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.493Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.494Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12082}
order-service-1    | {"error":"Connection error: getaddrinfo ENOTFOUND kafka","level":"error","message":"Failed to connect Kafka producer on startup","timestamp":"2026-02-25T10:50:11.496Z"}
order-service-1    | Server is running on port 3001
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.515Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.516Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":303}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.832Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.833Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":694}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:12.537Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:12.538Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1448}
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10812,
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:14.000Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:14.001Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3236}
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:17.249Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:17.250Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5360}
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
order-service-1    | {"level":"info","message":"::ffff:172.18.0.2 - - [25/Feb/2026:10:50:18 +0000] \"GET /metrics HTTP/1.1\" 200 - \"-\" \"Prometheus/3.7.1\"","timestamp":"2026-02-25T10:50:18.058Z"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:22.630Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:22.632Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11900}
order-service-1    | node:internal/process/promises:288
order-service-1    |             triggerUncaughtException(err, true /* fromPromise */);
kafka-1            | [2026-02-25 10:39:38,610] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,610] INFO [LogLoader partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,611] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-31, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=31, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 5ms (35/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
order-service-1    |             ^
order-service-1    | 
order-service-1    | KafkaJSNonRetriableError
kafka-1            | [2026-02-25 10:39:38,613] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,613] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
order-service-1    |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |     ... 3 lines matching cause stack trace ...
order-service-1    |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   name: 'KafkaJSNumberOfRetriesExceeded',
order-service-1    |   retriable: false,
kafka-1            | [2026-02-25 10:39:38,613] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,613] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,615] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,615] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,615] INFO [LogLoader partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |   helpUrl: undefined,
order-service-1    |   retryCount: 5,
order-service-1    |   retryTime: 11900,
order-service-1    |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
order-service-1    |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
order-service-1    |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10812,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
order-service-1    |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
order-service-1    |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
order-service-1    |     retriable: true,
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
order-service-1    |     helpUrl: undefined,
order-service-1    |     broker: 'kafka:9092',
order-service-1    |     code: 'ENOTFOUND',
order-service-1    |     [cause]: undefined
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
order-service-1    |   }
order-service-1    | }
order-service-1    | 
order-service-1    | Node.js v18.20.8
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:44:45.313Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
order-service-1    | 
order-service-1    | > order-service@1.0.0 start
order-service-1    | > node index.js
order-service-1    | 
order-service-1    | [dotenv@17.3.1] injecting env (0) from .env -- tip: ðŸ¤– agentic secret storage: https://dotenvx.com/as2
order-service-1    | {"level":"WARN","timestamp":"2026-02-25T10:50:23.570Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Orders'
kafka-1            | [2026-02-25 10:39:38,616] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-1, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=1, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 4ms (36/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,617] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,617] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,617] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,617] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Orders' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Orders' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "userId" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "userId" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "userId" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "status" SET DEFAULT 'Pending';ALTER TABLE "Orders" ALTER COLUMN "status" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 10:39:38,620] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,620] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,620] INFO [LogLoader partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,621] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-43, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=43, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 6ms (37/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,623] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,623] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,623] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "Orders" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Orders" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Orders' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'OrderItems'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'OrderItems' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'OrderItems' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
kafka-1            | [2026-02-25 10:39:38,623] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,625] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,626] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,627] INFO [LogLoader partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "quantity" TYPE INTEGER;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "OrderItems" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "OrderItems" DROP CONSTRAINT "OrderItems_OrderId_fkey"
order-service-1    | Executing (default): ALTER TABLE "OrderItems"  ADD FOREIGN KEY ("OrderId") REFERENCES "Orders" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'OrderItems' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'DeadLetterEvents'
order-service-1    | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'DeadLetterEvents' AND c.table_schema = 'public'
order-service-1    | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'DeadLetterEvents' AND tc.table_catalog = 'orders_db'
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
kafka-1            | [2026-02-25 10:39:38,631] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-25, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=25, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 9ms (38/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,633] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "payload" TYPE JSON;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "errorMessage" TYPE TEXT;
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:45.406Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:45.407Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":275}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:45.701Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:45.703Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":496}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:46.212Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:46.213Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":832}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:47.055Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:47.056Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1702}
kafka-1            | [2026-02-25 10:39:38,633] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
order-service-1    | Executing (default): ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "DeadLetterEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
order-service-1    | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'DeadLetterEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:23.678Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:23.679Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":335}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:24.029Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:24.031Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":796}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:24.840Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
kafka-1            | [2026-02-25 10:39:38,633] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,633] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,635] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,636] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,636] INFO [LogLoader partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,637] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-6, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=6, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 6ms (39/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.785Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:48.786Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3762}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:52.564Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
order-service-1    | {"level":"ERROR","timestamp":"2026-02-25T10:50:24.841Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1324}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:52.565Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6748}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2026-02-25 10:39:38,639] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,639] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,639] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,639] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2026-02-25 10:39:38,641] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,642] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,642] INFO [LogLoader partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,647] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-2, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=2, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 10ms (40/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,648] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,649] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,649] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,649] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,651] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6748,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
kafka-1            | [2026-02-25 10:39:38,651] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,651] INFO [LogLoader partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,655] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-28, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=28, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (41/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,656] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,656] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,656] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
kafka-1            | [2026-02-25 10:39:38,656] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,659] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,659] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,659] INFO [LogLoader partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
kafka-1            | [2026-02-25 10:39:38,661] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-11, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=11, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (42/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,663] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,664] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2026-02-25 10:39:38,664] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,664] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,666] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,667] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,667] INFO [LogLoader partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,672] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-23, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=23, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 11ms (43/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
kafka-1            | [2026-02-25 10:39:38,674] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,674] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,675] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6748,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
kafka-1            | [2026-02-25 10:39:38,675] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,676] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  override existing env vars with { override: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:44:53.489Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
kafka-1            | [2026-02-25 10:39:38,677] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
kafka-1            | [2026-02-25 10:39:38,677] INFO [LogLoader partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,679] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-9, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=9, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 6ms (44/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,680] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,680] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,680] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,680] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,682] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,682] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,682] INFO [LogLoader partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,684] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-42, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=42, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 5ms (45/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,687] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
kafka-1            | [2026-02-25 10:39:38,688] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,688] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,688] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,689] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,689] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,689] INFO [LogLoader partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,692] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-19, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=19, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (46/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,694] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,694] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,694] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,694] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,698] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,698] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
kafka-1            | [2026-02-25 10:39:38,698] INFO [LogLoader partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,699] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-30, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=30, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 7ms (47/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,700] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,700] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,700] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,700] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,702] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,702] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,702] INFO [LogLoader partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,704] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-32, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=32, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 4ms (48/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
kafka-1            | [2026-02-25 10:39:38,706] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,706] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
kafka-1            | [2026-02-25 10:39:38,706] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,706] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,709] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,709] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,709] INFO [LogLoader partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,717] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-35, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=35, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 13ms (49/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,722] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,722] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,722] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,722] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,724] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,724] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
kafka-1            | [2026-02-25 10:39:38,724] INFO [LogLoader partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,726] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-45, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=45, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 8ms (50/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,728] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Recovering unflushed segment 0 (kafka.log.LogLoader$)
kafka-1            | [2026-02-25 10:39:38,728] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,728] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,728] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
kafka-1            | [2026-02-25 10:39:38,731] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,731] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,731] INFO [LogLoader partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.Log$)
kafka-1            | [2026-02-25 10:39:38,735] INFO Completed load of Log(dir=/var/lib/kafka/data/__consumer_offsets-7, topicId=lUJe58ubQHKzh0NJNktnNQ, topic=__consumer_offsets, partition=7, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=0) with 1 segments in 9ms (51/51 loaded in /var/lib/kafka/data) (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,754] INFO Loaded 51 logs in 679ms. (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,758] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:53.588Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:53.589Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":285}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:53.888Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:53.889Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":500}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:54.401Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:54.402Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1194}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:55.608Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:55.609Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2176}
kafka-1            | [2026-02-25 10:39:38,760] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:38,781] INFO Starting the log cleaner (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 10:39:38,818] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 10:39:39,140] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Starting (kafka.server.BrokerToControllerRequestThread)
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:57.793Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:44:57.794Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3616}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:01.422Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:01.423Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7794}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
kafka-1            | [2026-02-25 10:39:39,351] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
kafka-1            | [2026-02-25 10:39:39,355] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
kafka-1            | [2026-02-25 10:39:39,398] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
kafka-1            | [2026-02-25 10:39:39,414] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Starting (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 10:39:39,434] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
kafka-1            | [2026-02-25 10:39:39,435] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,439] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,441] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,458] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka-1            | [2026-02-25 10:39:39,551] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
kafka-1            | [2026-02-25 10:39:39,579] ERROR Error while creating ephemeral at /brokers/ids/1, node already exists and owner '72057986241986561' does not match current session '72057596910239745' (kafka.zk.KafkaZkClient$CheckedEphemeral)
kafka-1            | [2026-02-25 10:39:39,584] ERROR [KafkaServer id=1] Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
kafka-1            | org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists
kafka-1            | 	at org.apache.zookeeper.KeeperException.create(KeeperException.java:126)
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7794,
kafka-1            | 	at kafka.zk.KafkaZkClient$CheckedEphemeral.getAfterNodeExists(KafkaZkClient.scala:1904)
kafka-1            | 	at kafka.zk.KafkaZkClient$CheckedEphemeral.create(KafkaZkClient.scala:1842)
kafka-1            | 	at kafka.zk.KafkaZkClient.checkedEphemeralCreate(KafkaZkClient.scala:1809)
kafka-1            | 	at kafka.zk.KafkaZkClient.registerBroker(KafkaZkClient.scala:96)
kafka-1            | 	at kafka.server.KafkaServer.startup(KafkaServer.scala:319)
kafka-1            | 	at kafka.Kafka$.main(Kafka.scala:109)
kafka-1            | 	at kafka.Kafka.main(Kafka.scala)
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
kafka-1            | [2026-02-25 10:39:39,589] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
kafka-1            | [2026-02-25 10:39:39,590] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
kafka-1            | [2026-02-25 10:39:39,597] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
kafka-1            | [2026-02-25 10:39:39,606] INFO [ReplicaManager broker=1] Shutting down (kafka.server.ReplicaManager)
kafka-1            | [2026-02-25 10:39:39,607] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka-1            | [2026-02-25 10:39:39,608] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka-1            | [2026-02-25 10:39:39,608] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
kafka-1            | [2026-02-25 10:39:39,610] INFO [ReplicaFetcherManager on broker 1] shutting down (kafka.server.ReplicaFetcherManager)
kafka-1            | [2026-02-25 10:39:39,611] INFO [ReplicaFetcherManager on broker 1] shutdown completed (kafka.server.ReplicaFetcherManager)
kafka-1            | [2026-02-25 10:39:39,611] INFO [ReplicaAlterLogDirsManager on broker 1] shutting down (kafka.server.ReplicaAlterLogDirsManager)
kafka-1            | [2026-02-25 10:39:39,612] INFO [ReplicaAlterLogDirsManager on broker 1] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
kafka-1            | [2026-02-25 10:39:39,612] INFO [ExpirationReaper-1-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,638] INFO [ExpirationReaper-1-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,638] INFO [ExpirationReaper-1-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,642] INFO [ExpirationReaper-1-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,839] INFO [ExpirationReaper-1-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,839] INFO [ExpirationReaper-1-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,840] INFO [ExpirationReaper-1-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,841] INFO [ExpirationReaper-1-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,841] INFO [ExpirationReaper-1-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,841] INFO [ExpirationReaper-1-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,842] INFO [ExpirationReaper-1-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,842] INFO [ExpirationReaper-1-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
kafka-1            | [2026-02-25 10:39:39,852] INFO [ReplicaManager broker=1] Shut down completely (kafka.server.ReplicaManager)
kafka-1            | [2026-02-25 10:39:39,853] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Shutting down (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 10:39:39,855] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Stopped (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 10:39:39,855] INFO [BrokerToControllerChannelManager broker=1 name=alterIsr]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 10:39:39,865] INFO Broker to controller channel manager for alterIsr shutdown (kafka.server.BrokerToControllerChannelManagerImpl)
kafka-1            | [2026-02-25 10:39:39,866] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Shutting down (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 10:39:39,866] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Stopped (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 10:39:39,866] INFO [BrokerToControllerChannelManager broker=1 name=forwarding]: Shutdown completed (kafka.server.BrokerToControllerRequestThread)
kafka-1            | [2026-02-25 10:39:39,869] INFO Broker to controller channel manager for forwarding shutdown (kafka.server.BrokerToControllerChannelManagerImpl)
kafka-1            | [2026-02-25 10:39:39,870] INFO Shutting down. (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:39,873] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 10:39:39,873] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 10:39:39,874] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 10:39:39,874] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner)
kafka-1            | [2026-02-25 10:39:39,949] INFO Shutdown complete. (kafka.log.LogManager)
kafka-1            | [2026-02-25 10:39:39,950] INFO [feature-zk-node-event-process-thread]: Shutting down (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
kafka-1            | [2026-02-25 10:39:39,950] INFO [feature-zk-node-event-process-thread]: Stopped (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
kafka-1            | [2026-02-25 10:39:39,950] INFO [feature-zk-node-event-process-thread]: Shutdown completed (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
kafka-1            | [2026-02-25 10:39:39,951] INFO [ZooKeeperClient Kafka server] Closing. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2026-02-25 10:39:40,057] INFO Session: 0x1000000ab340001 closed (org.apache.zookeeper.ZooKeeper)
kafka-1            | [2026-02-25 10:39:40,057] INFO EventThread shut down for session: 0x1000000ab340001 (org.apache.zookeeper.ClientCnxn)
kafka-1            | [2026-02-25 10:39:40,067] INFO [ZooKeeperClient Kafka server] Closed. (kafka.zookeeper.ZooKeeperClient)
kafka-1            | [2026-02-25 10:39:40,067] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
kafka-1            | [2026-02-25 10:39:41,013] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:41,013] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7794,
kafka-1            | [2026-02-25 10:39:41,014] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,014] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,014] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,014] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
kafka-1            | [2026-02-25 10:39:42,015] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,015] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,015] INFO [ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,016] INFO [ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,016] INFO [ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
kafka-1            | [2026-02-25 10:39:42,020] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
kafka-1            | [2026-02-25 10:39:42,085] INFO [SocketServer listenerType=ZK_BROKER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
kafka-1            | [2026-02-25 10:39:42,086] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics)
kafka-1            | [2026-02-25 10:39:42,089] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics)
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
kafka-1            | [2026-02-25 10:39:42,090] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics)
kafka-1            | [2026-02-25 10:39:42,103] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
kafka-1            | [2026-02-25 10:39:42,114] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
kafka-1            | [2026-02-25 10:39:42,114] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)
kafka-1            | [2026-02-25 10:39:42,114] ERROR Exiting Kafka. (kafka.Kafka$)
kafka-1            | [2026-02-25 10:39:42,115] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:45:02.719Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:02.809Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:02.810Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":275}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:03.100Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:03.101Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":644}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:03.757Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:03.759Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1306}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:05.078Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:05.080Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3098}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:08.190Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:08.190Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6190}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:14.401Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:14.402Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":14266}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 14266,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 14266,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:45:14.991Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.082Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.083Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":252}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.355Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.356Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":558}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.922Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:15.923Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1124}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:17.060Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:17.062Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1862}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:18.938Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:18.939Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3960}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:22.916Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:22.917Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8406}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8406,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8406,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:45:23.690Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.793Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:23.794Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":295}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:24.103Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:24.104Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":702}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:24.820Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:24.821Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1284}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:26.121Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:26.123Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3064}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:29.201Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:29.202Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5830}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:35.046Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:35.048Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":13918}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 13918,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 13918,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:45:35.738Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:35.832Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:35.833Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":273}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:36.126Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:36.127Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":570}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:36.714Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:36.715Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1180}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:37.914Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:37.916Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2212}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:40.142Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:40.142Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4816}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:44.972Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:44.974Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8712}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8712,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8712,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:45:45.794Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:45.949Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:45.950Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":347}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:46.323Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:46.324Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":756}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:47.098Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:47.101Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1760}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:48.873Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:48.876Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3984}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:52.873Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:45:52.874Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":7282}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:00.166Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:00.167Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":13408}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 13408,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 13408,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” prevent committing .env to code: https://dotenvx.com/precommit
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:46:00.910Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:01.017Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:01.018Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":334}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:01.364Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:01.365Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":636}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:02.016Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:02.017Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1380}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:03.413Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:03.415Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2564}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.993Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:05.994Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4972}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:10.984Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:10.986Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10014}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10014,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10014,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  enable debug logging with { debug: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:46:11.661Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:11.773Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:11.774Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":346}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:12.136Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:12.137Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":748}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:12.898Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:12.900Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1198}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:14.115Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:14.118Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1928}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:16.054Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:16.054Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4230}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:20.306Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:20.310Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8986}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8986,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8986,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ”„ add secrets lifecycle management: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:46:21.149Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:21.269Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:21.271Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":320}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:21.605Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:21.607Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":702}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:22.334Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:22.334Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1654}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:24.004Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:24.009Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3458}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:27.475Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:27.477Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5956}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:33.454Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:33.457Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10126}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10126,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10126,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” encrypt with Dotenvx: https://dotenvx.com
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:46:34.252Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:34.369Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:34.370Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":277}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:34.661Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:34.663Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":552}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:35.262Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:35.263Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1286}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:36.564Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:36.566Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2482}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:39.065Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:39.066Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3990}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:43.069Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:43.071Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6402}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6402,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6402,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  override existing env vars with { override: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:46:43.824Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:43.977Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:43.980Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":350}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:44.344Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:44.346Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":788}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:45.148Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:45.149Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1694}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:46.881Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:46.884Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3904}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:50.801Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:50.802Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":9164}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:59.986Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:46:59.987Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":19712}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 19712,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 19712,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” prevent building .env in docker: https://dotenvx.com/prebuild
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:47:00.880Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:00.992Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:00.992Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":319}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:01.330Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:01.331Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":530}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:01.877Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:01.878Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1080}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:02.975Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:02.977Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2296}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:05.286Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:05.287Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4040}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:09.341Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:09.344Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8744}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8744,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8744,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ› ï¸  run anywhere with `dotenvx run -- yourcommand`
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:47:10.164Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:10.273Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:10.274Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":296}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:10.586Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:10.588Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":500}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:11.105Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:11.106Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":928}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:12.056Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:12.058Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1934}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:14.004Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:14.005Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3898}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:17.918Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:17.919Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7214}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7214,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7214,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:47:19.042Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:19.180Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:19.182Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":337}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:19.534Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:19.536Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":582}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:20.138Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:20.139Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":936}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:21.086Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:21.087Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2102}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:23.201Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:23.203Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3722}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:26.939Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:26.940Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8230}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8230,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8230,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:47:28.344Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:28.458Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:28.459Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":334}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:28.811Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:28.812Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":666}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:29.489Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:29.489Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1468}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:30.971Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:30.972Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3492}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:34.480Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:34.481Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6280}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:40.778Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:40.780Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":14706}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 14706,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 14706,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:47:41.726Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:41.866Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:41.866Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":352}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:42.231Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:42.232Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":802}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:43.047Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:43.048Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1472}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:44.536Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:44.538Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3176}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:47.732Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:47.733Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6270}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:54.022Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:54.024Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":14536}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 14536,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 14536,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:47:54.745Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:54.891Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:54.892Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":326}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:55.235Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:55.237Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":692}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:55.945Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:55.945Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1298}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:57.269Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:57.271Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2262}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:59.557Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:47:59.560Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3826}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:03.410Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:03.413Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":6456}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6456,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 6456,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ‘¥ sync secrets across teammates & machines: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:48:04.726Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:04.951Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:04.952Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":311}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:05.440Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:05.499Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":648}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:06.161Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:06.162Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1390}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:07.571Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:07.574Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2786}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:10.370Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:10.371Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":6462}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:16.853Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:16.854Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":11484}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 11484,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 11484,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ› ï¸  run anywhere with `dotenvx run -- yourcommand`
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:48:17.756Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:17.925Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:17.926Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":292}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:18.231Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:18.232Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":594}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:18.920Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:18.921Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":966}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:19.900Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:19.909Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1880}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:21.810Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:21.811Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3820}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:25.650Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:25.651Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8492}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8492,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8492,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  load multiple .env files with { path: ['.env.local', '.env'] }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:48:26.553Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:26.693Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:26.694Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":296}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:27.003Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:27.004Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":554}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:27.570Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:27.570Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1026}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:28.609Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:28.610Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2090}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:30.717Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:30.717Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3898}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:34.632Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:34.634Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8030}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8030,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8030,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ—‚ï¸ backup and recover secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:48:35.946Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:36.093Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:36.095Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":301}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:36.410Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:36.411Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":602}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:37.027Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:37.028Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1306}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.345Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:38.346Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2172}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:40.542Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:40.548Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4346}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:44.907Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:44.910Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7008}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7008,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7008,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  suppress all logs with { quiet: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:48:46.518Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:46.652Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:46.654Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":304}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:46.971Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:46.972Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":528}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:47.512Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:47.512Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":916}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:48.442Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:48.444Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1734}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:50.200Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:50.202Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4042}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:54.258Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:54.259Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":8614}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8614,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 8614,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ“¡ add observability to secrets: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:48:56.580Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:56.754Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:56.755Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":296}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:57.070Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:57.071Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":512}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:57.595Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:57.596Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":976}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:58.587Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:48:58.589Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2026}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:00.630Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:00.631Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4762}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:05.409Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:05.412Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9642}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 9642,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 9642,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  specify custom .env file path with { path: '/custom/path/.env' }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:49:09.311Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:09.439Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:09.440Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":295}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:09.751Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:09.753Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":568}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:10.335Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:10.336Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1208}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:11.557Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:11.560Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1996}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:13.568Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:13.568Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3636}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:17.223Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:17.231Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7840}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7840,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7840,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: ðŸ” prevent building .env in docker: https://dotenvx.com/prebuild
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:49:24.385Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:24.520Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:24.521Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":285}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:24.822Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:24.823Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":490}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.330Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:25.331Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1120}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:26.467Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:26.470Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2058}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:28.541Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:28.542Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4546}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:33.102Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:33.103Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":9278}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 9278,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 9278,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  specify custom .env file path with { path: '/custom/path/.env' }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:49:46.779Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:46.911Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:46.912Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":248}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:47.179Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:47.180Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":580}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:47.775Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:47.775Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1390}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:49.183Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:49.186Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":3042}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:52.241Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:52.242Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5640}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:57.908Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:57.912Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":10836}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10836,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 10836,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:49:58.977Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.108Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.109Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":305}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.424Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:49:59.425Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":674}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:00.113Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:00.114Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1240}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:01.369Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:01.372Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2264}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:03.650Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:03.651Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":5354}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:09.020Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:09.023Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":12816}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12816,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 12816,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âš™ï¸  enable debug logging with { debug: true }
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:50:09.757Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:09.893Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:09.894Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":341}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:10.270Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:10.273Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":586}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:10.869Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:10.869Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":1060}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.942Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:11.944Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":2254}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:14.215Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:14.215Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":4428}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:18.661Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:18.663Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":5,"retryTime":7938}
product-service-1  | Error starting Kafka consumer: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7938,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | Startup error: KafkaJSNonRetriableError
product-service-1  |   Caused by: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |     at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |     ... 3 lines matching cause stack trace ...
product-service-1  |     at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |   name: 'KafkaJSNumberOfRetriesExceeded',
product-service-1  |   retriable: false,
product-service-1  |   helpUrl: undefined,
product-service-1  |   retryCount: 5,
product-service-1  |   retryTime: 7938,
product-service-1  |   [cause]: KafkaJSConnectionError: Connection error: getaddrinfo ENOTFOUND kafka
product-service-1  |       at Socket.onError (/app/node_modules/kafkajs/src/network/connection.js:210:23)
product-service-1  |       at Socket.emit (node:events:517:28)
product-service-1  |       at emitErrorNT (node:internal/streams/destroy:151:8)
product-service-1  |       at emitErrorCloseNT (node:internal/streams/destroy:116:3)
product-service-1  |       at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
product-service-1  |     retriable: true,
product-service-1  |     helpUrl: undefined,
product-service-1  |     broker: 'kafka:9092',
product-service-1  |     code: 'ENOTFOUND',
product-service-1  |     [cause]: undefined
product-service-1  |   }
product-service-1  | }
product-service-1  | 
product-service-1  | > product-service@1.0.0 start
product-service-1  | > node index.js
product-service-1  | 
product-service-1  | [dotenv@17.2.3] injecting env (0) from .env -- tip: âœ… audit secrets and track compliance: https://dotenvx.com/ops
product-service-1  | {"level":"WARN","timestamp":"2026-02-25T10:50:19.734Z","logger":"kafkajs","message":"KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\""}
product-service-1  | Executing (default): SELECT 1+1 AS result
product-service-1  | Database connected successfully
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'Products'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'Products' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'Products' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "name" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "name" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "name" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "stock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "stock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "stock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "reservedStock" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "reservedStock" SET DEFAULT 0;ALTER TABLE "Products" ALTER COLUMN "reservedStock" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "version" DROP NOT NULL;ALTER TABLE "Products" ALTER COLUMN "version" SET DEFAULT 1;ALTER TABLE "Products" ALTER COLUMN "version" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "Products" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "Products" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "Products" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'Products' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'StockReservations'
product-service-1  | Executing (default): SELECT t.typname enum_name, array_agg(e.enumlabel ORDER BY enumsortorder) enum_value FROM pg_type t JOIN pg_enum e ON t.oid = e.enumtypid JOIN pg_catalog.pg_namespace n ON n.oid = t.typnamespace WHERE n.nspname = 'public' AND t.typname='enum_StockReservations_status' GROUP BY 1
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'StockReservations' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'StockReservations' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "orderId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "orderId" DROP DEFAULT;ALTER TABLE "StockReservations"  ADD UNIQUE ("orderId");ALTER TABLE "StockReservations" ALTER COLUMN "orderId" TYPE INTEGER  ;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "productId" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "productId" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "productId" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "quantity" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "quantity" TYPE INTEGER;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "status" DROP NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "status" SET DEFAULT 'RESERVED';DO 'BEGIN CREATE TYPE "public"."enum_StockReservations_status" AS ENUM(''RESERVED'', ''CONFIRMED'', ''RELEASED''); EXCEPTION WHEN duplicate_object THEN null; END';ALTER TABLE "StockReservations" ALTER COLUMN "status" TYPE "public"."enum_StockReservations_status" USING ("status"::"public"."enum_StockReservations_status");
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "expiresAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "StockReservations" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "StockReservations" DROP CONSTRAINT "StockReservations_ProductId_fkey"
product-service-1  | Executing (default): ALTER TABLE "StockReservations"  ADD FOREIGN KEY ("ProductId") REFERENCES "Products" ("id") ON DELETE SET NULL ON UPDATE CASCADE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'StockReservations' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Executing (default): SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'ProcessedEvents'
product-service-1  | Executing (default): SELECT pk.constraint_type as "Constraint",c.column_name as "Field", c.column_default as "Default",c.is_nullable as "Null", (CASE WHEN c.udt_name = 'hstore' THEN c.udt_name ELSE c.data_type END) || (CASE WHEN c.character_maximum_length IS NOT NULL THEN '(' || c.character_maximum_length || ')' ELSE '' END) as "Type", (SELECT array_agg(e.enumlabel) FROM pg_catalog.pg_type t JOIN pg_catalog.pg_enum e ON t.oid=e.enumtypid WHERE t.typname=c.udt_name) AS "special", (SELECT pgd.description FROM pg_catalog.pg_statio_all_tables AS st INNER JOIN pg_catalog.pg_description pgd on (pgd.objoid=st.relid) WHERE c.ordinal_position=pgd.objsubid AND c.table_name=st.relname) AS "Comment" FROM information_schema.columns c LEFT JOIN (SELECT tc.table_schema, tc.table_name, cu.column_name, tc.constraint_type FROM information_schema.TABLE_CONSTRAINTS tc JOIN information_schema.KEY_COLUMN_USAGE  cu ON tc.table_schema=cu.table_schema and tc.table_name=cu.table_name and tc.constraint_name=cu.constraint_name and tc.constraint_type='PRIMARY KEY') pk ON pk.table_schema=c.table_schema AND pk.table_name=c.table_name AND pk.column_name=c.column_name WHERE c.table_name = 'ProcessedEvents' AND c.table_schema = 'public'
product-service-1  | Executing (default): SELECT DISTINCT tc.constraint_name as constraint_name, tc.constraint_schema as constraint_schema, tc.constraint_catalog as constraint_catalog, tc.table_name as table_name,tc.table_schema as table_schema,tc.table_catalog as table_catalog,tc.initially_deferred as initially_deferred,tc.is_deferrable as is_deferrable,kcu.column_name as column_name,ccu.table_schema  AS referenced_table_schema,ccu.table_catalog  AS referenced_table_catalog,ccu.table_name  AS referenced_table_name,ccu.column_name AS referenced_column_name FROM information_schema.table_constraints AS tc JOIN information_schema.key_column_usage AS kcu ON tc.constraint_name = kcu.constraint_name JOIN information_schema.constraint_column_usage AS ccu ON ccu.constraint_name = tc.constraint_name WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = 'ProcessedEvents' AND tc.table_catalog = 'products_db'
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "eventType" TYPE VARCHAR(255);
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "processedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "createdAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" SET NOT NULL;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" DROP DEFAULT;ALTER TABLE "ProcessedEvents" ALTER COLUMN "updatedAt" TYPE TIMESTAMP WITH TIME ZONE;
product-service-1  | Executing (default): SELECT i.relname AS name, ix.indisprimary AS primary, ix.indisunique AS unique, ix.indkey AS indkey, array_agg(a.attnum) as column_indexes, array_agg(a.attname) AS column_names, pg_get_indexdef(ix.indexrelid) AS definition FROM pg_class t, pg_class i, pg_index ix, pg_attribute a WHERE t.oid = ix.indrelid AND i.oid = ix.indexrelid AND a.attrelid = t.oid AND t.relkind = 'r' and t.relname = 'ProcessedEvents' GROUP BY i.relname, ix.indexrelid, ix.indisprimary, ix.indisunique, ix.indkey ORDER BY i.relname;
product-service-1  | Database synced
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:19.873Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:19.875Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":0,"retryTime":257}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:20.157Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:20.161Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":1,"retryTime":592}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:20.772Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:20.775Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":2,"retryTime":978}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:21.781Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:21.785Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":3,"retryTime":1706}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:23.504Z","logger":"kafkajs","message":"[Connection] Connection error: getaddrinfo ENOTFOUND kafka","broker":"kafka:9092","clientId":"kafkajs","stack":"Error: getaddrinfo ENOTFOUND kafka\n    at GetAddrInfoReqWrap.onlookup [as oncomplete] (node:dns:107:26)"}
product-service-1  | {"level":"ERROR","timestamp":"2026-02-25T10:50:23.505Z","logger":"kafkajs","message":"[BrokerPool] Failed to connect to seed broker, trying another broker from the list: Connection error: getaddrinfo ENOTFOUND kafka","retryCount":4,"retryTime":3290}
